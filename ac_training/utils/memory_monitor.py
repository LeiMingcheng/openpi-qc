"""
è¯¦ç»†çš„æ˜¾å­˜ç›‘æ§å·¥å…·
åŸºäºFSDPè°ƒè¯•æŠ¥å‘Šçš„åˆ†ææ–¹æ³•ï¼Œæä¾›ç»„ä»¶çº§æ˜¾å­˜è¿½è¸ª
"""

import jax
import jax.numpy as jnp
from typing import Dict, Any, Optional
import logging
import time

logger = logging.getLogger(__name__)

class GPUMemoryMonitor:
    """GPUå†…å­˜ä½¿ç”¨ç›‘æ§å™¨ï¼Œæ”¯æŒç»„ä»¶çº§è¿½è¸ª"""
    
    def __init__(self):
        self.baseline_memory = self._get_gpu_memory_usage()
        self.checkpoints = {}
        
    def _get_gpu_memory_usage(self) -> Dict[str, float]:
        """è·å–GPUå†…å­˜ä½¿ç”¨è¯¦æƒ…"""
        try:
            # JAXå†…å­˜ç»Ÿè®¡
            memory_info = {}
            
            # XLAå†…å­˜ç»Ÿè®¡
            for device in jax.devices():
                stats = device.memory_stats()
                device_id = f"GPU_{device.id}"
                memory_info[device_id] = {
                    'bytes_in_use': stats.get('bytes_in_use', 0),
                    'pool_bytes': stats.get('pool_bytes', 0), 
                    'peak_bytes_in_use': stats.get('peak_bytes_in_use', 0),
                    'bytes_reserved': stats.get('bytes_reserved', 0)
                }
                
            return memory_info
        except Exception as e:
            logger.warning(f"Failed to get GPU memory stats: {e}")
            return {}
    
    def _calculate_memory_size(self, pytree: Any, name: str = "") -> float:
        """è®¡ç®—pytreeçš„å®é™…å†…å­˜å ç”¨ï¼ˆå­—èŠ‚ï¼‰"""
        try:
            total_bytes = 0
            count = 0
            
            def count_bytes(leaf):
                nonlocal total_bytes, count
                if hasattr(leaf, 'nbytes'):
                    # JAXæ•°ç»„
                    total_bytes += leaf.nbytes
                    count += 1
                elif hasattr(leaf, 'size') and hasattr(leaf, 'itemsize'):
                    # NumPyæ•°ç»„
                    total_bytes += leaf.size * leaf.itemsize
                    count += 1
                return leaf
                
            jax.tree_map(count_bytes, pytree)
            
            logger.debug(f"Memory analysis for {name}: {total_bytes/1e9:.3f}GB across {count} arrays")
            return total_bytes
            
        except Exception as e:
            logger.warning(f"Failed to calculate memory for {name}: {e}")
            return 0
    
    def analyze_train_state_memory(self, train_state: Any) -> Dict[str, Dict[str, float]]:
        """åˆ†æè®­ç»ƒçŠ¶æ€çš„è¯¦ç»†å†…å­˜ç»„æˆ"""
        analysis = {
            'Ï€â‚€_parameters': {},
            'critic_parameters': {}, 
            'optimizer_states': {},
            'ema_parameters': {},
            'total_summary': {}
        }
        
        try:
            # Ï€â‚€å‚æ•°åˆ†æ
            if hasattr(train_state, 'pi0_params') and train_state.pi0_params is not None:
                pi0_bytes = self._calculate_memory_size(train_state.pi0_params, "Ï€â‚€_parameters")
                analysis['Ï€â‚€_parameters'] = {
                    'bytes': pi0_bytes,
                    'GB': pi0_bytes / 1e9,
                    'percentage': pi0_bytes / (24.7e9) * 100  # åŸºäº3.2Bå‚æ•°çš„ç†è®ºå€¼
                }
            
            # Criticå‚æ•°åˆ†æ
            if hasattr(train_state, 'critic_params') and train_state.critic_params is not None:
                critic_bytes = self._calculate_memory_size(train_state.critic_params, "critic_parameters")
                analysis['critic_parameters'] = {
                    'bytes': critic_bytes,
                    'GB': critic_bytes / 1e9,
                    'networks': len(train_state.critic_params) if isinstance(train_state.critic_params, (list, tuple)) else 1
                }
            
            # ä¼˜åŒ–å™¨çŠ¶æ€åˆ†æ
            if hasattr(train_state, 'opt_state') and train_state.opt_state is not None:
                opt_bytes = self._calculate_memory_size(train_state.opt_state, "optimizer_states")
                analysis['optimizer_states'] = {
                    'bytes': opt_bytes,
                    'GB': opt_bytes / 1e9,
                    'components': self._analyze_optimizer_components(train_state.opt_state)
                }
            
            # EMAå‚æ•°åˆ†æ
            if hasattr(train_state, 'ema_params') and train_state.ema_params is not None:
                ema_bytes = self._calculate_memory_size(train_state.ema_params, "ema_parameters")
                analysis['ema_parameters'] = {
                    'bytes': ema_bytes,
                    'GB': ema_bytes / 1e9
                }
            
            # æ€»ç»“åˆ†æ
            total_bytes = sum([
                analysis['Ï€â‚€_parameters'].get('bytes', 0),
                analysis['critic_parameters'].get('bytes', 0),
                analysis['optimizer_states'].get('bytes', 0),
                analysis['ema_parameters'].get('bytes', 0)
            ])
            
            analysis['total_summary'] = {
                'total_bytes': total_bytes,
                'total_GB': total_bytes / 1e9,
                'theoretical_GB': 6.2,  # ä»FSDPæŠ¥å‘Š
                'efficiency': (total_bytes / 1e9) / 6.2 * 100
            }
            
        except Exception as e:
            logger.error(f"Failed to analyze train state memory: {e}")
        
        return analysis
    
    def _analyze_optimizer_components(self, opt_state: Any) -> Dict[str, float]:
        """åˆ†æä¼˜åŒ–å™¨çŠ¶æ€çš„ç»„ä»¶åˆ†å¸ƒ"""
        components = {}
        try:
            # Adamä¼˜åŒ–å™¨é€šå¸¸æœ‰mu(momentum)å’Œnu(variance)
            if hasattr(opt_state, 'mu') and opt_state.mu is not None:
                components['momentum'] = self._calculate_memory_size(opt_state.mu, "momentum") / 1e9
            if hasattr(opt_state, 'nu') and opt_state.nu is not None:
                components['variance'] = self._calculate_memory_size(opt_state.nu, "variance") / 1e9
            
            # å¤„ç†åµŒå¥—çš„ä¼˜åŒ–å™¨çŠ¶æ€
            if isinstance(opt_state, (list, tuple)):
                for i, state in enumerate(opt_state):
                    if hasattr(state, '__dict__'):
                        for attr_name, attr_value in state.__dict__.items():
                            if attr_value is not None:
                                size_gb = self._calculate_memory_size(attr_value, f"opt_component_{i}_{attr_name}") / 1e9
                                if size_gb > 0.001:  # åªè®°å½•å¤§äº1MBçš„ç»„ä»¶
                                    components[f"component_{i}_{attr_name}"] = size_gb
                                    
        except Exception as e:
            logger.warning(f"Failed to analyze optimizer components: {e}")
        
        return components
    
    def checkpoint_memory(self, name: str, train_state: Optional[Any] = None):
        """åˆ›å»ºå†…å­˜ä½¿ç”¨æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'timestamp': time.time(),
            'gpu_memory': self._get_gpu_memory_usage(),
            'train_state_analysis': None
        }
        
        if train_state is not None:
            checkpoint['train_state_analysis'] = self.analyze_train_state_memory(train_state)
        
        self.checkpoints[name] = checkpoint
        
        # æ‰“å°å†…å­˜æ‘˜è¦
        self._print_memory_summary(name, checkpoint)
        
    def _print_memory_summary(self, checkpoint_name: str, checkpoint: Dict[str, Any]):
        """æ‰“å°å†…å­˜ä½¿ç”¨æ‘˜è¦"""
        logger.info(f"ğŸ” å†…å­˜æ£€æŸ¥ç‚¹: {checkpoint_name}")
        
        # GPUå†…å­˜
        for device_id, stats in checkpoint['gpu_memory'].items():
            bytes_in_use_gb = stats['bytes_in_use'] / 1e9
            pool_bytes_gb = stats['pool_bytes'] / 1e9
            peak_gb = stats['peak_bytes_in_use'] / 1e9
            
            logger.info(f"   {device_id}: æ•°æ®ä½¿ç”¨ {bytes_in_use_gb:.2f}GB | å†…å­˜æ±  {pool_bytes_gb:.2f}GB | å³°å€¼ {peak_gb:.2f}GB")
        
        # è®­ç»ƒçŠ¶æ€åˆ†æ
        if checkpoint['train_state_analysis']:
            analysis = checkpoint['train_state_analysis']
            
            logger.info("   ğŸ“Š ç»„ä»¶å†…å­˜åˆ†å¸ƒ:")
            for component, info in analysis.items():
                if component != 'total_summary' and info.get('GB', 0) > 0:
                    logger.info(f"      {component}: {info['GB']:.3f}GB")
            
            # æ€»ç»“
            summary = analysis.get('total_summary', {})
            if summary:
                logger.info(f"   ğŸ“ˆ æ€»è®¡: {summary.get('total_GB', 0):.2f}GB " +
                          f"(ç†è®ºå€¼: {summary.get('theoretical_GB', 0)}GB, " +
                          f"æ•ˆç‡: {summary.get('efficiency', 0):.1f}%)")
    
    def compare_checkpoints(self, checkpoint1: str, checkpoint2: str):
        """æ¯”è¾ƒä¸¤ä¸ªæ£€æŸ¥ç‚¹çš„å†…å­˜å˜åŒ–"""
        if checkpoint1 not in self.checkpoints or checkpoint2 not in self.checkpoints:
            logger.error(f"Checkpoints {checkpoint1} or {checkpoint2} not found")
            return
        
        cp1 = self.checkpoints[checkpoint1]
        cp2 = self.checkpoints[checkpoint2]
        
        logger.info(f"ğŸ”„ å†…å­˜å˜åŒ–å¯¹æ¯”: {checkpoint1} â†’ {checkpoint2}")
        
        # æ¯”è¾ƒGPUå†…å­˜
        for device_id in cp1['gpu_memory']:
            if device_id in cp2['gpu_memory']:
                usage1 = cp1['gpu_memory'][device_id]['bytes_in_use'] / 1e9
                usage2 = cp2['gpu_memory'][device_id]['bytes_in_use'] / 1e9
                delta = usage2 - usage1
                
                logger.info(f"   {device_id}: {usage1:.2f}GB â†’ {usage2:.2f}GB (Î”{delta:+.2f}GB)")
    
    def get_memory_report(self) -> str:
        """ç”Ÿæˆè¯¦ç»†çš„å†…å­˜ä½¿ç”¨æŠ¥å‘Š"""
        report = ["=" * 50, "GPUå†…å­˜ä½¿ç”¨è¯¦ç»†æŠ¥å‘Š", "=" * 50]
        
        current_memory = self._get_gpu_memory_usage()
        
        for device_id, stats in current_memory.items():
            report.append(f"\n{device_id}:")
            report.append(f"  å®é™…æ•°æ®ä½¿ç”¨: {stats['bytes_in_use']/1e9:.2f}GB")
            report.append(f"  XLAå†…å­˜æ± :   {stats['pool_bytes']/1e9:.2f}GB") 
            report.append(f"  å³°å€¼ä½¿ç”¨:    {stats['peak_bytes_in_use']/1e9:.2f}GB")
            report.append(f"  ä¿ç•™å†…å­˜:    {stats.get('bytes_reserved', 0)/1e9:.2f}GB")
        
        report.append("\nåŸºäºFSDPè°ƒè¯•æŠ¥å‘Šçš„å†…å­˜ç†è§£:")
        report.append("  â€¢ å®é™…æ•°æ®ä½¿ç”¨ ~6GB: æ¨¡å‹å‚æ•° + ä¼˜åŒ–å™¨çŠ¶æ€")
        report.append("  â€¢ XLAå†…å­˜æ±  ~60GB: æ€§èƒ½ä¼˜åŒ–çš„é¢„åˆ†é…ç­–ç•¥")
        report.append("  â€¢ è¿™æ˜¯XLAå¤§è§„æ¨¡è®­ç»ƒçš„æ­£å¸¸è¡Œä¸º")
        
        return "\n".join(report)


# å…¨å±€ç›‘æ§å™¨å®ä¾‹
memory_monitor = GPUMemoryMonitor()


def log_memory_usage(step: int, train_state: Any = None, phase: str = "training"):
    """ä¾¿æ·çš„å†…å­˜ä½¿ç”¨è®°å½•å‡½æ•°"""
    checkpoint_name = f"{phase}_step_{step}"
    memory_monitor.checkpoint_memory(checkpoint_name, train_state)


def log_gpu_memory(step_name: str = ""):
    """ç»Ÿä¸€çš„ GPU å†…å­˜ç›‘æ§å‡½æ•°ï¼Œå®¢è§‚è¾“å‡ºæ•°æ®"""
    try:
        import subprocess
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=index,memory.used,memory.total', '--format=csv,noheader,nounits'], 
            capture_output=True, text=True
        )
        if result.returncode == 0:
            step_prefix = f"GPU Memory - {step_name}: " if step_name else "GPU Memory Status:"
            logger.info(step_prefix)
            total_used = 0
            total_capacity = 0
            usage_data = []
            max_usage_gpu = 0
            max_usage_mb = 0
            
            for line in result.stdout.strip().split('\n'):
                parts = line.split(', ')
                if len(parts) == 3:
                    gpu_id, used, total = parts
                    used_mb = int(used)
                    total_mb = int(total)
                    usage_pct = (used_mb / total_mb) * 100
                    total_used += used_mb
                    total_capacity += total_mb
                    usage_data.append(used_mb)
                    
                    if used_mb > max_usage_mb:
                        max_usage_mb = used_mb
                        max_usage_gpu = int(gpu_id)
                    
                    logger.info(f"  GPU {gpu_id}: {used}MB/{total}MB ({usage_pct:.1f}%)")
            
            # è®¡ç®—ç»Ÿè®¡æ•°æ®
            overall_pct = (total_used / total_capacity) * 100 if total_capacity > 0 else 0
            avg_per_gpu = total_used / len(usage_data) if len(usage_data) > 0 else 0
            std_dev = (sum((x - avg_per_gpu) ** 2 for x in usage_data) / len(usage_data)) ** 0.5 if len(usage_data) > 0 else 0
            
            logger.info(f"  Total: {total_used}MB/{total_capacity}MB ({overall_pct:.1f}%)")
            logger.info(f"  Avg/GPU: {avg_per_gpu:.0f}MB, Max: {max_usage_mb:.0f}MB (GPU{max_usage_gpu}), StdDev: {std_dev:.0f}MB")
            
            # è¿”å›æ•°æ®ç”¨äºè¿›ä¸€æ­¥åˆ†æ
            return {
                'usage_data': usage_data,
                'avg_per_gpu': avg_per_gpu,
                'max_usage': max_usage_mb,
                'max_usage_gpu': max_usage_gpu,
                'std_dev': std_dev,
                'overall_pct': overall_pct
            }
                    
    except Exception as e:
        logger.warning(f"Failed to get GPU memory info: {e}")
        return None


def enable_memory_monitoring():
    """å¯ç”¨å†…å­˜ç›‘æ§ï¼ˆåœ¨è®­ç»ƒå¾ªç¯å¼€å§‹æ—¶è°ƒç”¨ï¼‰"""
    logger.info("ğŸ”§ å¯ç”¨è¯¦ç»†å†…å­˜ç›‘æ§ç³»ç»Ÿ")
    memory_monitor.checkpoint_memory("training_start")
    
    # æ‰“å°ç³»ç»Ÿä¿¡æ¯
    logger.info(f"   GPUè®¾å¤‡æ•°: {len(jax.devices())}")
    logger.info(f"   JAXåç«¯: {jax.lib.xla_bridge.get_backend().platform}")
    logger.info(memory_monitor.get_memory_report())