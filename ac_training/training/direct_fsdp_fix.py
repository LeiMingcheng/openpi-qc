"""
Direct FSDP Fix: ä¿®å¤8å¡FSDPå†…å­˜çˆ†ç‚¸é—®é¢˜
======================================

æ ¹æ®OpenPIæ ‡å‡†å®ç°å’Œè¯¦ç»†åˆ†æç»“æœï¼Œæ ¸å¿ƒé—®é¢˜æ˜¯JAX FSDPåœ¨å¤§æ¨¡å‹ä¸Šçš„åº•å±‚å¤±æ•ˆã€‚
æœ¬ä¿®å¤é‡‡ç”¨OpenPIçš„å®Œå…¨ç›¸åŒæ¨¡å¼æ¥è§£å†³å†…å­˜çˆ†ç‚¸é—®é¢˜ï¼šä»53GB/GPUé™åˆ°<8GB/GPUã€‚

æ ¸å¿ƒåŸç†ï¼š
1. ä½¿ç”¨jax.jitçš„out_shardingsç›´æ¥åˆ›å»ºåˆ†ç‰‡å‚æ•°
2. é¿å…å…ˆåˆ›å»ºå®Œæ•´å‚æ•°å†åˆ†ç‰‡çš„æ¨¡å¼ï¼ˆè¿™ä¼šå¯¼è‡´11.5xå†…å­˜æ”¾å¤§ï¼‰
3. å®Œå…¨æ¨¡ä»¿OpenPIçš„init_train_stateå®ç°

å†…å­˜åˆ†æï¼š
- ç†è®ºå‚æ•°: Ï€â‚€(12.4GB) + AdamW(24.7GB) + EMA(0GB) = 37GB
- ç†è®ºFSDP: 37GB Ã· 8 GPU = 4.6GB/GPU
- å®é™…å¤±æ•ˆ: 53GB/GPU (11.5xæ”¾å¤§)
- ä¿®å¤ç›®æ ‡: <8GB/GPU (å¯æ¥å—çš„åˆ†ç‰‡æ•ˆæœ)
"""

import logging
import jax
import jax.numpy as jnp
import dataclasses
from typing import Any, Tuple, Callable
from flax import nnx
import optax
import openpi.training.sharding as sharding
import openpi.training.optimizer as _optimizer
from openpi.training.utils import TrainState

logger = logging.getLogger(__name__)


def log_memory_usage(step_name: str):
    """è®°å½•GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    try:
        import subprocess
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=index,memory.used,memory.total', '--format=csv,noheader,nounits'],
            capture_output=True, text=True
        )
        if result.returncode == 0:
            total_used = 0
            gpu_count = 0
            for line in result.stdout.strip().split('\n'):
                if ',' in line:
                    gpu_id, used, total = line.split(', ')
                    total_used += int(used)
                    gpu_count += 1
                    logger.info(f"  GPU {gpu_id}: {used}MB/{total}MB")
            
            if gpu_count > 0:
                avg_memory = total_used / gpu_count
                logger.info(f"ğŸ” {step_name}: å¹³å‡ {avg_memory:.0f}MB/GPU")
                return avg_memory
    except Exception as e:
        logger.warning(f"å†…å­˜ç›‘æ§å¤±è´¥: {e}")
    return 0


def create_direct_fsdp_train_state(
    rl_config,
    mesh: jax.sharding.Mesh,
    rng: jax.Array,
    global_pi0_tx: optax.GradientTransformation = None,
    global_critic_tx: optax.GradientTransformation = None
) -> Tuple[Any, Any, Callable]:
    """
    ä½¿ç”¨OpenPIæ ‡å‡†æ¨¡å¼åˆ›å»ºFSDPè®­ç»ƒçŠ¶æ€
    
    è¿™æ˜¯ä¿®å¤8å¡FSDPå†…å­˜çˆ†ç‚¸çš„æ ¸å¿ƒå‡½æ•°ï¼Œå®Œå…¨é‡‡ç”¨OpenPIçš„init_train_stateæ¨¡å¼ï¼š
    1. ä½¿ç”¨jax.eval_shapeè·å–ç»“æ„ï¼ˆæ— å†…å­˜åˆ†é…ï¼‰
    2. ä½¿ç”¨fsdp_shardingè®¡ç®—åˆ†ç‰‡ç­–ç•¥
    3. ä½¿ç”¨jax.jit(out_shardings=...)ç›´æ¥åˆ›å»ºåˆ†ç‰‡å‚æ•°
    """
    from agents.acrlpd_pi0_agent import create_acrlpd_pi0_agent_from_rl_config
    from training.acrlpd_train_state import ACRLPDTrainState, acrlpd_train_step
    
    logger.info("ğŸš€ å¼€å§‹OpenPIæ ‡å‡†FSDPåˆå§‹åŒ–ï¼ˆä¿®å¤å†…å­˜çˆ†ç‚¸ï¼‰")
    log_memory_usage("FSDPåˆå§‹åŒ–å‰")
    
    # **å…³é”®ä¿®å¤ï¼šä½¿ç”¨å…¨å±€ä¼˜åŒ–å™¨é¿å…pytreeå…ƒæ•°æ®ä¸åŒ¹é…**
    if global_pi0_tx is None or global_critic_tx is None:
        logger.warning("è­¦å‘Šï¼šæœªæä¾›å…¨å±€ä¼˜åŒ–å™¨ï¼Œå°†åˆ›å»ºæ–°å®ä¾‹ï¼ˆå¯èƒ½å¯¼è‡´pytreeé—®é¢˜ï¼‰")
        pi0_tx = _optimizer.create_optimizer(rl_config.actor_optimizer, rl_config.get_effective_actor_lr_schedule())
        critic_tx = _optimizer.create_optimizer(rl_config.critic_optimizer, rl_config.get_effective_critic_lr_schedule())
    else:
        logger.info("âœ… ä½¿ç”¨å…¨å±€ä¼˜åŒ–å™¨å®ä¾‹ï¼Œç¡®ä¿pytreeä¸€è‡´æ€§")
        pi0_tx = global_pi0_tx
        critic_tx = global_critic_tx
    
    # Step 1: å®šä¹‰åˆå§‹åŒ–å‡½æ•°ï¼ˆå®Œå…¨æ¨¡ä»¿OpenPIï¼‰
    def init_fn(rng: jax.Array, partial_params: Any = None) -> ACRLPDTrainState:
        """
        åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€ - è¿™ä¸ªå‡½æ•°ä¼šè¢«JITç¼–è¯‘å¹¶ç›´æ¥è¾“å‡ºåˆ†ç‰‡å‚æ•°
        
        å…³é”®ç‚¹ï¼šå½“ä½¿ç”¨out_shardingsæ—¶ï¼Œè¿™ä¸ªå‡½æ•°çš„è¾“å‡ºå°†ç›´æ¥ä»¥åˆ†ç‰‡å½¢å¼åˆ›å»º
        é¿å…äº†å…ˆåˆ›å»ºå®Œæ•´å‚æ•°å†åˆ†ç‰‡çš„å†…å­˜çˆ†ç‚¸é—®é¢˜
        """
        # åˆ›å»ºagentï¼ˆå’Œä¹‹å‰ä¸€æ ·ï¼‰
        agent = create_acrlpd_pi0_agent_from_rl_config(rl_config, rng)
        
        # å¦‚æœæœ‰éƒ¨åˆ†å‚æ•°ï¼Œåˆå¹¶è¿›å»ï¼ˆOpenPIæ¨¡å¼ï¼‰
        if partial_params is not None:
            # TODO: å®ç°å‚æ•°åˆå¹¶é€»è¾‘ï¼Œå½“å‰ä¸ºNoneè·³è¿‡
            pass
        
        # è·å–å‚æ•°å’Œç»„ä»¶å®šä¹‰  
        pi0_params = nnx.state(agent.pi0_model)
        pi0_model_def = nnx.graphdef(agent.pi0_model)
        critic_params = nnx.state(agent.critic_networks)
        critic_model_def = nnx.graphdef(agent.critic_networks)
        
        # å…³é”®ä¿®å¤ï¼šå¦‚æœæœ‰é¢„è®­ç»ƒæƒé‡ï¼Œåˆå¹¶è¿›å»
        if partial_params is not None:
            logger.info("ğŸ”„ åˆå¹¶é¢„è®­ç»ƒæƒé‡åˆ°Ï€â‚€æ¨¡å‹...")
            graphdef, state = nnx.split(agent.pi0_model)
            # åªåˆå¹¶Ï€â‚€ç›¸å…³çš„æƒé‡
            pi0_weights = {k: v for k, v in partial_params.items() if 'pi0' in k or not any(x in k for x in ['critic', 'temperature'])}
            if pi0_weights:
                state.replace_by_pure_dict(pi0_weights)
                agent.pi0_model = nnx.merge(graphdef, state)
                pi0_params = nnx.state(agent.pi0_model)
                logger.info("âœ… Ï€â‚€é¢„è®­ç»ƒæƒé‡åˆå¹¶æˆåŠŸ")
        
        # åˆ›å»ºä¼˜åŒ–å™¨çŠ¶æ€
        pi0_opt_state = pi0_tx.init(pi0_params)
        critic_opt_state = critic_tx.init(critic_params)
        
        # æ¸©åº¦æ¨¡å—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        temp_params = None
        temp_model_def = None
        temp_opt_state = None
        if hasattr(agent, 'temperature_module') and agent.temperature_module:
            temp_params = nnx.state(agent.temperature_module)
            temp_model_def = nnx.graphdef(agent.temperature_module)
            temp_tx = _optimizer.create_optimizer(rl_config.actor_optimizer, rl_config.get_effective_actor_lr_schedule())
            temp_opt_state = temp_tx.init(temp_params)
        else:
            temp_tx = None
        
        # è¿”å›è®­ç»ƒçŠ¶æ€ï¼ˆå…³é”®ï¼šå½“ç”¨out_shardingsæ—¶ï¼Œè¿™ä¸ªè¿”å›å€¼å°†ç›´æ¥åˆ†ç‰‡åˆ›å»ºï¼‰
        # ä¿®å¤ï¼šç¡®ä¿EMA decayä¸å…¶ä»–åˆ›å»ºç‚¹ä¿æŒä¸€è‡´ï¼ŒåŒ…æ‹¬use_emaæ£€æŸ¥
        use_ema = getattr(rl_config.acrlpd, 'use_ema', True)
        pi0_ema_decay_value = getattr(rl_config.acrlpd, 'pi0_ema_decay', 0.999) if use_ema else None
        return ACRLPDTrainState(
            step=0,
            pi0_params=pi0_params,
            pi0_model_def=pi0_model_def,
            pi0_opt_state=pi0_opt_state,
            pi0_tx=pi0_tx,
            critic_params=critic_params,
            critic_model_def=critic_model_def,
            critic_opt_state=critic_opt_state,
            critic_tx=critic_tx,
            pi0_ema_decay=pi0_ema_decay_value,  # å…³é”®ä¿®å¤ï¼šæ·»åŠ ç¼ºå¤±çš„EMA decayå‚æ•°
            pi0_ema_params=pi0_params,  # EMAå‚æ•°å¼•ç”¨ï¼ˆæ— é¢å¤–å†…å­˜ï¼‰
            temperature_params=temp_params,
            temperature_model_def=temp_model_def,
            temperature_opt_state=temp_opt_state,
            temperature_tx=temp_tx,
            config={}
        )
    
    logger.info("ğŸ“ Step 1: ä½¿ç”¨eval_shapeè·å–è®­ç»ƒçŠ¶æ€ç»“æ„...")
    # Step 2: è·å–ç»“æ„ï¼ˆæ— å†…å­˜åˆ†é…ï¼ŒOpenPIæ ‡å‡†æ¨¡å¼ï¼‰
    train_state_shape = jax.eval_shape(init_fn, rng, None)
    log_memory_usage("eval_shapeå")
    
    logger.info("ğŸ¯ Step 2: è®¡ç®—FSDPåˆ†ç‰‡ç­–ç•¥...")
    # Step 3: è®¡ç®—FSDPåˆ†ç‰‡ç­–ç•¥ï¼ˆOpenPIæ ‡å‡†æ¨¡å¼ï¼‰
    state_sharding = sharding.fsdp_sharding(train_state_shape, mesh, log=True)
    
    logger.info("âš¡ Step 3: JITç¼–è¯‘initå‡½æ•°ä¸out_shardings...")
    # Step 4: åˆ›å»ºåˆ†ç‰‡è§„æ ¼
    replicated_sharding = jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec())
    
    # Step 5: å…³é”®ä¿®å¤ - JITç¼–è¯‘æ—¶æŒ‡å®šout_shardings
    # è¿™ç¡®ä¿å‚æ•°ç›´æ¥ä»¥åˆ†ç‰‡å½¢å¼åˆ›å»ºï¼Œè€Œä¸æ˜¯å…ˆåˆ›å»ºå®Œæ•´å†åˆ†ç‰‡
    init_fn_sharded = jax.jit(
        init_fn,
        donate_argnums=(1,),  # å¦‚æœæœ‰partial_paramså¯ä»¥donate
        in_shardings=(replicated_sharding, replicated_sharding),  # RNGå’Œpartial_paramséƒ½æ˜¯replicated
        out_shardings=state_sharding  # ğŸ”¥ å…³é”®ï¼šè¾“å‡ºç›´æ¥åˆ†ç‰‡åˆ›å»ºï¼
    )
    
    logger.info("ğŸ’« Step 4: åˆ›å»ºåˆ†ç‰‡è®­ç»ƒçŠ¶æ€ï¼ˆå‚æ•°born shardedï¼‰...")
    # Step 6: åœ¨meshä¸Šä¸‹æ–‡ä¸­åˆ›å»ºåˆ†ç‰‡è®­ç»ƒçŠ¶æ€
    with sharding.set_mesh(mesh):
        train_state = init_fn_sharded(rng, None)
    
    # ç­‰å¾…å®Œæˆå¹¶æ£€æŸ¥å†…å­˜
    jax.block_until_ready(train_state)
    memory_after = log_memory_usage("FSDPåˆ›å»ºå")
    
    # è®°å½•FSDPå†…å­˜ä½¿ç”¨æƒ…å†µ
    logger.info(f"ğŸ’¾ FSDPå†…å­˜ä½¿ç”¨: {memory_after:.0f}MB/GPU")
    
    # Step 7: åˆ›å»ºè®­ç»ƒæ­¥éª¤å‡½æ•°
    def train_step_wrapper(train_state, batch, rng):
        """åŒ…è£…è®­ç»ƒæ­¥éª¤ï¼Œä¿æŒæ¥å£ä¸€è‡´"""
        return acrlpd_train_step(train_state, batch, rng, {
            'critic_weight': getattr(rl_config.acrlpd, 'critic_weight', 1.0),
            'actor_weight': getattr(rl_config.acrlpd, 'actor_weight', 1.0),
            'bc_weight': getattr(rl_config.acrlpd, 'bc_loss_weight', 0.01),
            'alpha_weight': getattr(rl_config.acrlpd, 'alpha_weight', 1.0),
            'freeze_pi0_backbone': False,
            'target_update_tau': getattr(rl_config.acrlpd, 'target_update_tau', 0.005)
        })
    
    # åˆ›å»ºæ•°æ®åˆ†ç‰‡
    data_sharding = jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec(sharding.DATA_AXIS))
    
    # JITç¼–è¯‘è®­ç»ƒæ­¥éª¤
    jit_train_step = jax.jit(
        train_step_wrapper,
        in_shardings=(state_sharding, data_sharding, replicated_sharding),
        out_shardings=(state_sharding, replicated_sharding),
        donate_argnums=()  # é¿å…donationé—®é¢˜
    )
    
    # åŒ…è£…æœ€ç»ˆè®­ç»ƒå‡½æ•°
    def fsdp_train_step(train_state, batch, rng):
        """FSDPè®­ç»ƒæ­¥éª¤ï¼Œåœ¨meshä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œ"""
        with sharding.set_mesh(mesh):
            return jit_train_step(train_state, batch, rng)
    
    logger.info("ğŸ‰ OpenPIæ ‡å‡†FSDPè®­ç»ƒçŠ¶æ€åˆ›å»ºå®Œæˆ")
    
    return train_state, state_sharding, fsdp_train_step


def test_direct_fsdp_memory_usage():
    """
    æµ‹è¯•direct FSDPä¿®å¤æ•ˆæœ
    
    è¿™ä¸ªæµ‹è¯•åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„FSDPè®¾ç½®æ¥éªŒè¯å†…å­˜ä½¿ç”¨æ˜¯å¦æ­£å¸¸
    """
    logger.info("ğŸ§ª æµ‹è¯•Direct FSDPå†…å­˜æ•ˆæœ...")
    
    # åˆ›å»ºæµ‹è¯•mesh
    mesh = sharding.make_mesh(8)
    
    # æµ‹è¯•ç”¨ç®€å•å‚æ•°ç»“æ„
    def create_test_params(rng):
        # åˆ›å»ºä¸€ä¸ªå¤§å‚æ•°æ•°ç»„æ¨¡æ‹ŸÏ€â‚€å‚æ•°
        return {
            'large_param': jnp.ones((4096, 4096), dtype=jnp.float32),  # 64MB
            'medium_param': jnp.ones((1024, 1024), dtype=jnp.float32),  # 4MB
            'small_param': jnp.ones((100, 100), dtype=jnp.float32),  # 0.04MB
        }
    
    # è·å–ç»“æ„
    param_shape = jax.eval_shape(create_test_params, jax.random.PRNGKey(0))
    
    # è®¡ç®—åˆ†ç‰‡ç­–ç•¥
    param_sharding = sharding.fsdp_sharding(param_shape, mesh, log=True)
    
    # ç”¨out_shardingsåˆ›å»º
    create_sharded = jax.jit(
        create_test_params,
        in_shardings=(jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec()),),
        out_shardings=param_sharding
    )
    
    memory_before = log_memory_usage("æµ‹è¯•å‰")
    
    with sharding.set_mesh(mesh):
        test_params = create_sharded(jax.random.PRNGKey(0))
    
    jax.block_until_ready(test_params)
    memory_after = log_memory_usage("æµ‹è¯•å")
    
    memory_increase = memory_after - memory_before
    
    # é¢„æœŸï¼š68MBå‚æ•° / 8 GPU â‰ˆ 8.5MB/GPU
    expected_per_gpu = 8.5
    
    if memory_increase < expected_per_gpu * 5:  # å…è®¸5å€è¯¯å·®
        logger.info(f"âœ… æµ‹è¯•é€šè¿‡: {memory_increase:.1f}MB/GPU (é¢„æœŸ~{expected_per_gpu}MB)")
        return True
    else:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {memory_increase:.1f}MB/GPU >> {expected_per_gpu}MB")
        return False


if __name__ == "__main__":
    """æµ‹è¯•å…¥å£"""
    logging.basicConfig(level=logging.INFO)
    
    logger.info("=" * 60)
    logger.info("Direct FSDPä¿®å¤æµ‹è¯•")
    logger.info("=" * 60)
    
    test_direct_fsdp_memory_usage()