"""
Training Loop for ACRLPD + Ï€â‚€ Integration.

This module provides the complete training system for ACRLPD + Ï€â‚€ agents,
including offline pretraining, online fine-tuning, evaluation, and monitoring.

Key features:
- Offline data pretraining with behavior cloning regularization
- Online environment interaction and fine-tuning
- Comprehensive evaluation and metrics tracking
- Checkpoint management and recovery
- WandB integration for experiment monitoring
- Multi-GPU training support
"""

import logging
import os
import time
from typing import Dict, Any, Optional, Callable, Tuple
import dataclasses
from pathlib import Path

import jax
import jax.numpy as jnp
import numpy as np
import wandb
# tqdm removed for professional logging
import math

import sys

# Add ac_training root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from agents.acrlpd_pi0_agent import ACRLPDPi0Agent
from agents.loss_functions import LossInfo
from data import ACRLPDDataLoader
from config import RLTrainConfig
from utils.memory_monitor import enable_memory_monitoring, log_memory_usage
import openpi.models.model as _model
import openpi.training.checkpoints as _checkpoints
import openpi.training.sharding as sharding

logger = logging.getLogger(__name__)
# ç¡®ä¿loggerèƒ½æ­£ç¡®è¾“å‡ºåˆ°console
logger.setLevel(logging.INFO)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(levelname)s:%(name)s:%(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)


def create_dynamic_lr_schedule(
    base_lr: float, 
    total_steps: int, 
    warmup_epochs: int, 
    total_epochs: int, 
    steps_per_epoch: int,
    lr_min_factor: float = 0.1,
    intra_epoch_min_factor: float = 0.9,
    lr_absolute_min: float = 1e-7  # å­¦ä¹ ç‡ç»å¯¹ä¸‹é™
):
    """
    åˆ›å»ºåŒå±‚cosineå­¦ä¹ ç‡è°ƒèŠ‚å‡½æ•°ï¼šEpoché—´ + Epochå†…
    
    Args:
        base_lr: åŸºç¡€å­¦ä¹ ç‡ï¼ˆpeakå€¼ï¼‰
        total_steps: æ€»è®­ç»ƒæ­¥æ•°
        warmup_epochs: é¢„çƒ­epochæ•°
        total_epochs: æ€»epochæ•°
        steps_per_epoch: æ¯ä¸ªepochæ­¥æ•°
        lr_min_factor: Epoché—´æœ€å°å­¦ä¹ ç‡å› å­
        intra_epoch_min_factor: Epochå†…æœ€å°å­¦ä¹ ç‡å› å­
        lr_absolute_min: å­¦ä¹ ç‡ç»å¯¹ä¸‹é™
    
    Returns:
        å­¦ä¹ ç‡è°ƒèŠ‚å‡½æ•° step -> learning_rate
    """
    def schedule(step):
        import jax
        import jax.numpy as jnp
        
        # è®¡ç®—å½“å‰epochå’Œepochå†…æ­¥æ•°
        current_epoch = step // steps_per_epoch
        step_in_epoch = step % steps_per_epoch
        
        # Epoché—´è°ƒèŠ‚ï¼šä½¿ç”¨JAXæ¡ä»¶å‡½æ•°
        def warmup_lr():
            return base_lr * (current_epoch + 1) / warmup_epochs
        
        def normal_lr():
            effective_epoch = current_epoch - warmup_epochs
            effective_total_epochs = total_epochs - warmup_epochs
            
            # ä½¿ç”¨JAXæ¡ä»¶å‡½æ•°é¿å…å¸ƒå°”è½¬æ¢
            cosine_progress = jnp.minimum(effective_epoch / jnp.maximum(effective_total_epochs, 1.0), 1.0)
            epoch_factor = lr_min_factor + (1 - lr_min_factor) * \
                0.5 * (1 + jnp.cos(jnp.pi * cosine_progress))
            return base_lr * epoch_factor
        
        # ä½¿ç”¨JAXæ¡ä»¶å‡½æ•°
        epoch_lr_base = jax.lax.cond(
            current_epoch < warmup_epochs,
            warmup_lr,
            normal_lr
        )
        
        # Epochå†…è°ƒèŠ‚ï¼šä½¿ç”¨JAXå‡½æ•°
        intra_progress = jnp.minimum(step_in_epoch / jnp.maximum(steps_per_epoch, 1.0), 1.0)
        intra_factor = intra_epoch_min_factor + (1 - intra_epoch_min_factor) * \
            0.5 * (1 + jnp.cos(jnp.pi * intra_progress))
        
        # æœ€ç»ˆå­¦ä¹ ç‡
        final_lr = epoch_lr_base * intra_factor
        
        # åº”ç”¨ç»å¯¹ä¸‹é™
        return jnp.maximum(final_lr, lr_absolute_min)
    
    return schedule


@dataclasses.dataclass
class ACRLPDTrainingConfig:
    """é¢å¤–çš„è®­ç»ƒå¾ªç¯å‚æ•°ï¼Œé…åˆRLTrainConfigä½¿ç”¨"""
    
    # è¯„ä¼°å’Œæ—¥å¿— (æ³¨æ„ï¼šç°åœ¨ä½¿ç”¨RLTrainConfig.num_train_stepsä½œä¸ºæ€»æ­¥æ•°)
    # eval_frequencyå·²ç§»é™¤ - å½“å‰æ— çœŸå®ç¯å¢ƒè¯„ä¼°å®ç°
    num_eval_episodes: int = 10            # æ¯æ¬¡è¯„ä¼°çš„episodeæ•°ï¼ˆå½“å‰æœªä½¿ç”¨ï¼‰
    eval_batch_size: int = 64              # è¯„ä¼°æ‰¹æ¬¡å¤§å°
    
    # è®­ç»ƒè¡Œä¸ºé…ç½®ï¼ˆå·²ç§»è‡³RLTrainConfig.acrlpdä¸­ï¼‰
    early_stopping_patience: int = 50      # æ—©åœè€å¿ƒå€¼
    
    # ç¯å¢ƒé…ç½®ï¼ˆåœ¨çº¿è®­ç»ƒç”¨ï¼‰
    env_name: Optional[str] = None         # ç¯å¢ƒåç§°
    max_episode_steps: int = 200           # æ¯episodeæœ€å¤§æ­¥æ•°


class TrainingMetrics:
    """Tracks training metrics and statistics."""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        """Reset all metrics."""
        self.metrics = {}
        self.step_count = 0
        self.episode_rewards = []
        self.episode_lengths = []
        self.best_eval_reward = -float('inf')
        self.steps_since_improvement = 0
    
    def update(self, step: int, loss_info: LossInfo, **kwargs):
        """Update metrics with new training step data."""
        self.step_count = step
        
        # Loss metrics
        self.metrics.update({
            'train/total_loss': float(loss_info.total_loss),
            'train/critic_loss': float(loss_info.critic_loss),
            'train/bc_loss': float(loss_info.bc_loss),
            # Ï€â‚€ loss is included in bc_loss
            'train/alpha_loss': float(loss_info.alpha_loss),
            'train/q_mean': float(loss_info.q_mean),
            'train/q_std': float(loss_info.q_std),
            'train/target_q_mean': float(loss_info.target_q_mean),
            'train/td_error_mean': float(loss_info.td_error_mean),
            'train/alpha_value': float(loss_info.alpha_value),
            'train/entropy_estimate': float(loss_info.entropy_estimate),
            'train/valid_samples': float(loss_info.valid_samples),
            'train/mask_ratio': float(loss_info.mask_ratio)
        })
        
        # Additional metrics
        self.metrics.update(kwargs)
        self.metrics['step'] = step
    
    def add_episode_result(self, reward: float, length: int):
        """Add episode result for evaluation."""
        self.episode_rewards.append(reward)
        self.episode_lengths.append(length)
    
    def get_episode_stats(self) -> Dict[str, float]:
        """Get episode statistics."""
        if not self.episode_rewards:
            return {}
        
        return {
            'eval/episode_reward_mean': np.mean(self.episode_rewards),
            'eval/episode_reward_std': np.std(self.episode_rewards),
            'eval/episode_reward_max': np.max(self.episode_rewards),
            'eval/episode_reward_min': np.min(self.episode_rewards),
            'eval/episode_length_mean': np.mean(self.episode_lengths),
            'eval/num_episodes': len(self.episode_rewards)
        }
    
    def check_improvement(self, eval_reward: float) -> bool:
        """Check if evaluation reward improved."""
        if eval_reward > self.best_eval_reward:
            self.best_eval_reward = eval_reward
            self.steps_since_improvement = 0
            return True
        else:
            self.steps_since_improvement += 1
            return False


class ACRLPDCheckpointManager:
    """ACRLPD-specific wrapper for OpenPI checkpoint management."""
    
    def __init__(self, checkpoint_dir: str, keep_period: int = 10000, overwrite: bool = False, resume: bool = False):
        self.checkpoint_dir = Path(checkpoint_dir)
        
        # Initialize OpenPI checkpoint manager
        self.manager, self.resuming = _checkpoints.initialize_checkpoint_dir(
            checkpoint_dir=self.checkpoint_dir,
            keep_period=keep_period,
            overwrite=overwrite,
            resume=resume
        )
        
        # ğŸ” æ–°å¢ï¼šè‡ªåŠ¨æ£€æµ‹å¯resumeçš„checkpoint
        self.auto_resume_path = None
        if resume and not overwrite:
            self.auto_resume_path = self._find_latest_checkpoint()
    
    def save_checkpoint(
        self,
        agent: ACRLPDPi0Agent,
        dataloader: Any,
        step: int
    ) -> str:
        """
        Save agent checkpoint with dual strategy:
        1. OpenPI-compatible format (Ï€â‚€ only) for inference 
        2. Complete component state for training recovery
        """
        try:
            # ğŸ” å…³é”®ä¿®å¤ï¼šæ£€æŸ¥checkpointè·¯å¾„æ˜¯å¦å·²å­˜åœ¨
            checkpoint_path = str(self.checkpoint_dir / str(step))
            if os.path.exists(checkpoint_path):
                logger.info(f"Checkpoint {checkpoint_path} already exists, skipping duplicate save")
                return checkpoint_path
            # === STRATEGY 1: OpenPI-compatible format (Ï€â‚€ only) ===
            # This creates standard params/ directory that can be used for inference
            train_state = agent.create_train_state()
            
            _checkpoints.save_state(
                checkpoint_manager=self.manager,
                state=train_state,
                data_loader=dataloader,
                step=step
            )
            logger.info(f"Saved OpenPI-compatible Ï€â‚€ weights at step {step}")
            
            # === STRATEGY 2: Complete component state ===
            # Save all components independently for training recovery
            components_dir = self.checkpoint_dir / str(step) / "components"
            agent.save_component_checkpoints(str(components_dir), step)
            logger.info(f"Saved complete component state at step {step}")
            
            checkpoint_path = str(self.checkpoint_dir / str(step))
            logger.info(f"Dual checkpoint saved: {checkpoint_path}")
            logger.info("  - params/: Ï€â‚€ weights (OpenPI inference compatible)")
            logger.info("  - components/: All components (training recovery)")
            
            return checkpoint_path
            
        except Exception as e:
            logger.error(f"Failed to save checkpoint at step {step}: {e}")
            return ""
    
    def load_checkpoint(
        self, 
        agent: ACRLPDPi0Agent, 
        dataloader: Any,
        step: Optional[int] = None
    ) -> Tuple[ACRLPDPi0Agent, int, Optional[Dict]]:
        """
        Load agent checkpoint with intelligent strategy:
        1. Try loading complete component state first (preferred for training)
        2. Fallback to OpenPI format (Ï€â‚€ only, for inference compatibility)
        
        Returns:
            Tuple[agent, step, saved_metrics]: Updated agent, checkpoint step, and optional metrics
        """
        try:
            # Determine step to load
            if step is None:
                available_steps = list(self.manager.all_steps())
                if not available_steps:
                    raise ValueError("No checkpoints found")
                step = max(available_steps)
            
            components_dir = self.checkpoint_dir / str(step) / "components"
            
            # === STRATEGY 1: Try complete component state first ===
            if components_dir.exists():
                logger.info(f"Found complete component state at step {step}")
                
                # Check if it's orbax format (preferred) or legacy pickle format
                pi0_dir = components_dir / "pi0"
                if (pi0_dir / "params").exists():
                    # New orbax format
                    agent.load_component_checkpoints(str(components_dir))
                    logger.info(f"Loaded complete training state (orbax format) from step {step}")
                elif (pi0_dir / "model_params.pkl").exists():
                    # Legacy pickle format
                    logger.warning("Loading from legacy pickle format - consider migrating to orbax")
                    agent.load_legacy_component_checkpoints(str(components_dir))
                    logger.info(f"Loaded complete training state (legacy format) from step {step}")
                else:
                    logger.error(f"Unrecognized checkpoint format at {components_dir}")
                    raise ValueError(f"Cannot determine checkpoint format at {components_dir}")
                
                # ğŸ” å°è¯•åŠ è½½metrics (å®¹é”™å¤„ç†)
                saved_metrics = None
                try:
                    metrics_path = self.checkpoint_dir / str(step) / "metrics" / "metrics"
                    if metrics_path.exists():
                        # TODO: å®ç°metricsåŠ è½½é€»è¾‘
                        logger.debug(f"Found metrics file at {metrics_path}")
                    else:
                        logger.debug(f"No metrics file found at {metrics_path}")
                except Exception as e:
                    logger.warning(f"Failed to load metrics for step {step}: {e}")
                    
                return agent, step, saved_metrics
            
            # === STRATEGY 2: Fallback to OpenPI format ===
            logger.info(f"Complete component state not found, using OpenPI format")
            logger.warning("Loading Ï€â‚€ only - critic and temperature states will be reinitialized")
            
            # Create template training state
            template_state = agent.create_train_state()
            
            # Restore using OpenPI checkpoint system
            restored_state = _checkpoints.restore_state(
                checkpoint_manager=self.manager,
                state=template_state,
                data_loader=dataloader,
                step=step
            )
            
            # Update agent from restored state (Ï€â‚€ only)
            agent.update_from_train_state(restored_state)
            
            logger.info(f"Loaded Ï€â‚€ weights from OpenPI format at step {restored_state.step}")
            
            # ğŸ” å°è¯•åŠ è½½metrics (å®¹é”™å¤„ç†)
            saved_metrics = None
            try:
                metrics_path = self.checkpoint_dir / str(restored_state.step) / "metrics" / "metrics"
                if metrics_path.exists():
                    # TODO: å®ç°metricsåŠ è½½é€»è¾‘
                    logger.debug(f"Found metrics file at {metrics_path}")
                else:
                    logger.debug(f"No metrics file found at {metrics_path}")
            except Exception as e:
                logger.warning(f"Failed to load metrics for step {restored_state.step}: {e}")
            
            return agent, restored_state.step, saved_metrics
            
        except Exception as e:
            logger.error(f"Failed to load checkpoint: {e}")
            raise
    
    def _find_latest_checkpoint(self) -> Optional[str]:
        """æŸ¥æ‰¾æœ€æ–°çš„å¯ç”¨checkpoint"""
        if not self.checkpoint_dir.exists():
            return None
            
        available_steps = []
        for step_dir in self.checkpoint_dir.iterdir():
            if step_dir.is_dir() and step_dir.name.isdigit():
                step = int(step_dir.name)
                # éªŒè¯checkpointå®Œæ•´æ€§
                if self._validate_checkpoint(step_dir):
                    available_steps.append(step)
        
        if available_steps:
            latest_step = max(available_steps)
            checkpoint_path = str(self.checkpoint_dir / str(latest_step))
            logger.info(f"ğŸ” Found latest checkpoint: {checkpoint_path}")
            return checkpoint_path
        else:
            logger.info("ğŸ” No valid checkpoints found for auto-resume")
            return None
    
    def _validate_checkpoint(self, checkpoint_dir: Path) -> bool:
        """éªŒè¯checkpointå®Œæ•´æ€§"""
        # æ£€æŸ¥å¿…éœ€çš„æ–‡ä»¶/ç›®å½•æ˜¯å¦å­˜åœ¨
        components_dir = checkpoint_dir / "components"
        params_dir = checkpoint_dir / "params"
        
        is_valid = components_dir.exists() or params_dir.exists()
        if is_valid:
            logger.debug(f"âœ… Valid checkpoint: {checkpoint_dir.name}")
        else:
            logger.debug(f"âŒ Invalid checkpoint: {checkpoint_dir.name}")
        
        return is_valid


class ACRLPDTrainer:
    """Complete training system for ACRLPD + Ï€â‚€ agents with OpenPI integration."""
    
    def __init__(
        self,
        agent: Optional[ACRLPDPi0Agent],
        dataloader: ACRLPDDataLoader,
        rl_config: RLTrainConfig,
        training_config: ACRLPDTrainingConfig,
        eval_fn: Optional[Callable] = None,
        # FSDPå‚æ•°
        mesh: Optional[jax.sharding.Mesh] = None,
        data_sharding: Optional[jax.sharding.Sharding] = None,
        replicated_sharding: Optional[jax.sharding.Sharding] = None,
        agent_rng: Optional[jax.Array] = None,
        # å…¨å±€ä¼˜åŒ–å™¨å‚æ•°ï¼ˆä¿®å¤pytreeä¸€è‡´æ€§ï¼‰
        global_pi0_tx: Optional[Any] = None,
        global_critic_tx: Optional[Any] = None
    ):
        self.agent = agent  # May be None initially
        self.dataloader = dataloader
        self.rl_config = rl_config
        self.training_config = training_config
        self.eval_fn = eval_fn
        self.agent_rng = agent_rng if agent_rng is not None else jax.random.PRNGKey(42)
        
        # FSDP support
        self.mesh = mesh
        self.data_sharding = data_sharding
        self.replicated_sharding = replicated_sharding
        self.use_fsdp = mesh is not None
        
        # å…¨å±€ä¼˜åŒ–å™¨ï¼ˆä¿®å¤pytreeä¸€è‡´æ€§ï¼‰
        self.global_pi0_tx = global_pi0_tx
        self.global_critic_tx = global_critic_tx
        
        # Training state (initialize before FSDP setup)
        self.rng = jax.random.PRNGKey(42)
        self.current_step = 0
        self.is_online_phase = False
        
        if self.use_fsdp:
            logger.info(f"FSDP enabled with mesh: {mesh}")
            # FSDP setup will be done later after main script provides components
            logger.info("FSDP components will be provided by main script")
        else:
            logger.info("FSDP disabled - creating agent for single device training")
            if self.agent is None:
                from agents.acrlpd_pi0_agent import create_acrlpd_pi0_agent_from_rl_config
                self.agent = create_acrlpd_pi0_agent_from_rl_config(rl_config, self.agent_rng)
        
        # Initialize systems
        self.metrics = TrainingMetrics()
        
        # Cache learning rate schedules for efficient logging (avoid recreating optax schedules)
        self._cached_actor_schedule = self.rl_config.get_effective_actor_lr_schedule().create()
        self._cached_critic_schedule = self.rl_config.get_effective_critic_lr_schedule().create()
        self.checkpoint_manager = ACRLPDCheckpointManager(
            checkpoint_dir=str(rl_config.checkpoint_dir),
            keep_period=rl_config.keep_period or 10000,
            overwrite=rl_config.overwrite,
            resume=rl_config.resume
        )
        
        # Initialize WandB if enabled
        if rl_config.wandb_enabled:
            self._init_wandb()
    
    def _init_wandb(self):
        """Initialize Weights & Biases logging."""
        try:
            wandb.init(
                project=self.rl_config.project_name,
                name=self.rl_config.exp_name,
                config={
                    'rl_config': dataclasses.asdict(self.rl_config),
                    'training_config': dataclasses.asdict(self.training_config),
                    'agent_config': dataclasses.asdict(self.agent.config) if hasattr(self.agent, 'config') else {}
                }
            )
            logger.info("Initialized WandB logging")
        except Exception as e:
            logger.warning(f"Failed to initialize WandB: {e}")
            # Disable wandb in rl_config by creating a copy
            object.__setattr__(self.rl_config, 'wandb_enabled', False)
    
    def _get_current_learning_rates(self):
        """è·å–çœŸå®çš„åŠ¨æ€å­¦ä¹ ç‡"""
        acrlpd_config = self.rl_config.acrlpd
        current_step = self.current_step
        
        if acrlpd_config.enable_epoch_based_lr_schedule:
            # ä½¿ç”¨åŠ¨æ€è®¡ç®—
            actor_schedule = create_dynamic_lr_schedule(
                base_lr=acrlpd_config.actor_lr,  # ä½¿ç”¨é…ç½®ä¸­çš„actor_lr
                total_steps=self.rl_config.num_train_steps,
                warmup_epochs=acrlpd_config.warmup_epochs,
                total_epochs=acrlpd_config.total_epochs,
                steps_per_epoch=acrlpd_config.steps_per_epoch,
                lr_min_factor=acrlpd_config.lr_min_factor,
                intra_epoch_min_factor=acrlpd_config.intra_epoch_min_factor,
                lr_absolute_min=getattr(acrlpd_config, 'lr_absolute_min', 1e-7)
            )
            
            critic_schedule = create_dynamic_lr_schedule(
                base_lr=acrlpd_config.critic_lr,  # ä½¿ç”¨é…ç½®ä¸­çš„critic_lr
                total_steps=self.rl_config.num_train_steps,
                warmup_epochs=acrlpd_config.warmup_epochs,
                total_epochs=acrlpd_config.total_epochs,
                steps_per_epoch=acrlpd_config.steps_per_epoch,
                lr_min_factor=acrlpd_config.lr_min_factor,
                intra_epoch_min_factor=acrlpd_config.intra_epoch_min_factor,
                lr_absolute_min=getattr(acrlpd_config, 'lr_absolute_min', 1e-7)
            )
            
            actor_lr = actor_schedule(current_step)
            critic_lr = critic_schedule(current_step)
            
            # è®¡ç®—è°ƒèŠ‚å› å­ç”¨äºè¯¦ç»†æ—¥å¿—
            current_epoch = current_step // acrlpd_config.steps_per_epoch
            step_in_epoch = current_step % acrlpd_config.steps_per_epoch
            
            # è®¡ç®—epochå› å­
            if current_epoch < acrlpd_config.warmup_epochs:
                epoch_factor = (current_epoch + 1) / acrlpd_config.warmup_epochs
            else:
                effective_epoch = current_epoch - acrlpd_config.warmup_epochs
                effective_total_epochs = acrlpd_config.total_epochs - acrlpd_config.warmup_epochs
                
                if effective_total_epochs > 0:
                    cosine_progress = min(effective_epoch / effective_total_epochs, 1.0)
                    epoch_factor = acrlpd_config.lr_min_factor + (1 - acrlpd_config.lr_min_factor) * \
                        0.5 * (1 + math.cos(math.pi * cosine_progress))
                else:
                    epoch_factor = 1.0
            
            # è®¡ç®—intra-epochå› å­
            if acrlpd_config.steps_per_epoch > 0:
                intra_progress = min(step_in_epoch / acrlpd_config.steps_per_epoch, 1.0)
                intra_epoch_factor = acrlpd_config.intra_epoch_min_factor + \
                    (1 - acrlpd_config.intra_epoch_min_factor) * 0.5 * (1 + math.cos(math.pi * intra_progress))
            else:
                intra_epoch_factor = 1.0
                
        else:
            # ä½¿ç”¨OpenPIæ ‡å‡†å­¦ä¹ ç‡è°ƒåº¦ï¼ˆstep-basedè°ƒèŠ‚ï¼‰
            # ä»ç¼“å­˜çš„è°ƒåº¦å™¨è·å–å½“å‰æ­¥éª¤çš„çœŸå®å­¦ä¹ ç‡
            actor_lr = float(self._cached_actor_schedule(current_step))
            critic_lr = float(self._cached_critic_schedule(current_step))
            epoch_factor = 1.0  # OpenPIè°ƒåº¦ä¸ä½¿ç”¨epochæ¦‚å¿µ
            intra_epoch_factor = 1.0  # OpenPIè°ƒåº¦ä¸ä½¿ç”¨intra-epochæ¦‚å¿µ
            
        return {
            'actor_lr': actor_lr,
            'critic_lr': critic_lr,
            'epoch_factor': epoch_factor,
            'intra_epoch_factor': intra_epoch_factor,
            'total_factor': epoch_factor * intra_epoch_factor
        }
    
    def train(
        self,
        resume_from: Optional[str] = None
    ) -> ACRLPDPi0Agent:
        """
        Run complete training pipeline.
        
        Args:
            resume_from: Path to checkpoint to resume from
            
        Returns:
            Trained agent
        """
        logger.info("Starting ACRLPD + Ï€â‚€ training")
        
        # å¯ç”¨è¯¦ç»†å†…å­˜ç›‘æ§
        enable_memory_monitoring()
        
        # ğŸ” å¢å¼ºresumeé€»è¾‘
        if resume_from:
            self._resume_training(resume_from)
        elif hasattr(self.checkpoint_manager, 'auto_resume_path') and self.checkpoint_manager.auto_resume_path:
            # è‡ªåŠ¨resumeæ¨¡å¼
            logger.info(f"ğŸ¤– Auto-resume detected: {self.checkpoint_manager.auto_resume_path}")
            self._resume_training(self.checkpoint_manager.auto_resume_path)
        
        # Phase 1: Offline pretraining (now uses num_train_steps for total training)
        logger.info("Starting offline pretraining phase")
        self.agent = self._train_offline()
        

        
        # Final evaluation and checkpoint
        self._final_evaluation()
        
        logger.info("Training completed successfully")
        return self.agent
    
    def _train_offline(self) -> ACRLPDPi0Agent:
        """Run offline pretraining phase with gradient accumulation support."""
        import time  # åœ¨å‡½æ•°å¼€å§‹å°±å¯¼å…¥timeæ¨¡å—
        start_step = self.current_step
        target_steps = self.rl_config.num_train_steps  # ä½¿ç”¨æ­£ç¡®çš„è®­ç»ƒæ­¥æ•°
        
        # ğŸ”§ ä»é…ç½®ä¸­è·å–æ¢¯åº¦ç§¯ç´¯å‚æ•°ï¼ˆç”¨æˆ·è¦æ±‚çš„å®Œæ•´åŠŸèƒ½ï¼‰
        gradient_accumulation_steps = getattr(
            self.rl_config.acrlpd, 'gradient_accumulation_steps', 4
        )
        enable_gradient_accumulation = getattr(
            self.rl_config.acrlpd, 'enable_gradient_accumulation', True
        )
        max_grad_norm = getattr(
            self.rl_config.acrlpd, 'max_grad_norm', 1.0
        )
        
        if not enable_gradient_accumulation:
            gradient_accumulation_steps = 1
            logger.info("ğŸ”§ æ¢¯åº¦ç§¯ç´¯å·²ç¦ç”¨ï¼Œä½¿ç”¨æ ‡å‡†è®­ç»ƒæ¨¡å¼")
        else:
            logger.info(f"ğŸ”§ æ¢¯åº¦ç§¯ç´¯å·²å¯ç”¨: {gradient_accumulation_steps}æ­¥ç§¯ç´¯ï¼Œæ¢¯åº¦è£å‰ªé˜ˆå€¼: {max_grad_norm}")
        
        # Add system monitoring for tqdm
        import psutil
        
        def get_memory_gpu_info():
            """Get current memory and GPU usage for monitoring."""
            try:
                # Memory usage
                process = psutil.Process()
                memory_gb = process.memory_info().rss / (1024**3)
                
                # GPU usage (fallback to nvidia-smi if GPUtil not available)
                try:
                    import GPUtil
                    gpus = GPUtil.getGPUs()
                    if gpus:
                        gpu = gpus[0]  # Primary GPU
                        gpu_mem_used = gpu.memoryUsed / 1024  # Convert MB to GB
                        gpu_mem_total = gpu.memoryTotal / 1024
                        gpu_util = gpu.load * 100
                        return f"RAM:{memory_gb:.1f}GB GPU:{gpu_mem_used:.1f}/{gpu_mem_total:.1f}GB({gpu_util:.0f}%)"
                except ImportError:
                    # Fallback: Use nvidia-smi for GPU info
                    try:
                        import subprocess
                        result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used,memory.total,utilization.gpu', '--format=csv,noheader,nounits'], 
                                              capture_output=True, text=True, timeout=2)
                        if result.returncode == 0:
                            lines = result.stdout.strip().split('\n')
                            if lines and lines[0]:
                                gpu_mem_used, gpu_mem_total, gpu_util = lines[0].split(', ')
                                gpu_mem_used_gb = float(gpu_mem_used) / 1024
                                gpu_mem_total_gb = float(gpu_mem_total) / 1024
                                return f"RAM:{memory_gb:.1f}GB GPU:{gpu_mem_used_gb:.1f}/{gpu_mem_total_gb:.1f}GB({gpu_util}%)"
                    except Exception:
                        pass
                
                return f"RAM:{memory_gb:.1f}GB GPU:N/A"
            except Exception:
                return "Monitor:Error"
        
        # åˆå§‹åŒ–ä¸“ä¸šæ—¥å¿—å’ŒEpochæœºåˆ¶
        logger.info(f"=== Starting Offline Training ===")
        logger.info(f"Steps: {start_step} -> {target_steps} ({target_steps - start_step} steps)")
        logger.info(f"Batch Size: {self.rl_config.batch_size}")
        
        # å­¦ä¹ ç‡ä¿¡æ¯
        acrlpd_config = self.rl_config.acrlpd
        
        if acrlpd_config.enable_epoch_based_lr_schedule:
            steps_per_epoch = getattr(acrlpd_config, 'steps_per_epoch', 10000)
            total_epochs = getattr(acrlpd_config, 'total_epochs', 100)
            logger.info(f"Learning Rate Schedule: Epoch-based {acrlpd_config.lr_decay_strategy} decay enabled")
            logger.info(f"Epoch Configuration: {steps_per_epoch} steps/epoch, {total_epochs} total epochs")
            logger.info(f"Warmup: {acrlpd_config.warmup_epochs} epochs, Min factor: {acrlpd_config.lr_min_factor}")
            if acrlpd_config.intra_epoch_lr_decay:
                logger.info(f"Intra-epoch: {acrlpd_config.intra_epoch_strategy} decay, Min factor: {acrlpd_config.intra_epoch_min_factor}")
            
            # Epochæœºåˆ¶åˆå§‹åŒ–ï¼ˆä»…åœ¨å¯ç”¨æ—¶ï¼‰
            current_epoch = 0
            steps_in_current_epoch = 0
            epoch_losses = []
            epoch_start_time = time.time()
        else:
            logger.info("Learning Rate Schedule: Using step-based schedule only")
        
        # è®°å½•è®­ç»ƒå¼€å§‹æ—¶çš„å†…å­˜çŠ¶æ€
        log_memory_usage(start_step, self.agent.create_train_state() if hasattr(self.agent, 'create_train_state') else None, "offline_training_start")
        
        # Gradient accumulation state
        accumulated_gradients = None
        accumulation_count = 0
        
        # é¢„ç¼–è¯‘JITå‡½æ•°ï¼ˆä¸€æ¬¡ç¼–è¯‘ï¼Œå¤šæ¬¡ä½¿ç”¨ï¼‰
        if not hasattr(self, '_jit_compute_gradients'):
            logger.info("Precompiling JIT gradient computation function...")
            self._jit_compute_gradients = self._create_jit_compute_gradients()
            logger.info("JIT gradient computation function compiled successfully")
        
        if not hasattr(self, '_jit_apply_gradients'):
            logger.info("Precompiling JIT gradient application function...")
            self._jit_apply_gradients = self._create_jit_apply_gradients()
            logger.info("JIT gradient application function compiled successfully")
        
        # æ¢¯åº¦ç´¯ç§¯ä¿¡æ¯è®°å½•
        if gradient_accumulation_steps > 1:
            logger.info(f"Gradient accumulation enabled: {gradient_accumulation_steps} steps")
            logger.info(f"Effective batch_size: {self.rl_config.batch_size} x {gradient_accumulation_steps} = {self.rl_config.batch_size * gradient_accumulation_steps}")
        
        # # ğŸ¯ AC_Trainingæ€§èƒ½åˆ†æï¼šè¯¦ç»†æ—¶é—´ç»Ÿè®¡åˆå§‹åŒ–
        # ac_perf_stats = {
        #     'total_iter_times': [],
        #     'grad_accumulation_times': [],
        #     'data_loading_times': [],
        #     'grad_compute_times': [],
        #     'grad_process_times': [],
        #     'param_update_times': [],
        #     'config_convert_times': [],
        #     'jax_sync_times': [],
        #     'memory_monitor_times': [],
        #     'logging_times': [],
        #     'existing_perf_analysis_times': []
        # }
        
        # # ğŸ” JITç¼–è¯‘æ—¶é—´åˆ†æï¼šè¿½è¸ªç¬¬ä¸€æ¬¡ç¼–è¯‘
        # jit_compilation_stats = {
        #     'first_grad_compute_compilation': None,
        #     'first_grad_apply_compilation': None,
        #     'first_model_forward_compilation': None,
        #     'compilation_completed': False
        # }
        
        # logger.info("ğŸ” AC_Trainingæ€§èƒ½åˆ†æï¼šå¼€å§‹è¯¦ç»†è®¡æ—¶...")
        # logger.info(f"é¢„æœŸæ€§èƒ½ç“¶é¢ˆï¼šæ¢¯åº¦ç§¯ç´¯({gradient_accumulation_steps}x)ï¼Œæ€§èƒ½åˆ†æä»£ç ï¼Œå†…å­˜ç›‘æ§")
        # logger.info("ğŸ” JITç¼–è¯‘åˆ†æï¼šå°†è¯¦ç»†ç›‘æ§ç¬¬ä¸€æ¬¡ç¼–è¯‘æ—¶é—´")
        # enable_perf_analysis = getattr(self.rl_config, 'enable_perf_analysis', False)

        while self.current_step < target_steps:
            # # ğŸ¯ AC_Trainingæ€§èƒ½åˆ†æï¼šæ€»è¿­ä»£æ—¶é—´å¼€å§‹
            # ac_iter_start_time = time.perf_counter()
            step_start_time = time.time()  # ä¿æŒåŸæœ‰è®¡æ—¶
            
            # Debug: Log every 100 steps to show progress (reduced frequency)
            if self.current_step % 100 == 0:
                logger.debug(f"Training step {self.current_step}/{target_steps} - Sampling batch...")
            
            
            # === ğŸš€ é«˜æ•ˆæ¢¯åº¦ç´¯ç§¯é€»è¾‘ï¼ˆé¢„ç¼–è¯‘JITï¼‰===
            if gradient_accumulation_steps > 1:
                # é«˜æ•ˆæ¢¯åº¦ç´¯ç§¯æ¨¡å¼ï¼šåˆ†ç¦»è®¡ç®—å’Œæ›´æ–°ï¼Œé¿å…é‡å¤ç¼–è¯‘
                accumulated_gradients = None
                total_loss_info = None
                accumulated_aux_info = []
                
                # å¤ç”¨é¢„åˆ›å»ºçš„è®­ç»ƒé…ç½®ï¼ˆé¿å…é‡å¤åˆ›å»ºï¼Œæé«˜JITç¼“å­˜å‘½ä¸­ç‡ï¼‰
                if not hasattr(self, '_cached_training_config'):
                    self._cached_training_config = {
                        'critic_weight': getattr(self.rl_config.acrlpd, 'critic_weight', 1.0),
                        'actor_weight': getattr(self.rl_config.acrlpd, 'actor_weight', 1.0),
                        'bc_loss_weight': self.rl_config.acrlpd.bc_loss_weight,
                        'alpha_weight': getattr(self.rl_config.acrlpd, 'alpha_weight', 1.0),
                        'freeze_pi0_backbone': getattr(self.rl_config, 'freeze_pi0_backbone', False),
                        'target_update_tau': getattr(self.rl_config.acrlpd, 'target_update_tau', 0.005),
                        'horizon_length': self.rl_config.qchunking.horizon_length,
                        'discount': getattr(self.rl_config.acrlpd, 'discount', 0.99),
                        'q_aggregation': getattr(self.rl_config.acrlpd, 'q_aggregation', 'min'),
                        'real_action_dim': self.rl_config.qchunking.action_dim  # ä»QChunkingConfigä¼ é€’çœŸå®åŠ¨ä½œç»´åº¦
                    }
                training_config = self._cached_training_config
                
                # ğŸ¯ æ€§èƒ½åˆ†æï¼šåˆå§‹åŒ–è®¡æ—¶å™¨
                #perf_timings = {}  # æ€»æ˜¯åˆå§‹åŒ–é¿å…ä½œç”¨åŸŸé—®é¢˜
                #if enable_perf_analysis:
                #step_start_time = time.time()
                
                # ğŸ¯ AC_Trainingæ€§èƒ½åˆ†æï¼šæ¢¯åº¦ç§¯ç´¯å¾ªç¯å¼€å§‹
                #grad_accum_start_time = time.perf_counter()
                # existing_perf_start_time = time.perf_counter()
                
                # æ¢¯åº¦ç§¯ç´¯å¾ªç¯ï¼šåªè®¡ç®—æ¢¯åº¦ï¼Œä¸æ›´æ–°å‚æ•°
                for accumulation_step in range(gradient_accumulation_steps):
                    # ğŸ¯ AC_Trainingæ€§èƒ½åˆ†æï¼šæ•°æ®åŠ è½½
                    #ac_data_load_start = time.perf_counter()
                    
                    # Sample training batch
                    '''if enable_perf_analysis:
                        data_load_start = time.time()'''
                    self.rng, batch_rng = jax.random.split(self.rng)
                    batch = self.dataloader.sample_batch()
                    '''if enable_perf_analysis:
                        data_load_time = time.time() - data_load_start'''
                    
                    #ac_data_load_time = time.perf_counter() - ac_data_load_start
                    
                    # Extract positive/negative sample counts for logging
                    if 'positive_samples' in batch and 'negative_samples' in batch:
                        self._last_batch_pos_samples = int(batch['positive_samples'])
                        self._last_batch_neg_samples = int(batch['negative_samples'])
                    
                    # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥æ‰¹æ¬¡æ•°æ®ç»“æ„ï¼ˆåªåœ¨debugæ¨¡å¼ä¸‹è¾“å‡ºï¼‰
                    if logger.isEnabledFor(logging.DEBUG) and self.current_step < 3 and accumulation_step == 0:
                        logger.debug(f"=== æ‰¹æ¬¡æ•°æ®è°ƒè¯• (æ­¥éª¤ {self.current_step}, ç´¯ç§¯æ­¥éª¤ {accumulation_step}) ===")
                        batch_keys = list(batch.keys()) if isinstance(batch, dict) else "Not a dict"
                        logger.debug(f"æ‰¹æ¬¡é”®: {batch_keys}")
                        
                        # æ£€æŸ¥å…³é”®çš„RLå­—æ®µ
                        rl_required_keys = ['next_observations', 'rewards', 'masks']
                        missing_keys = [key for key in rl_required_keys if key not in batch]
                        if missing_keys:
                            logger.debug(f"âŒ ç¼ºå°‘RLå¿…éœ€å­—æ®µ: {missing_keys}")
                        else:
                            logger.debug(f"âœ… RLå­—æ®µå®Œæ•´: {rl_required_keys}")
                            # æ‰“å°å­—æ®µå½¢çŠ¶
                            for key in rl_required_keys:
                                if hasattr(batch[key], 'shape'):
                                    logger.debug(f"  {key}: {batch[key].shape}")
                                else:
                                    logger.debug(f"  {key}: {type(batch[key])}")
                        
                        # æ£€æŸ¥åŸºç¡€å­—æ®µ
                        basic_keys = ['observation', 'actions', 'action']
                        for key in basic_keys:
                            if key in batch:
                                if hasattr(batch[key], 'shape'):
                                    logger.debug(f"  {key}: {batch[key].shape}")
                                elif isinstance(batch[key], dict):
                                    logger.debug(f"  {key}: dict with keys {list(batch[key].keys())}")
                        logger.debug("=== æ‰¹æ¬¡æ•°æ®è°ƒè¯•ç»“æŸ ===")
                    
                    self.rng, train_rng = jax.random.split(self.rng)
                    
                    if self.use_fsdp:
                        # ğŸ¯ AC_Trainingæ€§èƒ½åˆ†æï¼šé…ç½®è½¬æ¢
                        #ac_config_convert_start = time.perf_counter()
                        
                        # ğŸš€ ä½¿ç”¨é¢„ç¼–è¯‘çš„JITæ¢¯åº¦è®¡ç®—ï¼ˆå¿«é€Ÿï¼ï¼‰
                        # è½¬æ¢dict configä¸ºå¯å“ˆå¸Œçš„frozen dataclass
                        #if enable_perf_analysis:
                        #    config_convert_start = time.time()
                        from training.acrlpd_train_state import ACRLPDJITConfig
                        jit_config = ACRLPDJITConfig.from_dict(training_config)
                        #if enable_perf_analysis:
                        #    config_convert_time = time.time() - config_convert_start
                        
                        #ac_config_convert_time = time.perf_counter() - ac_config_convert_start
                        
                        # ğŸ¯ AC_Trainingæ€§èƒ½åˆ†æï¼šæ¢¯åº¦è®¡ç®—
                        #ac_grad_compute_start = time.perf_counter()
                        
                        # ğŸ” JITç¼–è¯‘åˆ†æï¼šç›‘æ§ç¬¬ä¸€æ¬¡æ¢¯åº¦è®¡ç®—ç¼–è¯‘
                        # if jit_compilation_stats['first_grad_compute_compilation'] is None:
                        #     logger.info(f"ğŸ” å¼€å§‹ç¬¬ä¸€æ¬¡JITç¼–è¯‘ - æ¢¯åº¦è®¡ç®—å‡½æ•° (æ­¥éª¤ {self.current_step}, ç§¯ç´¯ {accumulation_step})")
                        #     jit_first_compile_start = time.perf_counter()
                        
                        # ğŸ¯ æ¢¯åº¦è®¡ç®—æ€§èƒ½åˆ†æ
                        #if enable_perf_analysis:
                        #    grad_compute_start = time.time()
                        step_gradients, step_loss_info, step_aux_info = self._jit_compute_gradients(
                            self.fsdp_train_state, batch, train_rng, jit_config
                        )
                        
                        # ğŸ” JITç¼–è¯‘åˆ†æï¼šè®°å½•ç¬¬ä¸€æ¬¡ç¼–è¯‘æ—¶é—´
                        #if jit_compilation_stats['first_grad_compute_compilation'] is None:
                        #     # JAXåŒæ­¥ä»¥ç¡®ä¿ç¼–è¯‘å®Œæˆ
                        #    jax.block_until_ready((step_gradients, step_loss_info, step_aux_info))
                        #     jit_first_compile_time = time.perf_counter() - jit_first_compile_start
                        #     jit_compilation_stats['first_grad_compute_compilation'] = jit_first_compile_time
                        #     logger.info(f"âœ… ç¬¬ä¸€æ¬¡JITç¼–è¯‘å®Œæˆ - æ¢¯åº¦è®¡ç®—: {jit_first_compile_time:.2f}ç§’")
                        #else:
                        #     # éç¬¬ä¸€æ¬¡è°ƒç”¨ï¼Œæ­£å¸¸åŒæ­¥
                        jax.block_until_ready((step_gradients, step_loss_info, step_aux_info))
                        
                        #ac_grad_compute_time = time.perf_counter() - ac_grad_compute_start
                        
                        #if enable_perf_analysis:
                        #    grad_compute_time = time.time() - grad_compute_start
                        
                        # ğŸ¯ AC_Trainingæ€§èƒ½åˆ†æï¼šæ¢¯åº¦å¤„ç†
                        #ac_grad_process_start = time.perf_counter()
                        
                        # ğŸ¯ æ¢¯åº¦å¤„ç†æ€§èƒ½åˆ†æ
                        #if enable_perf_analysis:
                        #    grad_process_start = time.time()
                        # ç¼©æ”¾æ¢¯åº¦ï¼ˆæ¢¯åº¦ç§¯ç´¯çš„å…³é”®æ­¥éª¤ï¼‰
                        scaled_gradients = jax.tree.map(
                            lambda g: g / gradient_accumulation_steps if g is not None else None,
                            step_gradients
                        )
                        
                        # ç§¯ç´¯æ¢¯åº¦
                        if accumulated_gradients is None:
                            accumulated_gradients = scaled_gradients
                        else:
                            accumulated_gradients = jax.tree.map(
                                lambda acc, new: acc + new if (acc is not None and new is not None) 
                                                  else (acc if new is None else new),
                                accumulated_gradients, 
                                scaled_gradients
                            )
                        
                        # JAXåŒæ­¥ç¡®ä¿æ¢¯åº¦å¤„ç†å®Œæˆ
                        jax.block_until_ready(accumulated_gradients)
                        #ac_grad_process_time = time.perf_counter() - ac_grad_process_start
                        
                        #if enable_perf_analysis:
                        #    grad_process_time = time.time() - grad_process_start
                        
                        # ç§¯ç´¯lossä¿¡æ¯ï¼ˆç”¨äºç›‘æ§ï¼‰
                        if total_loss_info is None:
                            total_loss_info = step_loss_info
                        else:
                            # ç´¯ç§¯lossä¿¡æ¯ï¼ˆä¿æŒæ•°å€¼ç¨³å®šæ€§ï¼‰
                            total_loss_info = {
                                key: (total_loss_info[key] * accumulation_step + step_loss_info[key]) / (accumulation_step + 1)
                                for key in total_loss_info.keys()
                            }
                        
                        accumulated_aux_info.append(step_aux_info)
                        
                        # ğŸ¯ AC_Trainingæ€§èƒ½åˆ†æï¼šæ”¶é›†æœ¬æ¬¡ç§¯ç´¯æ­¥éª¤çš„æ—¶é—´ç»Ÿè®¡
                        '''if accumulation_step == 0:
                            # åˆå§‹åŒ–ç´¯ç§¯æ—¶é—´ç»Ÿè®¡
                            total_data_load_time = ac_data_load_time
                            total_config_convert_time = ac_config_convert_time
                            total_grad_compute_time = ac_grad_compute_time
                            total_grad_process_time = ac_grad_process_time
                        else:
                            # ç´¯ç§¯æ—¶é—´ç»Ÿè®¡
                            total_data_load_time += ac_data_load_time
                            total_config_convert_time += ac_config_convert_time
                            total_grad_compute_time += ac_grad_compute_time
                            total_grad_process_time += ac_grad_process_time
                        
                        # ğŸ¯ ç´¯ç§¯è®¡æ—¶ä¿¡æ¯ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
                        if enable_perf_analysis:
                            if accumulation_step == 0:
                                perf_timings['data_loading'] = data_load_time
                                perf_timings['config_convert'] = config_convert_time
                                perf_timings['grad_compute'] = grad_compute_time  
                                perf_timings['grad_process'] = grad_process_time
                            else:
                                perf_timings['data_loading'] += data_load_time
                                perf_timings['config_convert'] += config_convert_time
                                perf_timings['grad_compute'] += grad_compute_time
                                perf_timings['grad_process'] += grad_process_time'''
                    
                    else:
                        # éFSDPæ¨¡å¼çš„å…¼å®¹å®ç°
                        step_loss_info = self.agent.train_step(batch, train_rng)
                        if total_loss_info is None:
                            total_loss_info = step_loss_info._asdict()
                        else:
                            # ç´¯ç§¯lossä¿¡æ¯
                            step_dict = step_loss_info._asdict()
                            total_loss_info = {
                                key: (total_loss_info[key] * accumulation_step + step_dict[key]) / (accumulation_step + 1)
                                for key in total_loss_info.keys()
                            }
                
                # ğŸ¯ AC_Trainingæ€§èƒ½åˆ†æï¼šæ¢¯åº¦ç§¯ç´¯å¾ªç¯æ€»æ—¶é—´
                #total_grad_accum_time = time.perf_counter() - grad_accum_start_time
                
                # åº”ç”¨ç§¯ç´¯çš„æ¢¯åº¦ï¼ˆä¸€æ¬¡å‚æ•°æ›´æ–°ï¼‰
                if self.use_fsdp and accumulated_gradients is not None:
                    # ğŸ¯ AC_Trainingæ€§èƒ½åˆ†æï¼šå‚æ•°æ›´æ–°
                    #ac_param_update_start = time.perf_counter()
                    
                    # ğŸ” JITç¼–è¯‘åˆ†æï¼šç›‘æ§ç¬¬ä¸€æ¬¡å‚æ•°æ›´æ–°ç¼–è¯‘
                    if jit_compilation_stats['first_grad_apply_compilation'] is None:
                        logger.info(f"ğŸ” å¼€å§‹ç¬¬ä¸€æ¬¡JITç¼–è¯‘ - å‚æ•°æ›´æ–°å‡½æ•° (æ­¥éª¤ {self.current_step})")
                        jit_apply_compile_start = time.perf_counter()
                    
                    # ğŸ¯ å‚æ•°æ›´æ–°æ€§èƒ½åˆ†æ
                    '''if enable_perf_analysis:
                        param_update_start = time.time()'''
                    # ğŸš€ ä½¿ç”¨é¢„ç¼–è¯‘çš„JITæ¢¯åº¦åº”ç”¨ï¼ˆå¿«é€Ÿï¼ï¼‰
                    # è½¬æ¢dict configä¸ºå¯å“ˆå¸Œçš„frozen dataclass
                    from training.acrlpd_train_state import ACRLPDJITConfig
                    jit_config = ACRLPDJITConfig.from_dict(training_config)
                    
                    self.fsdp_train_state = self._jit_apply_gradients(
                        self.fsdp_train_state, accumulated_gradients, jit_config
                    )
                    
                    # ğŸ” JITç¼–è¯‘åˆ†æï¼šè®°å½•ç¬¬ä¸€æ¬¡å‚æ•°æ›´æ–°ç¼–è¯‘æ—¶é—´
                    if jit_compilation_stats['first_grad_apply_compilation'] is None:
                        # JAXåŒæ­¥ç¡®ä¿å‚æ•°æ›´æ–°å’Œç¼–è¯‘å®Œæˆ
                        jax.block_until_ready(self.fsdp_train_state)
                        jit_apply_compile_time = time.perf_counter() - jit_apply_compile_start
                        jit_compilation_stats['first_grad_apply_compilation'] = jit_apply_compile_time
                        logger.info(f"âœ… ç¬¬ä¸€æ¬¡JITç¼–è¯‘å®Œæˆ - å‚æ•°æ›´æ–°: {jit_apply_compile_time:.2f}ç§’")
                    else:
                        # JAXåŒæ­¥ç¡®ä¿å‚æ•°æ›´æ–°å®Œæˆ
                        jax.block_until_ready(self.fsdp_train_state)
                    
                    ac_param_update_time = time.perf_counter() - ac_param_update_start
                    
                    # Update current step from FSDP train state
                    self.current_step = int(self.fsdp_train_state.step)
                    '''if enable_perf_analysis:
                        param_update_time = time.time() - param_update_start
                        perf_timings['param_update'] = param_update_time'''
                
                # è½¬æ¢loss infoä¸ºæ ‡å‡†æ ¼å¼
                from agents.loss_functions import LossInfo
                loss_info = LossInfo(
                    total_loss=total_loss_info.get('total_loss', 0.0),
                    critic_loss=total_loss_info.get('critic_loss', 0.0),
                    actor_loss=total_loss_info.get('actor_loss', 0.0),
                    bc_loss=total_loss_info.get('bc_loss', 0.0),
                    alpha_loss=total_loss_info.get('alpha_loss', 0.0),
                    q_mean=total_loss_info.get('q_mean', 0.0),
                    q_std=total_loss_info.get('q_std', 0.0),
                    target_q_mean=total_loss_info.get('target_q_mean', 0.0),
                    td_error_mean=total_loss_info.get('td_error_mean', 0.0),
                    bc_loss_raw=total_loss_info.get('bc_loss_raw', 0.0),
                    alpha_value=total_loss_info.get('alpha_value', 0.0),
                    entropy_estimate=total_loss_info.get('entropy_estimate', 0.0),
                    q_values_for_actor=total_loss_info.get('q_values_for_actor', 0.0),
                    valid_samples=total_loss_info.get('valid_samples', 0.0),
                    mask_ratio=total_loss_info.get('mask_ratio', 1.0)
                )
                
                # Extract positive/negative sample counts from accumulated loss_info (æ¢¯åº¦ç§¯ç´¯æ¨¡å¼)
                self._last_batch_pos_samples = int(float(total_loss_info.get('bc_positive_samples', 0.0)))
                total_samples = int(float(total_loss_info.get('bc_total_samples', 0.0)))
                self._last_batch_neg_samples = total_samples - self._last_batch_pos_samples
                
            else:
                # ğŸš€ OPTIMIZED: Streamlined training step (éæ¢¯åº¦ç§¯ç´¯)
                # ğŸ¯ AC_Trainingæ€§èƒ½åˆ†æï¼šæ•°æ®åŠ è½½
                #ac_data_load_start = time.perf_counter()
                
                #if enable_perf_analysis:
                #    data_load_start = time.time()
                batch = self.dataloader.sample_batch()
                #if enable_perf_analysis:
                #    data_load_time = time.time() - data_load_start
                
                #ac_data_load_time = time.perf_counter() - ac_data_load_start
                
                # Extract positive/negative sample counts for logging (from loss_info instead of batch)
                # Note: In non-gradient accumulation mode, sample info comes from loss computation output
                self._last_batch_pos_samples = 0  # Will be updated from loss_info_dict after training step
                self._last_batch_neg_samples = 0  # Will be updated from loss_info_dict after training step
                
                self.rng, train_rng = jax.random.split(self.rng)
                
                # ğŸ¯ æ€§èƒ½åˆ†æï¼šåˆå§‹åŒ–è®¡æ—¶å™¨ï¼ˆç¡®ä¿åœ¨æ‰€æœ‰åˆ†æ”¯ä¸­éƒ½å®šä¹‰ï¼‰
                # existing_perf_start_time = time.perf_counter()
                perf_timings = {}  # åˆå§‹åŒ–æ€§èƒ½è®¡æ—¶å­—å…¸
                
                if self.use_fsdp:
                    # ğŸ” JITç¼–è¯‘åˆ†æï¼šç›‘æ§ç¬¬ä¸€æ¬¡ç›´æ¥è®­ç»ƒæ­¥ç¼–è¯‘
                    '''if jit_compilation_stats.get('first_direct_train_compilation') is None:
                        logger.info(f"ğŸ” å¼€å§‹ç¬¬ä¸€æ¬¡JITç¼–è¯‘ - ç›´æ¥è®­ç»ƒæ­¥å‡½æ•° (æ­¥éª¤ {self.current_step})")
                        jit_direct_compile_start = time.perf_counter()'''
                    
                    # ğŸ¯ ç›´æ¥è®­ç»ƒæ­¥æ€§èƒ½åˆ†æ
                    '''if enable_perf_analysis:
                        direct_train_start = time.time()'''
                    
                    # Direct FSDP training with optimized acrlpd_train_step
                    self.fsdp_train_state, loss_info_dict = self.fsdp_train_step(
                        self.fsdp_train_state, batch, train_rng
                    )
                    
                    # Extract positive/negative sample counts from loss_info_dict (FIXED)
                    self._last_batch_pos_samples = int(float(loss_info_dict.get('bc_positive_samples', 0.0)))
                    total_samples = int(float(loss_info_dict.get('bc_total_samples', 0.0)))
                    self._last_batch_neg_samples = total_samples - self._last_batch_pos_samples
                    
                    # ğŸ” JITç¼–è¯‘åˆ†æï¼šè®°å½•ç¬¬ä¸€æ¬¡ç›´æ¥è®­ç»ƒæ­¥ç¼–è¯‘æ—¶é—´
                    '''if jit_compilation_stats.get('first_direct_train_compilation') is None:
                        # JAXåŒæ­¥ç¡®ä¿ç¼–è¯‘å®Œæˆ
                        jax.block_until_ready((self.fsdp_train_state, loss_info_dict))
                        jit_direct_compile_time = time.perf_counter() - jit_direct_compile_start
                        jit_compilation_stats['first_direct_train_compilation'] = jit_direct_compile_time
                        logger.info(f"âœ… ç¬¬ä¸€æ¬¡JITç¼–è¯‘å®Œæˆ - ç›´æ¥è®­ç»ƒæ­¥: {jit_direct_compile_time:.2f}ç§’")
                        
                        # æ›´æ–°ç¼–è¯‘å®ŒæˆçŠ¶æ€æ£€æŸ¥é€»è¾‘
                        if not jit_compilation_stats.get('compilation_completed', False):
                            jit_compilation_stats['compilation_completed'] = True
                            logger.info("=" * 60)
                            logger.info("ğŸ” AC_Training JITç¼–è¯‘å®Œæˆ - ç›´æ¥è®­ç»ƒæ¨¡å¼")
                            logger.info("=" * 60)
                            logger.info(f"ç›´æ¥è®­ç»ƒæ­¥å‡½æ•°ç¼–è¯‘: {jit_direct_compile_time:.2f}ç§’")
                            logger.info("JITç¼–è¯‘å®Œæˆï¼åç»­è®­ç»ƒæ­¥éª¤å°†ä½¿ç”¨å·²ç¼–è¯‘çš„ä¼˜åŒ–ä»£ç ã€‚")
                            logger.info("=" * 60)
                    else:'''
                        # éç¬¬ä¸€æ¬¡è°ƒç”¨ï¼Œæ­£å¸¸åŒæ­¥
                    jax.block_until_ready((self.fsdp_train_state, loss_info_dict))
                    
                    #if enable_perf_analysis:
                    #    direct_train_time = time.time() - direct_train_start
                    #    perf_timings['direct_train'] = direct_train_time
                    
                    # ğŸ¯ AC_Trainingæ€§èƒ½åˆ†æï¼šæŸå¤±ä¿¡æ¯è½¬æ¢
                    #ac_loss_convert_start = time.perf_counter()
                    
                    # ğŸš€ OPTIMIZED: Simplified loss info conversion
                    #if enable_perf_analysis:
                    #    loss_convert_start = time.time()
                    from agents.loss_functions import LossInfo
                    loss_info = LossInfo(**{
                        **{k: loss_info_dict.get(k, 0.0) for k in [
                            'total_loss', 'critic_loss', 'actor_loss', 'bc_loss', 'alpha_loss',
                            'q_mean', 'q_std', 'target_q_mean', 'td_error_mean', 'bc_loss_raw',
                            'alpha_value', 'entropy_estimate', 'q_values_for_actor'
                        ]},
                        'valid_samples': float(loss_info_dict.get('valid_samples', 16.0)),
                        'mask_ratio': loss_info_dict.get('mask_ratio', 1.0)
                    })
                    #if enable_perf_analysis:
                    #    loss_convert_time = time.time() - loss_convert_start
                    #    perf_timings['loss_convert'] = loss_convert_time
                    
                    #ac_loss_convert_time = time.perf_counter() - ac_loss_convert_start
                    
                    # Update current step from FSDP train state
                    self.current_step = int(self.fsdp_train_state.step)
                    
                    # # ğŸ¯ AC_Trainingæ€§èƒ½åˆ†æï¼šæ”¶é›†éæ¢¯åº¦ç§¯ç´¯æ¨¡å¼çš„æ—¶é—´ç»Ÿè®¡
                    # # åˆå§‹åŒ–éæ¢¯åº¦ç§¯ç´¯ä¸“ç”¨çš„ç»Ÿè®¡å­—å…¸
                    # if not hasattr(ac_perf_stats, 'non_grad_accum_data_load'):
                    #     ac_perf_stats['non_grad_accum_data_load'] = []
                    #     ac_perf_stats['non_grad_accum_direct_train'] = []
                    #     ac_perf_stats['non_grad_accum_loss_convert'] = []
                    
                    # ac_perf_stats['non_grad_accum_data_load'].append(ac_data_load_time)
                    # ac_perf_stats['non_grad_accum_loss_convert'].append(ac_loss_convert_time)
                    
                    # è®¾ç½®æ€§èƒ½åˆ†æçš„åŸºæœ¬æ—¶é—´ç»Ÿè®¡
                    '''if enable_perf_analysis:
                        perf_timings['data_loading'] = data_load_time
                        perf_timings['config_convert'] = 0.0  # éæ¢¯åº¦ç§¯ç´¯æ¨¡å¼æ— é…ç½®è½¬æ¢
                        perf_timings['grad_compute'] = 0.0   # éæ¢¯åº¦ç§¯ç´¯æ¨¡å¼æ— åˆ†ç¦»çš„æ¢¯åº¦è®¡ç®—
                        perf_timings['grad_process'] = 0.0   # éæ¢¯åº¦ç§¯ç´¯æ¨¡å¼æ— åˆ†ç¦»çš„æ¢¯åº¦å¤„ç†
                        perf_timings['param_update'] = direct_train_time  # ç›´æ¥è®­ç»ƒåŒ…å«äº†å‚æ•°æ›´æ–°'''
                    
                else:
                    # åŸå§‹æ–¹å¼ï¼šç›´æ¥è°ƒç”¨ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
                    #if enable_perf_analysis:
                    #    agent_train_start = time.time()
                    self.agent, loss_info = self.agent.train_step(batch, train_rng)
                    '''if enable_perf_analysis:
                        agent_train_time = time.time() - agent_train_start
                        perf_timings = {
                            'data_loading': data_load_time,
                            'agent_train': agent_train_time,
                            'config_convert': 0.0,
                            'grad_compute': 0.0,
                            'grad_process': 0.0,
                            'param_update': agent_train_time
                        }'''
            
            # ğŸ¯ AC_Trainingæ€§èƒ½åˆ†æï¼šæ”¶é›†æœ¬æ¬¡è¿­ä»£çš„ç»Ÿè®¡æ•°æ®
            # ac_total_iter_time = time.perf_counter() - ac_iter_start_time
            
            # # æ”¶é›†ç°æœ‰æ€§èƒ½åˆ†æä»£ç çš„æ—¶é—´å¼€é”€
            # existing_perf_analysis_time = time.perf_counter() - existing_perf_start_time if enable_perf_analysis else 0
            
            # æ·»åŠ åˆ°ç»Ÿè®¡æ•°ç»„
            # if gradient_accumulation_steps > 1:
            #     # æ¢¯åº¦ç§¯ç´¯æ¨¡å¼çš„ç»Ÿè®¡
            #     ac_perf_stats['total_iter_times'].append(ac_total_iter_time)
            #    ac_perf_stats['grad_accumulation_times'].append(total_grad_accum_time)
            #    ac_perf_stats['data_loading_times'].append(total_data_load_time)
            #    ac_perf_stats['grad_compute_times'].append(total_grad_compute_time)
            #    ac_perf_stats['grad_process_times'].append(total_grad_process_time)
            #    ac_perf_stats['param_update_times'].append(ac_param_update_time if 'ac_param_update_time' in locals() else 0)
            #    ac_perf_stats['config_convert_times'].append(total_config_convert_time)
            #    ac_perf_stats['existing_perf_analysis_times'].append(existing_perf_analysis_time)
            # else:
            #     # éæ¢¯åº¦ç§¯ç´¯æ¨¡å¼çš„ç»Ÿè®¡
            #     ac_perf_stats['total_iter_times'].append(ac_total_iter_time)
                
                # ä¸ºéæ¢¯åº¦ç§¯ç´¯æ¨¡å¼æ”¶é›†å¯¹åº”çš„ç»Ÿè®¡
                #if 'ac_data_load_time' in locals():
                #    ac_perf_stats['data_loading_times'].append(ac_data_load_time)
                #if 'ac_loss_convert_time' in locals():
                    # å°†æŸå¤±è½¬æ¢æ—¶é—´ä½œä¸ºå¤„ç†æ—¶é—´
                #    ac_perf_stats.setdefault('loss_convert_times', []).append(ac_loss_convert_time)
                
                # å¯¹äºéæ¢¯åº¦ç§¯ç´¯æ¨¡å¼ï¼Œç›´æ¥è®­ç»ƒæ­¥åŒ…å«äº†æ‰€æœ‰è®¡ç®—
                # direct_train_time = ac_total_iter_time - ac_data_load_time - ac_loss_convert_time if 'ac_data_load_time' in locals() else 0
                #ac_perf_stats.setdefault('direct_train_times', []).append(direct_train_time)
                
                # # è®¾ç½®å ä½å€¼ä»¥ä¿æŒç»Ÿè®¡ä¸€è‡´æ€§
                # ac_perf_stats.setdefault('grad_accumulation_times', []).append(0)
                # ac_perf_stats.setdefault('grad_compute_times', []).append(direct_train_time * 0.7)  # ä¼°ç®—å¤§éƒ¨åˆ†æ—¶é—´åœ¨è®¡ç®—
                # ac_perf_stats.setdefault('grad_process_times', []).append(0)  # éæ¢¯åº¦ç§¯ç´¯æ— å•ç‹¬å¤„ç†
                # ac_perf_stats.setdefault('param_update_times', []).append(direct_train_time * 0.3)  # ä¼°ç®—å‚æ•°æ›´æ–°
                # ac_perf_stats.setdefault('config_convert_times', []).append(0)  # éæ¢¯åº¦ç§¯ç´¯æ— é…ç½®è½¬æ¢
                # ac_perf_stats['existing_perf_analysis_times'].append(existing_perf_analysis_time)
            
            # # ğŸ¯ å…¶ä»–å¼€é”€è®¡æ—¶ï¼šGPUç›‘æ§
            # ac_memory_monitor_start = time.perf_counter()
            # if enable_perf_analysis:
            #     gpu_monitor_start = time.time()
                
            # ğŸ”§ å®šæœŸç›‘æ§æ˜¾å­˜ä½¿ç”¨å’ŒFSDPæ•ˆæœ
            '''if self.current_step % 100 == 0:
                try:
                    import subprocess
                    logger.info(f"ğŸ” è®­ç»ƒæ­¥éª¤ {self.current_step} - æ˜¾å­˜ç›‘æ§:")
                    result = subprocess.run(['nvidia-smi',
            '--query-gpu=index,memory.used,memory.total', '--format=csv,noheader,nounits'],
                        capture_output=True, text=True)
                    if result.returncode == 0:
                        for line in result.stdout.strip().split('\n'):
                            parts = line.split(', ')
                            if len(parts) == 3:
                                gpu_id, used, total = parts
                                usage_pct = (int(used) / int(total)) * 100
                                logger.info(f"  GPU {gpu_id}: {used}MB/{total}MB ({usage_pct:.1f}%)")

                    # æ¢¯åº¦ç´¯ç§¯æ•ˆæœè®°å½•
                    if gradient_accumulation_steps > 1:
                        logger.info(f"ğŸ”§ æ¢¯åº¦ç´¯ç§¯æ•ˆæœ: æœ‰æ•ˆbatch_size={self.rl_config.batch_size * gradient_accumulation_steps}")
                except Exception as e:
                    logger.warning(f"æ˜¾å­˜ç›‘æ§å¤±è´¥: {e}")'''
            
            # # æ”¶é›†å†…å­˜ç›‘æ§æ—¶é—´
            # ac_memory_monitor_time = time.perf_counter() - ac_memory_monitor_start
            # ac_perf_stats['memory_monitor_times'].append(ac_memory_monitor_time)
            
            # === ğŸ” JITç¼–è¯‘å®Œæˆæ£€æŸ¥å’ŒæŠ¥å‘Š ===
            '''if not jit_compilation_stats['compilation_completed']:
                all_compiled = (
                    jit_compilation_stats['first_grad_compute_compilation'] is not None and 
                    jit_compilation_stats['first_grad_apply_compilation'] is not None
                )
                
                if all_compiled:
                    jit_compilation_stats['compilation_completed'] = True
                    total_compile_time = (
                        jit_compilation_stats['first_grad_compute_compilation'] + 
                        jit_compilation_stats['first_grad_apply_compilation']
                    )
                    
                    logger.info("=" * 60)
                    logger.info("ğŸ” AC_Training JITç¼–è¯‘å®Œæˆ - è¯¦ç»†æ—¶é—´æŠ¥å‘Š")
                    logger.info("=" * 60)
                    logger.info(f"æ€»JITç¼–è¯‘æ—¶é—´: {total_compile_time:.2f}ç§’")
                    logger.info(f"â”œâ”€ æ¢¯åº¦è®¡ç®—å‡½æ•°ç¼–è¯‘: {jit_compilation_stats['first_grad_compute_compilation']:.2f}ç§’ ({jit_compilation_stats['first_grad_compute_compilation']/total_compile_time*100:.1f}%)")
                    logger.info(f"â””â”€ å‚æ•°æ›´æ–°å‡½æ•°ç¼–è¯‘: {jit_compilation_stats['first_grad_apply_compilation']:.2f}ç§’ ({jit_compilation_stats['first_grad_apply_compilation']/total_compile_time*100:.1f}%)")
                    logger.info("")
                    logger.info("JITç¼–è¯‘å®Œæˆï¼åç»­è®­ç»ƒæ­¥éª¤å°†ä½¿ç”¨å·²ç¼–è¯‘çš„ä¼˜åŒ–ä»£ç ã€‚")
                    logger.info("=" * 60)'''
            
            # === AC_Trainingæ€§èƒ½åˆ†ææŠ¥å‘Šï¼ˆè°ƒè¯•å·²å®Œæˆï¼Œæ³¨é‡Šæ‰è¯¦ç»†è¾“å‡ºï¼‰===
            # if (self.current_step % 2 == 0 or self.current_step == start_step) and ac_perf_stats['total_iter_times']:
            #     recent_samples = min(2, len(ac_perf_stats['total_iter_times']))
            #     
            #     avg_total = np.mean(ac_perf_stats['total_iter_times'][-recent_samples:])
            #     avg_grad_accum = np.mean(ac_perf_stats['grad_accumulation_times'][-recent_samples:])
            #     avg_data_load = np.mean(ac_perf_stats['data_loading_times'][-recent_samples:])
            #     avg_grad_compute = np.mean(ac_perf_stats['grad_compute_times'][-recent_samples:])
            #     avg_grad_process = np.mean(ac_perf_stats['grad_process_times'][-recent_samples:])
            #     avg_param_update = np.mean([t for t in ac_perf_stats['param_update_times'][-recent_samples:] if t > 0])
            #     avg_config_convert = np.mean(ac_perf_stats['config_convert_times'][-recent_samples:])
            #     avg_memory_monitor = np.mean(ac_perf_stats['memory_monitor_times'][-recent_samples:])
            #     avg_existing_perf = np.mean(ac_perf_stats['existing_perf_analysis_times'][-recent_samples:])
            #     
            #     # æ ¹æ®æ¢¯åº¦ç§¯ç´¯æ¨¡å¼æ˜¾ç¤ºä¸åŒçš„æ€§èƒ½åˆ†æ
            #     if gradient_accumulation_steps > 1:
            #         # æ¢¯åº¦ç§¯ç´¯æ¨¡å¼
            #         logger.info(f"ğŸ” AC_Trainingæ€§èƒ½åˆ†æ (Step {self.current_step}) - æ¢¯åº¦ç§¯ç´¯æ¨¡å¼:")
            #         logger.info(f"  æ€»è¿­ä»£æ—¶é—´: {avg_total*1000:.2f}ms")
            #         logger.info(f"  â”œâ”€ æ¢¯åº¦ç§¯ç´¯å¾ªç¯: {avg_grad_accum*1000:.2f}ms ({avg_grad_accum/avg_total*100:.1f}%)")
            #         logger.info(f"  â”‚  â”œâ”€ æ•°æ®åŠ è½½(x{gradient_accumulation_steps}): {avg_data_load*1000:.2f}ms ({avg_data_load/avg_total*100:.1f}%)")
            #         logger.info(f"  â”‚  â”œâ”€ æ¢¯åº¦è®¡ç®—(x{gradient_accumulation_steps}): {avg_grad_compute*1000:.2f}ms ({avg_grad_compute/avg_total*100:.1f}%)")
            #         logger.info(f"  â”‚  â”œâ”€ æ¢¯åº¦å¤„ç†(x{gradient_accumulation_steps}): {avg_grad_process*1000:.2f}ms ({avg_grad_process/avg_total*100:.1f}%)")
            #         logger.info(f"  â”‚  â””â”€ é…ç½®è½¬æ¢(x{gradient_accumulation_steps}): {avg_config_convert*1000:.2f}ms ({avg_config_convert/avg_total*100:.1f}%)")
            #         logger.info(f"  â”œâ”€ å‚æ•°æ›´æ–°: {avg_param_update*1000:.2f}ms ({avg_param_update/avg_total*100:.1f}%)")
            #         logger.info(f"  â”œâ”€ å†…å­˜ç›‘æ§: {avg_memory_monitor*1000:.2f}ms ({avg_memory_monitor/avg_total*100:.1f}%)")
            #         logger.info(f"  â””â”€ ç°æœ‰æ€§èƒ½åˆ†æ: {avg_existing_perf*1000:.2f}ms ({avg_existing_perf/avg_total*100:.1f}%)")
            #         logger.info(f"  æ¯ç§’è¿­ä»£æ•°: {1.0/avg_total:.2f} iter/s (æœ‰æ•ˆbatch: {self.rl_config.batch_size * gradient_accumulation_steps})")
            #     else:
            #         # éæ¢¯åº¦ç§¯ç´¯æ¨¡å¼ - ç›´æ¥è®­ç»ƒ
            #         avg_direct_train = np.mean(ac_perf_stats.get('direct_train_times', [0])[-recent_samples:])
            #         avg_loss_convert = np.mean(ac_perf_stats.get('loss_convert_times', [0])[-recent_samples:])
            #         
            #         logger.info(f"ğŸ” AC_Trainingæ€§èƒ½åˆ†æ (Step {self.current_step}) - ç›´æ¥è®­ç»ƒæ¨¡å¼:")
            #         logger.info(f"  æ€»è¿­ä»£æ—¶é—´: {avg_total*1000:.2f}ms")
            #         logger.info(f"  â”œâ”€ æ•°æ®åŠ è½½: {avg_data_load*1000:.2f}ms ({avg_data_load/avg_total*100:.1f}%)")
            #         logger.info(f"  â”œâ”€ ç›´æ¥è®­ç»ƒæ­¥: {avg_direct_train*1000:.2f}ms ({avg_direct_train/avg_total*100:.1f}%)")
            #         logger.info(f"  â”‚  â”œâ”€ æ¢¯åº¦è®¡ç®—+å‚æ•°æ›´æ–°: {avg_grad_compute*1000:.2f}ms ({avg_grad_compute/avg_total*100:.1f}%)")
            #         logger.info(f"  â”‚  â””â”€ å†…éƒ¨å‚æ•°æ›´æ–°: {avg_param_update*1000:.2f}ms ({avg_param_update/avg_total*100:.1f}%)")
            #         logger.info(f"  â”œâ”€ æŸå¤±ä¿¡æ¯è½¬æ¢: {avg_loss_convert*1000:.2f}ms ({avg_loss_convert/avg_total*100:.1f}%)")
            #         logger.info(f"  â”œâ”€ å†…å­˜ç›‘æ§: {avg_memory_monitor*1000:.2f}ms ({avg_memory_monitor/avg_total*100:.1f}%)")
            #         logger.info(f"  â””â”€ ç°æœ‰æ€§èƒ½åˆ†æ: {avg_existing_perf*1000:.2f}ms ({avg_existing_perf/avg_total*100:.1f}%)")
            #         logger.info(f"  æ¯ç§’è¿­ä»£æ•°: {1.0/avg_total:.2f} iter/s (batch: {self.rl_config.batch_size})")
            
            '''if enable_perf_analysis and self.current_step % 100 == 0:
                gpu_monitor_time = time.time() - gpu_monitor_start
                if 'gpu_monitor' not in perf_timings:
                    perf_timings['gpu_monitor'] = 0
                perf_timings['gpu_monitor'] += gpu_monitor_time
                
            # ğŸ¯ å…¶ä»–å¼€é”€è®¡æ—¶ï¼šEMAæ›´æ–°
            if enable_perf_analysis:
                ema_start = time.time()
                
            # ğŸš€ EMAæ›´æ–°ä¼˜åŒ–ï¼šåœ¨FSDPæ¨¡å¼ä¸‹ï¼ŒEMAæ›´æ–°å·²åœ¨gradient applicationä¸­è‡ªåŠ¨å®Œæˆ
            # åˆ é™¤å¤šä½™çš„EMAæ›´æ–°ï¼Œé¿å…é‡å»ºè®­ç»ƒçŠ¶æ€çš„16.6så¼€é”€
            if enable_perf_analysis:
                ema_time = time.time() - ema_start
                # EMAæ›´æ–°æ—¶é—´ç°åœ¨åº”è¯¥æ¥è¿‘0ï¼Œå› ä¸ºå·²ç»åœ¨gradient applicationä¸­å¤„ç†
                if 'ema_update' not in perf_timings:
                    perf_timings['ema_update'] = 0
                perf_timings['ema_update'] += ema_time'''
                
            # æ³¨æ„ï¼šå¯¹äºéFSDPæ¨¡å¼ï¼ŒEMAæ›´æ–°åœ¨agent.train_stepä¸­å¤„ç†
                    
            # ğŸ¯ å…¶ä»–å¼€é”€è®¡æ—¶ï¼šæŒ‡æ ‡æ›´æ–°
            '''if enable_perf_analysis:
                metrics_start = time.time()'''
            
            # Update metrics
            step_time = time.time() - step_start_time
            effective_batch_size = self.rl_config.batch_size * gradient_accumulation_steps
            self.metrics.update(
                self.current_step,
                loss_info,
                timing_step_duration=step_time,
                timing_samples_per_sec=effective_batch_size / step_time,
                effective_batch_size=effective_batch_size,
                gradient_accumulation_steps=gradient_accumulation_steps
            )
            
            '''if enable_perf_analysis:
                metrics_time = time.time() - metrics_start
                if 'metrics_update' not in perf_timings:
                    perf_timings['metrics_update'] = 0
                perf_timings['metrics_update'] += metrics_time'''
            
            # è¯¦ç»†å†…å­˜ç›‘æ§ï¼ˆæ¯100æ­¥è®°å½•ä¸€æ¬¡ï¼‰
            if self.current_step % 100 == 0 or self.current_step <= 5:
                log_memory_usage(self.current_step, self.agent.create_train_state() if hasattr(self.agent, 'create_train_state') else None, "offline_training")
            
            # ä»…åœ¨å¯ç”¨epochæœºåˆ¶æ—¶è¿›è¡Œepochç»Ÿè®¡
            if acrlpd_config.enable_epoch_based_lr_schedule:
                steps_in_current_epoch += 1
                epoch_losses.append(float(loss_info.total_loss))
                
                # ä½¿ç”¨é…ç½®ä¸­çš„steps_per_epochå‚æ•°
                steps_per_epoch = getattr(self.rl_config.acrlpd, 'steps_per_epoch', 10000)
                
                if steps_in_current_epoch >= steps_per_epoch:
                    # Epochç»“æŸç»Ÿè®¡
                    epoch_time = time.time() - epoch_start_time
                    avg_epoch_loss = np.mean(epoch_losses)
                    lr_info = self._get_current_learning_rates()
                    
                    logger.info(f"=== Epoch {current_epoch} Complete ===")
                    logger.info(f"Steps: {steps_in_current_epoch} | Avg Loss: {avg_epoch_loss:.4f} | Time: {epoch_time:.1f}s | Speed: {steps_in_current_epoch/epoch_time:.1f} steps/s")
                    logger.info(f"Learning Rates: Actor={lr_info['actor_lr']:.6f}, Critic={lr_info['critic_lr']:.6f} (Epoch factor: {lr_info['epoch_factor']:.3f})")
                    
                    # é‡ç½®epochè®¡æ•°å™¨
                    current_epoch += 1
                    steps_in_current_epoch = 0
                    epoch_losses = []
                    epoch_start_time = time.time()
            
            # Professional Training Logging
            if self.current_step % self.rl_config.log_interval == 0:
                # Calculate progress and timing
                progress_pct = (self.current_step / target_steps) * 100
                elapsed_time = time.time() - (time.time() - step_time * self.current_step)  # Approximate
                samples_per_sec = effective_batch_size / step_time if step_time > 0 else 0
                
                # Get current learning rates
                lr_info = self._get_current_learning_rates()
                
                # Extract positive/negative sample info from last batch  
                pos_samples = getattr(self, '_last_batch_pos_samples', 0)
                neg_samples = getattr(self, '_last_batch_neg_samples', 0)
                total_samples = pos_samples + neg_samples
                pos_ratio = pos_samples / max(total_samples, 1) * 100
                
                # Main training log with learning rates and sample composition
                logger.info(
                    f"Step {self.current_step:6d}/{target_steps} ({progress_pct:5.1f}%) | "
                    f"Loss: {float(loss_info.total_loss):.4f} | "
                    f"Critic: {float(loss_info.critic_loss):.4f} | "
                    f"Actor: {float(loss_info.actor_loss):.4f} | "
                    f"BC: {float(loss_info.bc_loss):.4f} | "
                    f"Q_mean: {float(loss_info.q_mean):.3f} | "
                    f"Samples: +{pos_samples}/-{neg_samples} ({pos_ratio:.1f}%) | "
                    f"LR_Actor: {lr_info['actor_lr']:.6f} | "
                    f"LR_Critic: {lr_info['critic_lr']:.6f} | "
                    f"Time: {step_time:.3f}s | "
                    f"Speed: {samples_per_sec:.1f} samples/s | "
                )
                
                self._log_metrics()
            
            # Detailed metrics every 1000 steps
            if self.current_step % (self.rl_config.log_interval * 10) == 0:
                lr_info = self._get_current_learning_rates()
                logger.info(f"=== Detailed Metrics at Step {self.current_step} ===")
                logger.info(
                    f"Losses - Total: {float(loss_info.total_loss):.6f} | "
                    f"Critic: {float(loss_info.critic_loss):.6f} | "
                    f"Actor: {float(loss_info.actor_loss):.6f} | "
                    f"BC: {float(loss_info.bc_loss):.6f} | "
                    f"Alpha: {float(loss_info.alpha_loss):.6f}"
                )
                logger.info(
                    f"Q-Values - Mean: {float(loss_info.q_mean):.3f} | "
                    f"Std: {float(loss_info.q_std):.3f} | "
                    f"Target: {float(loss_info.target_q_mean):.3f} | "
                    f"TD_Error: {float(loss_info.td_error_mean):.4f}"
                )
                logger.info(
                    f"Learning Rates - Actor: {lr_info['actor_lr']:.6f} | "
                    f"Critic: {lr_info['critic_lr']:.6f} | "
                    f"Epoch_Factor: {lr_info['epoch_factor']:.4f} | "
                    f"Intra_Epoch_Factor: {lr_info['intra_epoch_factor']:.4f}"
                )
            
            # Evaluationå·²ç§»é™¤ - å½“å‰æ— çœŸå®ç¯å¢ƒè¯„ä¼°å®ç°
            
            # Checkpointing
            if self.current_step % self.rl_config.save_interval == 0:
                # Sync agent state from FSDP train state before checkpointing
                if self.use_fsdp:
                    self.agent.from_train_state(self.fsdp_train_state)
                
                self.checkpoint_manager.save_checkpoint(
                    self.agent, self.dataloader, self.current_step
                )
            
            # ğŸ¯ å®Œæ•´æ€§èƒ½åˆ†æè¾“å‡º
            '''if enable_perf_analysis:
                total_step_time = time.time() - step_start_time
                perf_timings['total_step'] = total_step_time
                perf_timings.setdefault('param_update', 0.0)  # å¦‚æœæ²¡æœ‰å‚æ•°æ›´æ–°
                
                # è®¡ç®—ä¸»è¦é˜¶æ®µæ—¶é—´
                main_stages_time = (perf_timings['data_loading'] + 
                                   perf_timings['grad_compute'] + 
                                   perf_timings['grad_process'] + 
                                   perf_timings['param_update'])
                
                # è®¡ç®—å…¶ä»–å¼€é”€ç»†åˆ†
                other_stages_time = 0
                other_stages = ['gpu_monitor', 'ema_update', 'metrics_update']
                for stage in other_stages:
                    other_stages_time += perf_timings.get(stage, 0)
                
                # æœªåˆ†ç±»çš„å¼€é”€
                unaccounted_time = total_step_time - main_stages_time - other_stages_time
                
                # æ¯5æ­¥è¾“å‡ºä¸€æ¬¡è¯¦ç»†æ€§èƒ½åˆ†æ
                if self.current_step % 5 == 0:
                    logger.info("=" * 80)
                    logger.info(f"ğŸ¯ æ€§èƒ½åˆ†æ (æ­¥éª¤ {self.current_step}) - æ¢¯åº¦ç´¯ç§¯: {gradient_accumulation_steps}æ­¥")
                    logger.info(f"ğŸ“Š æ€»è€—æ—¶: {total_step_time:.3f}s ({1.0/total_step_time:.1f} sample/s)")
                    
                    # ä¸»è¦é˜¶æ®µ
                    logger.info(f"ğŸ“ˆ æ•°æ®åŠ è½½: {perf_timings['data_loading']:.3f}s ({perf_timings['data_loading']/total_step_time*100:.1f}%)")
                    logger.info(f"ğŸ“ˆ é…ç½®è½¬æ¢: {perf_timings['config_convert']:.3f}s ({perf_timings['config_convert']/total_step_time*100:.1f}%)")
                    logger.info(f"ğŸ“ˆ æ¢¯åº¦è®¡ç®—: {perf_timings['grad_compute']:.3f}s ({perf_timings['grad_compute']/total_step_time*100:.1f}%)")
                    logger.info(f"ğŸ“ˆ æ¢¯åº¦å¤„ç†: {perf_timings['grad_process']:.3f}s ({perf_timings['grad_process']/total_step_time*100:.1f}%)")
                    logger.info(f"ğŸ“ˆ å‚æ•°æ›´æ–°: {perf_timings['param_update']:.3f}s ({perf_timings['param_update']/total_step_time*100:.1f}%)")
                    
                    # å…¶ä»–å¼€é”€ç»†åˆ†
                    if other_stages_time > 0:
                        logger.info(f"ğŸ“ˆ å…¶ä»–å¼€é”€ç»†åˆ†:")
                        for stage in other_stages:
                            if stage in perf_timings and perf_timings[stage] > 0:
                                logger.info(f"    {stage}: {perf_timings[stage]:.3f}s ({perf_timings[stage]/total_step_time*100:.1f}%)")
                    
                    # æœªåˆ†ç±»å¼€é”€
                    if unaccounted_time > 0:
                        logger.info(f"ğŸ“ˆ æœªåˆ†ç±»å¼€é”€: {unaccounted_time:.3f}s ({unaccounted_time/total_step_time*100:.1f}%)")
                    
                    logger.info("=" * 80)'''
            
            self.current_step += 1
        
        # è®­ç»ƒå®Œæˆç»Ÿè®¡
        total_training_time = time.time() - (time.time() - (self.current_step - start_step) * 0.5)  # åŸºäºstepç²—ç•¥ä¼°ç®—
        logger.info(f"=== Offline Training Complete ===")
        logger.info(f"Total Steps: {self.current_step - start_step}")
        if acrlpd_config.enable_epoch_based_lr_schedule:
            logger.info(f"Total Epochs: {current_epoch}")
        logger.info(f"Total Time: {total_training_time/3600:.2f}h | Avg Speed: {(self.current_step - start_step)/(total_training_time/60):.1f} steps/min")
        
        # # === æœ€ç»ˆAC_Trainingæ€§èƒ½æŠ¥å‘Š ===
        # if ac_perf_stats['total_iter_times']:
        #     logger.info("=" * 60)
        #     logger.info("ğŸ” AC_Trainingè®­ç»ƒå®Œæˆ - æœ€ç»ˆæ€§èƒ½æŠ¥å‘Š")
        #     logger.info("=" * 60)
        #     
        #     # JITç¼–è¯‘æ—¶é—´æŠ¥å‘Š
        #     if jit_compilation_stats.get('compilation_completed', False):
        #         if gradient_accumulation_steps > 1:
        #             # æ¢¯åº¦ç§¯ç´¯æ¨¡å¼çš„ç¼–è¯‘ç»Ÿè®¡
        #             total_compile_time = (
        #                 jit_compilation_stats.get('first_grad_compute_compilation', 0) + 
        #                 jit_compilation_stats.get('first_grad_apply_compilation', 0)
        #             )
        #             logger.info("JITç¼–è¯‘æ—¶é—´ç»Ÿè®¡ (æ¢¯åº¦ç§¯ç´¯æ¨¡å¼):")
        #             logger.info(f"  æ€»ç¼–è¯‘æ—¶é—´: {total_compile_time:.2f}ç§’")
        #             logger.info(f"  æ¢¯åº¦è®¡ç®—å‡½æ•°: {jit_compilation_stats.get('first_grad_compute_compilation', 0):.2f}ç§’")
        #             logger.info(f"  å‚æ•°æ›´æ–°å‡½æ•°: {jit_compilation_stats.get('first_grad_apply_compilation', 0):.2f}ç§’")
        #         else:
        #             # ç›´æ¥è®­ç»ƒæ¨¡å¼çš„ç¼–è¯‘ç»Ÿè®¡
        #             direct_compile_time = jit_compilation_stats.get('first_direct_train_compilation', 0)
        #             logger.info("JITç¼–è¯‘æ—¶é—´ç»Ÿè®¡ (ç›´æ¥è®­ç»ƒæ¨¡å¼):")
        #             logger.info(f"  ç›´æ¥è®­ç»ƒæ­¥å‡½æ•°: {direct_compile_time:.2f}ç§’")
        #         logger.info("")
        #     
        #     total_samples = len(ac_perf_stats['total_iter_times'])
        #     avg_total = np.mean(ac_perf_stats['total_iter_times'])
        #     std_total = np.std(ac_perf_stats['total_iter_times'])
        #     
        #     avg_grad_accum = np.mean(ac_perf_stats['grad_accumulation_times'])
        #     avg_data_load = np.mean(ac_perf_stats['data_loading_times'])
        #     avg_grad_compute = np.mean(ac_perf_stats['grad_compute_times'])
        #     avg_grad_process = np.mean(ac_perf_stats['grad_process_times'])
        #     avg_param_update = np.mean([t for t in ac_perf_stats['param_update_times'] if t > 0])
        #     avg_config_convert = np.mean(ac_perf_stats['config_convert_times'])
        #     avg_memory_monitor = np.mean(ac_perf_stats['memory_monitor_times'])
        #     avg_existing_perf = np.mean(ac_perf_stats['existing_perf_analysis_times'])
        #     
        #     logger.info(f"æ ·æœ¬æ•°é‡: {total_samples} æ¬¡è¿­ä»£")
        #     logger.info(f"å¹³å‡è¿­ä»£æ—¶é—´: {avg_total*1000:.2f} Â± {std_total*1000:.2f}ms")
        #     logger.info(f"å¹³å‡æ¯ç§’è¿­ä»£æ•°: {1.0/avg_total:.2f} iter/s")
        #     
        #     # æ ¹æ®æ¨¡å¼æ˜¾ç¤ºä¸åŒçš„æ‰¹æ¬¡ä¿¡æ¯
        #     if gradient_accumulation_steps > 1:
        #         logger.info(f"æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {self.rl_config.batch_size * gradient_accumulation_steps} (æ¢¯åº¦ç§¯ç´¯æ¨¡å¼)")
        #     else:
        #         logger.info(f"æ‰¹æ¬¡å¤§å°: {self.rl_config.batch_size} (ç›´æ¥è®­ç»ƒæ¨¡å¼)")
        #     logger.info("")
        
        """
        # Performance analysis section commented out
        # # # æ ¹æ®æ¨¡å¼æ˜¾ç¤ºä¸åŒçš„æ—¶é—´åˆ†è§£
        # # if gradient_accumulation_steps > 1:
        #     logger.info("æ—¶é—´åˆ†è§£ (æ¢¯åº¦ç§¯ç´¯æ¨¡å¼):")
                logger.info(f"  æ¢¯åº¦ç§¯ç´¯å¾ªç¯: {avg_grad_accum*1000:.2f}ms ({avg_grad_accum/avg_total*100:.1f}%)")
                logger.info(f"    â”œâ”€ æ•°æ®åŠ è½½(x{gradient_accumulation_steps}): {avg_data_load*1000:.2f}ms ({avg_data_load/avg_total*100:.1f}%)")
                logger.info(f"    â”œâ”€ æ¢¯åº¦è®¡ç®—(x{gradient_accumulation_steps}): {avg_grad_compute*1000:.2f}ms ({avg_grad_compute/avg_total*100:.1f}%)")
                logger.info(f"    â”œâ”€ æ¢¯åº¦å¤„ç†(x{gradient_accumulation_steps}): {avg_grad_process*1000:.2f}ms ({avg_grad_process/avg_total*100:.1f}%)")
                logger.info(f"    â””â”€ é…ç½®è½¬æ¢(x{gradient_accumulation_steps}): {avg_config_convert*1000:.2f}ms ({avg_config_convert/avg_total*100:.1f}%)")
                logger.info(f"  å‚æ•°æ›´æ–°: {avg_param_update*1000:.2f}ms ({avg_param_update/avg_total*100:.1f}%)")
                logger.info(f"  å†…å­˜ç›‘æ§: {avg_memory_monitor*1000:.2f}ms ({avg_memory_monitor/avg_total*100:.1f}%)")
                logger.info(f"  ç°æœ‰æ€§èƒ½åˆ†æ: {avg_existing_perf*1000:.2f}ms ({avg_existing_perf/avg_total*100:.1f}%)")
                
                logger.info("")
                logger.info("æ€§èƒ½ç“¶é¢ˆåˆ†æ:")
                bottleneck_info = [
                    ("æ¢¯åº¦ç§¯ç´¯å¾ªç¯", avg_grad_accum, avg_grad_accum/avg_total*100),
                    ("æ•°æ®åŠ è½½", avg_data_load, avg_data_load/avg_total*100),
                    ("æ¢¯åº¦è®¡ç®—", avg_grad_compute, avg_grad_compute/avg_total*100),
                    ("æ¢¯åº¦å¤„ç†", avg_grad_process, avg_grad_process/avg_total*100),
                    ("å‚æ•°æ›´æ–°", avg_param_update, avg_param_update/avg_total*100),
                    ("å†…å­˜ç›‘æ§", avg_memory_monitor, avg_memory_monitor/avg_total*100),
                ]
            else:
                # éæ¢¯åº¦ç§¯ç´¯æ¨¡å¼çš„ç»Ÿè®¡
                avg_direct_train = np.mean(ac_perf_stats.get('direct_train_times', [0]))
                avg_loss_convert = np.mean(ac_perf_stats.get('loss_convert_times', [0]))
                
                logger.info("æ—¶é—´åˆ†è§£ (ç›´æ¥è®­ç»ƒæ¨¡å¼):")
                logger.info(f"  æ•°æ®åŠ è½½: {avg_data_load*1000:.2f}ms ({avg_data_load/avg_total*100:.1f}%)")
                logger.info(f"  ç›´æ¥è®­ç»ƒæ­¥: {avg_direct_train*1000:.2f}ms ({avg_direct_train/avg_total*100:.1f}%)")
                logger.info(f"    â”œâ”€ æ¢¯åº¦è®¡ç®—+å‚æ•°æ›´æ–°: {avg_grad_compute*1000:.2f}ms ({avg_grad_compute/avg_total*100:.1f}%)")
                logger.info(f"    â””â”€ å†…éƒ¨å‚æ•°æ›´æ–°: {avg_param_update*1000:.2f}ms ({avg_param_update/avg_total*100:.1f}%)")
                logger.info(f"  æŸå¤±ä¿¡æ¯è½¬æ¢: {avg_loss_convert*1000:.2f}ms ({avg_loss_convert/avg_total*100:.1f}%)")
                logger.info(f"  å†…å­˜ç›‘æ§: {avg_memory_monitor*1000:.2f}ms ({avg_memory_monitor/avg_total*100:.1f}%)")
                logger.info(f"  ç°æœ‰æ€§èƒ½åˆ†æ: {avg_existing_perf*1000:.2f}ms ({avg_existing_perf/avg_total*100:.1f}%)")
                
                logger.info("")
                logger.info("æ€§èƒ½ç“¶é¢ˆåˆ†æ:")
                bottleneck_info = [
                    ("ç›´æ¥è®­ç»ƒæ­¥", avg_direct_train, avg_direct_train/avg_total*100),
                    ("æ•°æ®åŠ è½½", avg_data_load, avg_data_load/avg_total*100),
                    ("æ¢¯åº¦è®¡ç®—+å‚æ•°æ›´æ–°", avg_grad_compute, avg_grad_compute/avg_total*100),
                    ("æŸå¤±ä¿¡æ¯è½¬æ¢", avg_loss_convert, avg_loss_convert/avg_total*100),
                    ("å†…å­˜ç›‘æ§", avg_memory_monitor, avg_memory_monitor/avg_total*100),
                ]
            
            bottleneck_info.sort(key=lambda x: x[2], reverse=True)
            for i, (name, time_val, pct) in enumerate(bottleneck_info[:3]):
                logger.info(f"  {i+1}. {name}: {time_val*1000:.2f}ms ({pct:.1f}%)")
            logger.info("=" * 60)
        """
        
        return self.agent
    
    def _create_jit_compute_gradients(self):
        """åˆ›å»ºJITç¼–è¯‘çš„æ¢¯åº¦è®¡ç®—å‡½æ•°ã€‚"""
        from training.acrlpd_train_state import acrlpd_compute_gradients
        import openpi.training.sharding as sharding
        
        jit_compute_gradients = jax.jit(
            acrlpd_compute_gradients, 
            static_argnames=['config']
        )
        
        logger.debug("âœ… JITæ¢¯åº¦è®¡ç®—å‡½æ•°ç¼–è¯‘å®Œæˆ")
        return jit_compute_gradients
    
    def _create_jit_apply_gradients(self):
        """åˆ›å»ºJITç¼–è¯‘çš„æ¢¯åº¦åº”ç”¨å‡½æ•°ã€‚"""
        from training.acrlpd_train_state import acrlpd_apply_gradients
        import openpi.training.sharding as sharding
        
        jit_apply_gradients = jax.jit(
            acrlpd_apply_gradients,
            static_argnames=['config']
        )
        
        logger.debug("âœ… JITæ¢¯åº¦åº”ç”¨å‡½æ•°ç¼–è¯‘å®Œæˆ")
        return jit_apply_gradients
    
    # _run_evaluationæ–¹æ³•å·²ç§»é™¤ - å½“å‰æ— çœŸå®ç¯å¢ƒè¯„ä¼°å®ç°
    def _log_metrics(self):
        """Log current metrics."""
        if self.rl_config.wandb_enabled:
            wandb.log(self.metrics.metrics, step=self.current_step)
        
        # Console logging
        if self.current_step % (self.rl_config.log_interval * 10) == 0:
            logger.info(f"Step {self.current_step}: "
                       f"Loss={self.metrics.metrics.get('train/total_loss', 0):.4f}, "
                       f"Q_mean={self.metrics.metrics.get('train/q_mean', 0):.3f}")
    
    def _final_evaluation(self):
        """Run final comprehensive evaluation."""
        logger.info("Running final evaluation")
        self._run_evaluation()
        
        # Sync agent state from FSDP train state before final checkpoint
        if self.use_fsdp:
            self.agent.from_train_state(self.fsdp_train_state)
        
        # Save final checkpoint
        final_checkpoint = self.checkpoint_manager.save_checkpoint(
            self.agent, self.dataloader, self.current_step
        )
        
        logger.info(f"Final checkpoint saved: {final_checkpoint}")
        
        if self.rl_config.wandb_enabled:
            wandb.finish()
    
    def _resume_training(self, checkpoint_path: str):
        """Resume training from checkpoint."""
        logger.info(f"ğŸ”„ Resuming training from {checkpoint_path}")
        try:
            # ğŸ” ä¿®å¤å‚æ•°ä¼ é€’ï¼šä»checkpoint_pathä¸­æå–step
            from pathlib import Path
            checkpoint_dir = Path(checkpoint_path)
            if checkpoint_dir.name.isdigit():
                step_to_load = int(checkpoint_dir.name)
            else:
                step_to_load = None
            
            # Load checkpoint using the checkpoint manager
            self.agent, self.current_step, saved_metrics = self.checkpoint_manager.load_checkpoint(
                self.agent, self.dataloader, step=step_to_load
            )
            
            # ğŸ” æ›´æ–°FSDP train state
            if self.use_fsdp and hasattr(self, 'fsdp_train_state'):
                # ä»æ¢å¤çš„agentåŒæ­¥FSDP state
                logger.info("ğŸ”„ Synchronizing FSDP train state from resumed agent")
                # é‡è¦ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„ACRLPD FSDPåˆå§‹åŒ–å‡½æ•°
                # ä¸è°ƒç”¨agent.create_train_state()å› ä¸ºå®ƒè¿”å›OpenPI TrainStateè€ŒéACRLPDTrainState
                from training.acrlpd_train_state import init_acrlpd_fsdp_training
                self.fsdp_train_state, self.train_state_sharding, self.jit_train_step_fn = init_acrlpd_fsdp_training(
                    rl_config=self.rl_config,
                    mesh=self.mesh,
                    rng=self.rng,
                    data_sharding=self.data_sharding,
                    step=self.current_step,
                    global_pi0_tx=self.global_pi0_tx,
                    global_critic_tx=self.global_critic_tx
                )
                logger.info("âœ… FSDP train state synchronized successfully")
            
            # Update metrics with saved values
            if saved_metrics:
                self.metrics.metrics.update(saved_metrics)
                logger.info("âœ… Restored training metrics")
            else:
                logger.info("â„¹ï¸ No metrics found, starting fresh metrics tracking")
            
            logger.info(f"âœ… Successfully resumed training from step {self.current_step}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to resume training from {checkpoint_path}: {e}")
            import traceback
            logger.error(f"Full traceback: {traceback.format_exc()}")
            raise


# create_simple_eval_fnå‡½æ•°å·²ç§»é™¤ - è™šå‡evalå®ç°å·²åˆ é™¤