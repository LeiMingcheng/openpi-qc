#!/usr/bin/env python3
"""
Training script for ACRLPD + Ï€â‚€ integration.

This script provides a complete command-line interface for training ACRLPD
agents with Ï€â‚€ models on robotic manipulation tasks.

Usage:
    # Basic training with H5 data
    python train_acrlpd_pi0.py --data_dir /path/to/data --config droid

    # Training with LeRobot dataset
    python train_acrlpd_pi0.py --lerobot_repo_id lerobot/aloha_sim_insertion_human --config aloha

    # Resume from checkpoint
    python train_acrlpd_pi0.py --data_dir /path/to/data --config aloha --resume /path/to/checkpoint

    # Custom hyperparameters
    python train_acrlpd_pi0.py --lerobot_repo_id lerobot/pusht --config libero \
        --horizon_length 10 --bc_alpha 0.01 --batch_size 128

    # Disable WandB logging
    python train_acrlpd_pi0.py --data_dir /path/to/data --config droid --no_wandb
"""

import argparse
import logging
import os
import sys
from pathlib import Path
from typing import Optional

import jax
import jax.numpy as jnp
import numpy as np

# CRITICAL: Set HuggingFace cache to data disk, not system disk
# This prevents "No space left on device" errors and enables local dataset access
os.environ['HF_HOME'] = '/era-ai/lm/dataset/huggingface'
os.environ['HF_CACHE_HOME'] = '/era-ai/lm/dataset/huggingface_cache'
os.environ['HF_DATASETS_CACHE'] = '/era-ai/lm/dataset/huggingface_cache/datasets'
os.environ['HF_LEROBOT_HOME'] = '/era-ai/lm/dataset/huggingface_cache/lerobot'

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from config import get_config, RLTrainConfig
from agents.acrlpd_pi0_agent import ACRLPDPi0Agent, create_acrlpd_pi0_agent, create_acrlpd_pi0_agent_from_rl_config
from training.training_loop import ACRLPDTrainer, ACRLPDTrainingConfig
from data import create_acrlpd_data_loader

import openpi.models.pi0 as _pi0
import openpi.training.config as openpi_config
import openpi.training.optimizer as _optimizer
import openpi.training.sharding as sharding

# Reduce verbose output before imports
os.environ.setdefault("TF_CPP_MIN_LOG_LEVEL", "1")      # Suppress TF info logs
os.environ.setdefault("JAX_LOG_COMPILES", "0")          # Disable JAX compilation logs

# Setup logging - å‡å°‘è¯¦ç»†è¾“å‡º
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# Suppress verbose output from JAX/Flax/transformers libraries
logging.getLogger("jax").setLevel(logging.WARNING)
logging.getLogger("flax").setLevel(logging.WARNING)  
logging.getLogger("optax").setLevel(logging.WARNING)
logging.getLogger("transformers").setLevel(logging.WARNING)
logging.getLogger("torch").setLevel(logging.WARNING)
logging.getLogger("absl").setLevel(logging.WARNING)
logger = logging.getLogger(__name__)


def create_argument_parser() -> argparse.ArgumentParser:
    """Create command-line argument parser for unified config system."""
    parser = argparse.ArgumentParser(
        description="Train ACRLPD + Ï€â‚€ agents using unified configuration system",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # Configuration selection (main argument)
    parser.add_argument(
        "--config", type=str, required=True,
        choices=["rl_aloha_fold", "rl_fold_box", "rl_libero", "rl_droid"],
        help="Predefined configuration for different robot platforms"
    )
    
    # Experiment setup
    parser.add_argument(
        "--exp_name", type=str, required=True,
        help="Experiment name for checkpoints and logging"
    )
    
    # Training control
    parser.add_argument(
        "--resume", action="store_true",
        help="Resume from last checkpoint"
    )
    parser.add_argument(
        "--overwrite", action="store_true", 
        help="Overwrite existing experiment"
    )
    
    # Environment (for online training)
    parser.add_argument(
        "--env_name", type=str, default=None,
        help="Environment name for online training evaluation"
    )
    
    # System configuration
    parser.add_argument(
        "--seed", type=int, default=42,
        help="Random seed"
    )
    
    # Debug options
    parser.add_argument(
        "--debug", action="store_true",
        help="Enable debug mode"
    )
    parser.add_argument(
        "--debug-transforms", action="store_true",
        help="Enable detailed transforms debugging output"
    )
    parser.add_argument(
        "--no_wandb", action="store_true",
        help="Disable WandB logging"
    )
    parser.add_argument(
        "--dry_run", action="store_true",
        help="Setup without training"
    )
    
    # FSDPä¿®å¤æ–¹æ¡ˆé€‰æ‹©
    parser.add_argument(
        "--fsdp_fix", type=str, default="direct", 
        choices=["direct", "manual"],
        help="é€‰æ‹©FSDPä¿®å¤æ–¹æ¡ˆ: direct(é»˜è®¤) æˆ– manual"
    )
    
    return parser


def load_rl_config(args: argparse.Namespace) -> RLTrainConfig:
    """åŠ è½½å¹¶å®šåˆ¶RLTrainConfig"""
    # è·å–é¢„å®šä¹‰é…ç½®
    rl_config = get_config(args.config)
    
    # ä½¿ç”¨dataclasses.replaceæ›´æ–°å®éªŒåå’Œé€‰é¡¹
    import dataclasses
    rl_config = dataclasses.replace(
        rl_config,
        exp_name=args.exp_name,
        resume=args.resume,
        overwrite=args.overwrite,
        seed=args.seed,
        wandb_enabled=not args.no_wandb
    )
    
    return rl_config


def log_gpu_memory(step_name: str = "", alert_threshold: float = 95.0):
    """Enhanced GPU memory logging with step tracking and alerts."""
    try:
        import subprocess
        result = subprocess.run(['nvidia-smi', '--query-gpu=index,memory.used,memory.total', '--format=csv,noheader,nounits'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            step_prefix = f"ğŸ” GPU Memory - {step_name}: " if step_name else "ğŸ” GPU Memory Status:"
            logger.info(step_prefix)
            total_used = 0
            total_capacity = 0
            usage_data = []
            max_usage_gpu = 0
            max_usage_mb = 0
            
            for line in result.stdout.strip().split('\n'):
                parts = line.split(', ')
                if len(parts) == 3:
                    gpu_id, used, total = parts
                    used_mb = int(used)
                    total_mb = int(total)
                    usage_pct = (used_mb / total_mb) * 100
                    total_used += used_mb
                    total_capacity += total_mb
                    usage_data.append(used_mb)
                    
                    # è®°å½•æœ€é«˜ä½¿ç”¨GPU
                    if used_mb > max_usage_mb:
                        max_usage_mb = used_mb
                        max_usage_gpu = int(gpu_id)
                    
                    logger.info(f"  GPU {gpu_id}: {used}MB/{total}MB ({usage_pct:.1f}%)")
            
            # è®¡ç®—ç»Ÿè®¡æ•°æ®
            overall_pct = (total_used / total_capacity) * 100 if total_capacity > 0 else 0
            avg_per_gpu = total_used / len(usage_data) if len(usage_data) > 0 else 0
            std_dev = (sum((x - avg_per_gpu) ** 2 for x in usage_data) / len(usage_data)) ** 0.5 if len(usage_data) > 0 else 0
            
            logger.info(f"  Total: {total_used}MB/{total_capacity}MB ({overall_pct:.1f}%) | Avg/GPU: {avg_per_gpu:.0f}MB")
            logger.info(f"  Max GPU: GPU{max_usage_gpu} ({max_usage_mb}MB) | StdDev: {std_dev:.0f}MB")
            
            # FSDPæ•ˆæœåˆ†æ
            analyze_fsdp_effectiveness(usage_data, avg_per_gpu, std_dev)
            
            # å†…å­˜å¼‚å¸¸æŠ¥è­¦
            if overall_pct > alert_threshold:
                logger.error(f"ğŸš¨ GPUå†…å­˜è­¦å‘Š: {overall_pct:.1f}% > {alert_threshold}% é˜ˆå€¼!")
                if max_usage_mb > 76000:  # 76GB alarm (95% of 80GB)
                    logger.error(f"ğŸš¨ ä¸¥é‡è­¦å‘Š: GPU{max_usage_gpu}ä½¿ç”¨{max_usage_mb}MBï¼Œæ¥è¿‘80GBä¸Šé™!")
            
            # è¿”å›æ•°æ®ç”¨äºè¿›ä¸€æ­¥åˆ†æ
            return {
                'usage_data': usage_data,
                'avg_per_gpu': avg_per_gpu,
                'max_usage': max_usage_mb,
                'max_usage_gpu': max_usage_gpu,
                'std_dev': std_dev,
                'overall_pct': overall_pct
            }
                    
    except Exception as e:
        logger.warning(f"Failed to get GPU memory info: {e}")
        return None


def analyze_fsdp_effectiveness(usage_data, avg_per_gpu, std_dev):
    """åˆ†æFSDPåˆ†ç‰‡æ•ˆæœ"""
    if len(usage_data) < 2 or avg_per_gpu == 0:
        return
        
    # è®¡ç®—å˜å¼‚ç³»æ•° (CV = std_dev / mean)
    cv = std_dev / avg_per_gpu
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„å†…å­˜ä¸å‡
    min_usage = min(usage_data)
    max_usage = max(usage_data)
    usage_range = max_usage - min_usage
    
    logger.info(f"  FSDPåˆ†æ: CV={cv:.3f}, Range={usage_range:.0f}MB ({min_usage:.0f}-{max_usage:.0f})")
    
    if cv < 0.1 and usage_range < 5000:  # å˜å¼‚ç³»æ•°<0.1ä¸”èŒƒå›´<5GB
        logger.info("  âœ… FSDPæ•ˆæœä¼˜ç§€ï¼šå†…å­˜ä½¿ç”¨å‡åŒ€ï¼Œåˆ†ç‰‡å·¥ä½œè‰¯å¥½")
    elif cv < 0.3 and usage_range < 15000:  # å˜å¼‚ç³»æ•°<0.3ä¸”èŒƒå›´<15GB
        logger.info("  âš ï¸  FSDPæ•ˆæœä¸€èˆ¬ï¼šå­˜åœ¨ä¸€å®šå†…å­˜ä¸å‡")
    elif max_usage > 50000:  # ä»»ä½•GPUè¶…è¿‡50GB
        logger.info("  âŒ FSDPåˆ†ç‰‡å¯èƒ½å¤±æ•ˆï¼šå­˜åœ¨é«˜å†…å­˜ä½¿ç”¨GPU")
        # æ£€æµ‹æ˜¯å¦ä¸ºå¤åˆ¶è€Œéåˆ†ç‰‡
        high_usage_count = sum(1 for x in usage_data if x > 40000)
        if high_usage_count >= len(usage_data) * 0.8:  # 80%ä»¥ä¸ŠGPUé«˜å†…å­˜ä½¿ç”¨
            logger.error("  ğŸš¨ FSDPå®Œå…¨å¤±æ•ˆï¼šå‚æ•°è¢«å¤åˆ¶åˆ°å¤šä¸ªGPUè€Œéåˆ†ç‰‡!")
    else:
        logger.info("  âœ… FSDPåŸºæœ¬æ­£å¸¸ï¼šå†…å­˜ä½¿ç”¨åœ¨åˆç†èŒƒå›´å†…")


def verify_fsdp_sharding(train_state, step_name: str = ""):
    """éªŒè¯FSDPåˆ†ç‰‡æ˜¯å¦æ­£ç¡®å·¥ä½œ"""
    step_prefix = f"ğŸ” FSDPéªŒè¯ - {step_name}: " if step_name else "ğŸ” FSDPåˆ†ç‰‡éªŒè¯:"
    logger.info(step_prefix)
    
    try:
        # æ£€æŸ¥å…³é”®å‚æ•°çš„åˆ†ç‰‡æƒ…å†µ
        if hasattr(train_state, 'pi0_params'):
            param_count = 0
            sharded_count = 0
            replicated_count = 0
            
            def check_sharding(param_path, param):
                nonlocal param_count, sharded_count, replicated_count
                if hasattr(param, 'sharding') and hasattr(param, 'shape'):
                    param_count += 1
                    size_mb = param.nbytes / (1024 * 1024)
                    
                    # æ£€æŸ¥åˆ†ç‰‡çŠ¶æ€
                    if hasattr(param.sharding, 'spec'):
                        spec = param.sharding.spec
                        is_sharded = any(axis is not None for axis in spec)
                        
                        if is_sharded:
                            sharded_count += 1
                            if size_mb > 10:  # åªè®°å½•å¤§å‚æ•°
                                logger.info(f"  âœ… SHARDED: {param_path} ({param.shape}, {size_mb:.1f}MB)")
                        else:
                            replicated_count += 1
                            if size_mb > 1:  # è®°å½•ä¸­ç­‰å¤§å°å‚æ•°
                                logger.info(f"  ğŸ”„ REPLICATED: {param_path} ({param.shape}, {size_mb:.1f}MB)")
            
            # éå†å‚æ•°
            def traverse_params(params, prefix=""):
                if isinstance(params, dict):
                    for key, value in params.items():
                        new_prefix = f"{prefix}.{key}" if prefix else key
                        traverse_params(value, new_prefix)
                elif hasattr(params, 'shape'):
                    check_sharding(prefix, params)
                elif hasattr(params, 'value') and hasattr(params.value, 'shape'):
                    check_sharding(prefix, params.value)
            
            traverse_params(train_state.pi0_params, "pi0_params")
            
            # æ€»ç»“åˆ†ç‰‡æ•ˆæœ
            if param_count > 0:
                sharded_pct = (sharded_count / param_count) * 100
                logger.info(f"  ğŸ“Š åˆ†ç‰‡ç»Ÿè®¡: {sharded_count}/{param_count} å‚æ•°è¢«åˆ†ç‰‡ ({sharded_pct:.1f}%)")
                
                if sharded_pct > 70:
                    logger.info("  âœ… FSDPåˆ†ç‰‡æ•ˆæœè‰¯å¥½")
                elif sharded_pct > 30:
                    logger.info("  âš ï¸  FSDPåˆ†ç‰‡æ•ˆæœä¸­ç­‰")
                else:
                    logger.info("  âŒ FSDPåˆ†ç‰‡å¯èƒ½å¤±æ•ˆ")
            else:
                logger.info("  âš ï¸  æœªæ‰¾åˆ°å¯æ£€æŸ¥çš„å‚æ•°")
                
    except Exception as e:
        logger.warning(f"FSDPéªŒè¯å¤±è´¥: {e}")


def setup_jax_environment_with_fsdp(args: argparse.Namespace, rl_config: RLTrainConfig):
    """Setup JAX environment with OpenPI FSDP support."""
    logger.info(f"JAX version: {jax.__version__}")
    logger.info(f"Available devices: {jax.devices()}")
    
    # Log initial GPU memory
    log_gpu_memory("INITIAL")
    
    # OpenPIæ ‡å‡†ï¼šæ£€æŸ¥æ‰¹æ¬¡å¤§å°ä¸è®¾å¤‡æ•°é‡çš„å…¼å®¹æ€§
    if rl_config.batch_size % jax.device_count() != 0:
        raise ValueError(
            f"Batch size {rl_config.batch_size} must be divisible by device count {jax.device_count()}"
        )
    
    # åˆ›å»ºOpenPIæ ‡å‡†meshå’Œsharding
    mesh = sharding.make_mesh(rl_config.fsdp_devices)
    
    # OpenPIæ ‡å‡†ï¼šå§‹ç»ˆä½¿ç”¨DATA_AXISè¿›è¡Œåˆ†ç‰‡
    data_sharding = jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec(sharding.DATA_AXIS))
    replicated_sharding = jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec())
    
    logger.info(f"Created FSDP mesh: {mesh}")
    logger.info(f"Mesh shape: {mesh.shape}")
    logger.info(f"Data sharding: {data_sharding}")
    
    # ğŸ” éªŒè¯OpenPIæ ‡å‡†FSDPé…ç½®
    batch_per_device = rl_config.batch_size // jax.device_count()
    logger.info("âœ… OpenPIæ ‡å‡†FSDPé…ç½®éªŒè¯:")
    logger.info(f"   æ€»batch_size: {rl_config.batch_size}")
    logger.info(f"   è®¾å¤‡æ•°é‡: {jax.device_count()}")
    logger.info(f"   æ¯è®¾å¤‡batch: {batch_per_device}")
    logger.info(f"   æ•°æ®åˆ†ç‰‡ç­–ç•¥: {data_sharding}")
    
    # Log GPU memory after FSDP setup
    log_gpu_memory("FSDP_SETUP")
    
    # Set memory preallocation
    os.environ.setdefault("XLA_PYTHON_CLIENT_MEM_FRACTION", "0.9")
    
    if args.debug:
        jax.config.update("jax_debug_nans", True)
        jax.config.update("jax_debug_infs", True)
    
    return mesh, data_sharding, replicated_sharding


def create_acrlpd_data_loader_with_sharding(rl_config: RLTrainConfig, batch_size: int, data_sharding: jax.sharding.Sharding, replicated_sharding: jax.sharding.Sharding, debug_transforms: bool = False):
    """Create ACRLPD data loader with FSDP sharding support."""
    # åŸºäºç°æœ‰create_acrlpd_data_loaderï¼Œæ·»åŠ shardingå‚æ•°
    dataloader = create_acrlpd_data_loader(
        rl_config=rl_config,
        batch_size=batch_size,
        episodes_per_memory_pool=rl_config.episodes_per_memory_pool,
        debug_transforms=debug_transforms  # ğŸ”‘ ä¼ é€’debugå‚æ•°
    )
    
    # ä¼ é€’æ€§èƒ½åˆ†æå‚æ•°åˆ°data loader
    if hasattr(rl_config, 'enable_perf_analysis') and rl_config.enable_perf_analysis:
        dataloader.enable_perf_analysis = True
    
    # åŒ…è£…æ•°æ®åŠ è½½å™¨ä»¥æ”¯æŒåˆ†ç‰‡
    class ShardedACRLPDDataLoader:
        def __init__(self, base_loader, sharding, replicated_sharding):
            self.base_loader = base_loader
            self.sharding = sharding
            self.replicated_sharding = replicated_sharding
        
        def __iter__(self):
            for batch in self.base_loader:
                # OpenPIæ ‡å‡†ï¼šä½¿ç”¨make_array_from_process_local_dataè¿›è¡Œåˆ†ç‰‡
                yield jax.tree.map(
                    lambda x: jax.make_array_from_process_local_data(self.sharding, np.asarray(x)), 
                    batch
                )
        
        def __len__(self):
            return len(self.base_loader)
            
        def sample_batch(self, max_retries: int = 3):
            """OpenPIæ ‡å‡†çš„FSDPåˆ†ç‰‡åº”ç”¨æ–¹æ³• - ä¿®å¤æ ‡é‡å­—æ®µåˆ†ç‰‡é—®é¢˜ï¼Œæ”¯æŒå¼‚å¸¸é‡è¯•"""
            import time
            
            # ğŸ¯ æ€§èƒ½åˆ†æï¼šFSDPåˆ†ç‰‡ç»†ç²’åº¦è®¡æ—¶
            enable_perf_analysis = getattr(self.base_loader, 'enable_perf_analysis', False)
            if enable_perf_analysis:
                total_start = time.time()
                perf_timings = {}
            
            # Step 1: åŸºç¡€æ•°æ®åŠ è½½ï¼ˆæ”¯æŒé‡è¯•ï¼‰
            if enable_perf_analysis:
                base_load_start = time.time()
            
            # ğŸš€ å…³é”®ä¿®å¤ï¼šæ·»åŠ å¼‚å¸¸æ•è·å’Œé‡è¯•æœºåˆ¶
            for retry_count in range(max_retries):
                try:
                    batch = self.base_loader.sample_batch()
                    break  # æˆåŠŸè·å–batchï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                except ValueError as e:
                    if "æ£€æµ‹åˆ°æç«¯å¼‚å¸¸æ•°æ®ï¼Œå½“å‰batchçš„episodeså·²åŠ å…¥ç°åå•" in str(e):
                        logger.warning(f"ğŸ”„ è®­ç»ƒå±‚æ£€æµ‹åˆ°æ•°æ®å¼‚å¸¸ï¼Œé‡è¯•é‡‡æ · ({retry_count + 1}/{max_retries}): {e}")
                        
                        # è®°å½•episodeç»Ÿè®¡ä¿¡æ¯
                        if hasattr(self.base_loader, 'log_episode_statistics'):
                            self.base_loader.log_episode_statistics(level='warning')
                        
                        if retry_count < max_retries - 1:
                            time.sleep(0.1)  # çŸ­æš‚å»¶è¿Ÿ
                            continue
                        else:
                            logger.error(f"âŒ è®­ç»ƒå±‚é‡è¯•æ¬¡æ•°ç”¨å®Œ ({max_retries}), æ•°æ®é‡‡æ ·å¤±è´¥")
                            raise RuntimeError(f"è®­ç»ƒæ— æ³•ç»§ç»­ï¼šæ•°æ®é‡‡æ ·é‡è¯• {max_retries} æ¬¡åä»ç„¶å¤±è´¥") from e
                    else:
                        # å…¶ä»–ç±»å‹çš„ValueErrorï¼Œç›´æ¥æŠ›å‡º
                        raise
                except Exception as e:
                    logger.error(f"âŒ æ•°æ®åŠ è½½æ—¶å‘ç”Ÿæœªé¢„æœŸå¼‚å¸¸: {e}")
                    raise
            
            if enable_perf_analysis:
                perf_timings['base_data_loading'] = time.time() - base_load_start
            
            # Step 2: FSDPåˆ†ç‰‡åº”ç”¨
            if enable_perf_analysis:
                sharding_start = time.time()
            
            # ğŸ”§ ä¿®å¤ï¼šåŒºåˆ†æ ‡é‡å’Œå¼ é‡å­—æ®µçš„åˆ†ç‰‡ç­–ç•¥
            def apply_appropriate_sharding(path, x):
                """æ ¹æ®å­—æ®µç±»å‹åº”ç”¨åˆé€‚çš„åˆ†ç‰‡ç­–ç•¥"""
                path_str = '.'.join(str(p) for p in path) if isinstance(path, tuple) else str(path)
                x_array = np.asarray(x)
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºæ ‡é‡å­—æ®µï¼ˆrank 0æˆ–å½¢çŠ¶ä¸º()ï¼‰
                is_scalar_field = (
                    x_array.ndim == 0 or  # æ ‡é‡æ•°ç»„
                    x_array.shape == () or  # ç©ºå½¢çŠ¶
                    # å·²çŸ¥çš„æ ‡é‡å­—æ®µå
                    any(scalar_name in path_str for scalar_name in [
                        'reward', 'done', 'terminated', 'truncated', 
                        'episode_id', 'step_count', 'timestamp',
                        'success', 'failure', 'timeout',
                        # ACRLPDç‰¹å®šçš„æ ‡é‡å­—æ®µ
                        'negative_samples', 'positive_samples',
                        'sample_count', 'episode_count'
                    ])
                )
                
                
                if is_scalar_field:
                    # æ ‡é‡å­—æ®µä½¿ç”¨replicatedåˆ†ç‰‡
                    return jax.make_array_from_process_local_data(self.replicated_sharding, x_array)
                else:
                    # å¼ é‡å­—æ®µä½¿ç”¨dataåˆ†ç‰‡
                    return jax.make_array_from_process_local_data(self.sharding, x_array)
            
            result = jax.tree_util.tree_map_with_path(apply_appropriate_sharding, batch)
            
            if enable_perf_analysis:
                perf_timings['fsdp_sharding'] = time.time() - sharding_start
                total_time = time.time() - total_start
                perf_timings['total'] = total_time
                
                # æ¯10ä¸ªbatchè¾“å‡ºä¸€æ¬¡FSDPåˆ†ç‰‡åˆ†æï¼ˆå‡å°‘æ—¥å¿—é‡ï¼‰
                if not hasattr(self, '_fsdp_perf_counter'):
                    self._fsdp_perf_counter = 0
                self._fsdp_perf_counter += 1
                
                if self._fsdp_perf_counter % 5 == 0:
                    import logging
                    logger = logging.getLogger(__name__)
                    logger.info(f"ğŸ” FSDPåˆ†ç‰‡ç»†ç²’åº¦åˆ†æ: æ€»è€—æ—¶ {total_time:.3f}s")
                    for stage, timing in perf_timings.items():
                        if stage != 'total':
                            logger.info(f"    {stage}: {timing:.3f}s ({timing/total_time*100:.1f}%)")
            
            return result
            
        def data_config(self):
            """Proxy data_config method."""
            if hasattr(self.base_loader, 'data_config'):
                return self.base_loader.data_config
            return None
            
        def __getattr__(self, name):
            """Proxy any other attributes to the base loader."""
            return getattr(self.base_loader, name)
    
    return ShardedACRLPDDataLoader(dataloader, data_sharding, replicated_sharding)


# create_agent_with_fsdp function removed to avoid nested mesh context
# FSDP initialization now handled directly in training loop using OpenPI patterns


def main():
    """Main training function using unified configuration system."""
    parser = create_argument_parser()
    args = parser.parse_args()
    
    # Setup logging
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
        logger.debug("Debug mode enabled")
    
    logger.info("ğŸš€ Starting ACRLPD + Ï€â‚€ training with OpenPI FSDP")
    logger.info(f"Arguments: {vars(args)}")
    
    # Load unified configuration first
    logger.info(f"Loading configuration: {args.config}")
    rl_config = load_rl_config(args)
    
    # Setup JAX environment with FSDP support
    mesh, data_sharding, replicated_sharding = setup_jax_environment_with_fsdp(args, rl_config)
    
    try:
        # Create training config for additional parameters
        training_config = ACRLPDTrainingConfig(
            env_name=args.env_name
        )
        
        logger.info(f"âœ“ Configuration loaded successfully")
        logger.info(f"  Model: {type(rl_config.model).__name__}")
        logger.info(f"  Dataset: {rl_config.data.repo_id if hasattr(rl_config.data, 'repo_id') else 'Unknown'}")
        logger.info(f"  Batch size: {rl_config.batch_size}")
        logger.info(f"  Training steps: {rl_config.num_train_steps}")
        logger.info(f"  FSDP devices: {rl_config.fsdp_devices}")
        
        if args.dry_run:
            logger.info("âœ“ Dry run completed successfully")
            return
        
        # Set random seed
        np.random.seed(rl_config.seed)
        rng = jax.random.PRNGKey(rl_config.seed)
        
        # Create data loader with FSDP sharding support
        logger.info("Creating data loader with FSDP sharding...")
        dataloader = create_acrlpd_data_loader_with_sharding(
            rl_config=rl_config,
            batch_size=rl_config.batch_size,
            data_sharding=data_sharding,
            replicated_sharding=replicated_sharding,
            debug_transforms=getattr(args, 'debug_transforms', False)  # ğŸ”‘ ä½¿ç”¨ä¸“é—¨çš„transforms debugå‚æ•°
        )
        logger.info(f"Data loader created with FSDP sharding support")
        
        # Create agent with proper FSDP support directly
        logger.info("Creating ACRLPD + Ï€â‚€ agent with FSDP...")
        logger.info(f"Debug: About to split RNG...")
        rng, agent_rng = jax.random.split(rng)
        logger.info(f"Debug: RNG split complete, entering mesh context...")
        
        # **å…³é”®ä¿®å¤ï¼šåˆ›å»ºå…¨å±€ä¼˜åŒ–å™¨é¿å…pytreeå…ƒæ•°æ®ä¸åŒ¹é…**
        logger.info("ğŸ”§ åˆ›å»ºå…¨å±€ä¼˜åŒ–å™¨å®ä¾‹ä»¥ç¡®ä¿pytreeä¸€è‡´æ€§...")
        
        # åœ¨FSDPä¸Šä¸‹æ–‡å¤–åˆ›å»ºå…¨å±€ä¼˜åŒ–å™¨ï¼ˆç¡®ä¿ä¸€è‡´æ€§ï¼‰
        import openpi.training.optimizer as _optimizer
        
        if rl_config.acrlpd.enable_epoch_based_lr_schedule:
            # ä½¿ç”¨åŠ¨æ€åŒå±‚å­¦ä¹ ç‡è°ƒèŠ‚
            import optax
            from training.training_loop import create_dynamic_lr_schedule
            
            # åˆ›å»ºåŠ¨æ€å­¦ä¹ ç‡è°ƒèŠ‚
            actor_schedule = create_dynamic_lr_schedule(
                base_lr=rl_config.acrlpd.actor_lr,  # ä½¿ç”¨é…ç½®ä¸­çš„actor_lr
                total_steps=rl_config.num_train_steps,
                warmup_epochs=rl_config.acrlpd.warmup_epochs,
                total_epochs=rl_config.acrlpd.total_epochs,
                steps_per_epoch=rl_config.acrlpd.steps_per_epoch,
                lr_min_factor=rl_config.acrlpd.lr_min_factor,
                intra_epoch_min_factor=rl_config.acrlpd.intra_epoch_min_factor,
                lr_absolute_min=getattr(rl_config.acrlpd, 'lr_absolute_min', 1e-7)
            )
            
            critic_schedule = create_dynamic_lr_schedule(
                base_lr=rl_config.acrlpd.critic_lr,  # ä½¿ç”¨é…ç½®ä¸­çš„critic_lr
                total_steps=rl_config.num_train_steps,
                warmup_epochs=rl_config.acrlpd.warmup_epochs,
                total_epochs=rl_config.acrlpd.total_epochs,
                steps_per_epoch=rl_config.acrlpd.steps_per_epoch,
                lr_min_factor=rl_config.acrlpd.lr_min_factor,
                intra_epoch_min_factor=rl_config.acrlpd.intra_epoch_min_factor,
                lr_absolute_min=getattr(rl_config.acrlpd, 'lr_absolute_min', 1e-7)
            )
            
            # ä½¿ç”¨optaxç›´æ¥åˆ›å»ºä¼˜åŒ–å™¨
            global_pi0_tx = optax.adamw(learning_rate=actor_schedule, weight_decay=1e-6)
            global_critic_tx = optax.adamw(learning_rate=critic_schedule, weight_decay=1e-5)
            
            logger.info("âœ… ä½¿ç”¨åŠ¨æ€åŒå±‚å­¦ä¹ ç‡è°ƒèŠ‚")
        else:
            # ä½¿ç”¨åŸºäºacrlpdé™æ€å­¦ä¹ ç‡çš„è°ƒåº¦å™¨ï¼ˆä¼˜é›…æ–¹æ¡ˆï¼šå•ä¸€æ•°æ®æºï¼‰
            global_pi0_tx = _optimizer.create_optimizer(rl_config.actor_optimizer, rl_config.get_effective_actor_lr_schedule())
            global_critic_tx = _optimizer.create_optimizer(rl_config.critic_optimizer, rl_config.get_effective_critic_lr_schedule())
            logger.info(f"âœ… ä½¿ç”¨åŸºäºacrlpdå­¦ä¹ ç‡çš„è°ƒåº¦å™¨ (Actor: {rl_config.acrlpd.actor_lr}, Critic: {rl_config.acrlpd.critic_lr})")
        global_temp_tx = None  # æš‚æ—¶ä¸ä½¿ç”¨æ¸©åº¦ä¼˜åŒ–å™¨
        
        logger.info("âœ… å…¨å±€ä¼˜åŒ–å™¨åˆ›å»ºæˆåŠŸï¼Œå°†ä¼ é€’ç»™FSDPåˆå§‹åŒ–")
        
        # **éªŒè¯ä¼˜åŒ–å™¨ä¸€è‡´æ€§ï¼ˆè°ƒè¯•pytreeé—®é¢˜ï¼‰**
        logger.info("ğŸ” éªŒè¯ä¼˜åŒ–å™¨pytreeä¸€è‡´æ€§...")
        from utils.pytree_checker import diagnose_pytree_structure, check_optimizer_consistency
        
        # è¯Šæ–­ä¼˜åŒ–å™¨ç»“æ„
        diagnose_pytree_structure(global_pi0_tx, "global_pi0_tx")
        diagnose_pytree_structure(global_critic_tx, "global_critic_tx")
        
        # åˆ›å»ºæµ‹è¯•ä¼˜åŒ–å™¨éªŒè¯ä¸€è‡´æ€§  
        test_pi0_tx = _optimizer.create_optimizer(rl_config.actor_optimizer, rl_config.get_effective_actor_lr_schedule())
        consistency_ok = check_optimizer_consistency(global_pi0_tx, test_pi0_tx, "global_pi0_tx", "test_pi0_tx")
        
        if not consistency_ok:
            logger.warning("âš ï¸ ä¼˜åŒ–å™¨ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥ï¼Œä½†ç»§ç»­è®­ç»ƒ...")
        else:
            logger.info("âœ… ä¼˜åŒ–å™¨ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")
        
        # **ä¿®å¤8å¡FSDPå†…å­˜çˆ†ç‚¸é—®é¢˜**
        # é—®é¢˜ï¼šå‚æ•°åœ¨æ‰€æœ‰GPUä¸Šå¤åˆ¶è€Œä¸æ˜¯åˆ†ç‰‡ï¼Œå¯¼è‡´æ¯ä¸ªGPUä½¿ç”¨61GB
        # è§£å†³æ–¹æ¡ˆï¼šé€‰æ‹©é€‚å½“çš„FSDPä¿®å¤æ–¹æ¡ˆï¼Œå¹¶ä¼ é€’å…¨å±€ä¼˜åŒ–å™¨
        
        logger.info(f"ğŸ”§ ä½¿ç”¨ {args.fsdp_fix.upper()} FSDPä¿®å¤æ–¹æ¡ˆ")
        
        # **ç»Ÿä¸€ä½¿ç”¨init_acrlpd_fsdp_trainingé¿å…æ¨¡å‹é‡å¤åˆ›å»º**
        logger.info("ğŸš€ ä½¿ç”¨ç»Ÿä¸€FSDPåˆå§‹åŒ–ï¼ˆé¿å…æ¨¡å‹é‡å¤åˆ›å»ºï¼‰")
        from training.acrlpd_train_state import init_acrlpd_fsdp_training
        
        train_state, state_sharding, lazy_jit_creator = init_acrlpd_fsdp_training(
            rl_config=rl_config,
            mesh=mesh,
            rng=agent_rng,
            data_sharding=data_sharding,
            step=0,
            global_pi0_tx=global_pi0_tx,
            global_critic_tx=global_critic_tx
        )
        
        # å»¶è¿Ÿåˆ›å»ºJITå‡½æ•°
        logger.info("ğŸ”§ åˆ›å»ºJITè®­ç»ƒå‡½æ•°...")
        fsdp_train_step_fn = lazy_jit_creator()
        
        # éªŒè¯FSDPæ˜¯å¦çœŸæ­£å·¥ä½œ
        logger.info("éªŒè¯FSDPåˆ†ç‰‡æ•ˆæœ...")
        
        # æ£€æŸ¥å®é™…æ˜¾å­˜ä½¿ç”¨
        memory_stats = log_gpu_memory("FSDPéªŒè¯")
        if memory_stats:
            avg_memory = memory_stats['avg_per_gpu']
            max_memory = memory_stats['max_usage']
            logger.info(f"ğŸ’¾ FSDPå†…å­˜ä½¿ç”¨: {avg_memory:.0f}MB/GPU (æœ€å¤§{max_memory:.0f}MB)")
        
        # åˆ›å»ºè½»é‡çº§agentç”¨äºtrainerï¼ˆé¿å…é‡å¤åˆ›å»ºæ¨¡å‹ï¼‰
        logger.info("ğŸ”§ åˆ›å»ºè½»é‡çº§Agent (lazy_init=True) é¿å…é‡å¤åˆ›å»ºæ¨¡å‹")
        with jax.default_device(jax.devices('cpu')[0]):
            agent = create_acrlpd_pi0_agent_from_rl_config(rl_config, agent_rng, lazy_init=True)
        
        # ä»FSDPçŠ¶æ€è®¾ç½®Agentæ¨¡å‹ç»„ä»¶
        logger.info("ğŸ”§ ä»FSDPè®­ç»ƒçŠ¶æ€è®¾ç½®Agentæ¨¡å‹ç»„ä»¶")
        agent.setup_from_fsdp_state(train_state)
        
        logger.info("FSDPéªŒè¯å®Œæˆ")
        logger.info("ç›´æ¥FSDPæ–¹å¼å·²ç»•è¿‡agent.to_train_state()çš„å¤æ‚è½¬æ¢")
        
        # Log GPU memory after FSDP creation and verify sharding
        logger.info("ğŸ” GPUå†…å­˜ä½¿ç”¨ - FSDPè®­ç»ƒçŠ¶æ€åˆ›å»ºå:")
        log_gpu_memory("FSDP_STATE_CREATED")
        
        # ğŸ”§ éªŒè¯FSDPåˆ†ç‰‡æ˜¯å¦æ­£ç¡®å·¥ä½œ
        verify_fsdp_sharding(train_state, "FSDP_STATE_CREATED")
        
        # Evaluation functionå·²ç§»é™¤ - å½“å‰æ— çœŸå®ç¯å¢ƒè¯„ä¼°å®ç°
        
        # Create trainer with OpenPIæ ‡å‡†FSDP agentå’Œtrain_state  
        logger.info("Creating trainer with OpenPIæ ‡å‡†FSDP agent...")
        trainer = ACRLPDTrainer(
            agent=agent,  # ä¼ é€’OpenPIæ ‡å‡†æ–¹å¼åˆ›å»ºçš„agent
            dataloader=dataloader,
            rl_config=rl_config,
            training_config=training_config,
            eval_fn=None,  # è¯„ä¼°åŠŸèƒ½å·²ç§»é™¤
            # FSDPå‚æ•°
            mesh=mesh,
            data_sharding=data_sharding,
            replicated_sharding=replicated_sharding,
            # å…¨å±€ä¼˜åŒ–å™¨å‚æ•°ï¼ˆä¿®å¤pytreeä¸€è‡´æ€§ï¼‰
            global_pi0_tx=global_pi0_tx,
            global_critic_tx=global_critic_tx
        )
        
        # è®¾ç½®è®­ç»ƒå™¨çš„FSDPçŠ¶æ€ï¼ˆç»Ÿä¸€æ–¹å¼åˆ›å»ºï¼‰
        trainer.fsdp_train_state = train_state
        trainer.train_state_sharding = state_sharding
        trainer.fsdp_train_step = fsdp_train_step_fn
        trainer.use_fsdp = True
        
        # éªŒè¯FSDPç»„ä»¶è®¾ç½®æˆåŠŸ
        logger.info("âœ… FSDP components set successfully")
        
        # **æ·»åŠ pytreeä¸€è‡´æ€§éªŒè¯**
        logger.info("ğŸ” éªŒè¯è®­ç»ƒçŠ¶æ€pytreeä¸€è‡´æ€§...")
        from utils.pytree_checker import diagnose_pytree_structure, validate_fsdp_compatibility
        
        # è¯Šæ–­è®­ç»ƒçŠ¶æ€ç»“æ„
        diagnose_pytree_structure(train_state, "train_state")
        
        # éªŒè¯FSDPå…¼å®¹æ€§
        fsdp_compatible = validate_fsdp_compatibility(train_state, mesh, data_sharding)
        if not fsdp_compatible:
            logger.warning("âš ï¸ FSDPå…¼å®¹æ€§éªŒè¯å¤±è´¥ï¼Œä½†ç»§ç»­è®­ç»ƒ...")
        else:
            logger.info("âœ… FSDPå…¼å®¹æ€§éªŒè¯é€šè¿‡")
        
        # ç¡®ä¿PyTreeç»“æ„ä¸€è‡´æ€§ - åœ¨JITç¼–è¯‘å‰é¢„çƒ­train_state
        logger.info("ğŸ”§ é¢„çƒ­è®­ç»ƒçŠ¶æ€ç¡®ä¿PyTreeä¸€è‡´æ€§...")
        jax.block_until_ready(train_state)
        
        logger.info("âœ… è®­ç»ƒå™¨åˆ›å»ºæˆåŠŸï¼Œä½¿ç”¨OpenPIæ ‡å‡†8å¡FSDPé…ç½®")
        
        # Log GPU memory before training starts
        logger.info("ğŸ” GPUå†…å­˜ä½¿ç”¨ - è®­ç»ƒå¼€å§‹å‰:")
        log_gpu_memory("BEFORE_TRAINING")
        
        # Start training
        logger.info("Starting training...")
        
        # ğŸ” å…³é”®ä¿®å¤ï¼šæ ¹æ®rl_config.resumeè‡ªåŠ¨ç¡®å®šresumeè·¯å¾„
        resume_from = None
        if rl_config.resume:
            from pathlib import Path
            
            # è‡ªåŠ¨å¯»æ‰¾æœ€æ–°checkpoint
            checkpoint_dir = Path(rl_config.checkpoint_dir)
            if checkpoint_dir.exists():
                # å¯»æ‰¾æœ€æ–°çš„checkpoint
                available_steps = []
                for step_dir in checkpoint_dir.iterdir():
                    if step_dir.is_dir() and step_dir.name.isdigit():
                        step = int(step_dir.name)
                        # éªŒè¯checkpointå®Œæ•´æ€§
                        components_dir = step_dir / "components"
                        params_dir = step_dir / "params"
                        if components_dir.exists() or params_dir.exists():
                            available_steps.append(step)
                
                if available_steps:
                    latest_step = max(available_steps)
                    resume_from = str(checkpoint_dir / str(latest_step))
                    logger.info(f"ğŸ”„ Auto-resuming from step {latest_step}: {resume_from}")
                else:
                    logger.warning(f"âš ï¸ Resume requested but no valid checkpoints found in {checkpoint_dir}")
            else:
                logger.warning(f"âš ï¸ Resume requested but checkpoint directory does not exist: {checkpoint_dir}")
        
        trained_agent = trainer.train(resume_from=resume_from)
        
        logger.info("Training completed successfully!")
        
        # Final evaluationå·²ç§»é™¤ - å½“å‰æ— çœŸå®ç¯å¢ƒè¯„ä¼°å®ç°
        
    except KeyboardInterrupt:
        logger.info(" Training interrupted by user")
        sys.exit(1)
    
    except Exception as e:
        # åˆ é™¤æ— ç”¨çš„é”™è¯¯åŒ…è£…ï¼Œç›´æ¥æŠ›å‡ºåŸå§‹å¼‚å¸¸è·å¾—å®Œæ•´stack trace
        if args.debug:
            import traceback
            traceback.print_exc()
        raise


if __name__ == "__main__":
    main()