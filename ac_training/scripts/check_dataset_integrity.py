#!/usr/bin/env python3
"""
æ•°æ®é›†å®Œæ•´æ€§æ£€æŸ¥å’Œæ¸…ç†è„šæœ¬

æ£€æŸ¥æŒ‡å®šæ•°æ®é›†ä¸­çš„æ‰€æœ‰episodeï¼Œæ‰¾å‡ºåŒ…å«å¼‚å¸¸æ•°æ®çš„episodeå¹¶å°†å…¶åˆ é™¤ã€‚

ç”¨æ³•:
    python scripts/check_dataset_integrity.py --config rl_fold_box --fix
    python scripts/check_dataset_integrity.py --config rl_fold_box --dry-run
"""

import argparse
import logging
import sys
from pathlib import Path
from typing import List, Dict, Any
import shutil

import jax.numpy as jnp
import numpy as np
import pandas as pd

# Add ac_training root to path  
sys.path.insert(0, str(Path(__file__).parent.parent))

from config import get_config
from data.acrlpd_data_loader_v2 import ACRLPDDataLoaderV2

# é…ç½®logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('/dev/shm/lmc/openpi/ac_training/logs/dataset_integrity_check.log')
    ]
)
logger = logging.getLogger(__name__)


class DatasetIntegrityChecker:
    """æ•°æ®é›†å®Œæ•´æ€§æ£€æŸ¥å™¨"""
    
    def __init__(self, config_name: str):
        self.config_name = config_name
        self.rl_config = get_config(config_name)
        self.problematic_episodes = []
        
        # æ•°å€¼æ£€æŸ¥é˜ˆå€¼
        self.max_abs_value = 1e6  # æœ€å¤§ç»å¯¹å€¼
        self.max_std = 1000       # æœ€å¤§æ ‡å‡†å·®
        self.check_fields = ['state', 'actions', 'reward']  # éœ€è¦æ£€æŸ¥çš„å­—æ®µ
        
        logger.info(f"åˆå§‹åŒ–æ•°æ®é›†æ£€æŸ¥å™¨: {config_name}")
        
    def _check_numerical_validity(self, data: Any, field_name: str, episode_path: str) -> List[str]:
        """
        æ£€æŸ¥æ•°å€¼æœ‰æ•ˆæ€§
        
        Returns:
            List[str]: å‘ç°çš„é—®é¢˜åˆ—è¡¨
        """
        problems = []
        
        if isinstance(data, (np.ndarray, jnp.ndarray)):
            data_array = np.asarray(data)
            
            # æ£€æŸ¥NaN
            if np.any(np.isnan(data_array)):
                problems.append(f"{field_name}: åŒ…å«NaNå€¼")
                
            # æ£€æŸ¥Inf  
            if np.any(np.isinf(data_array)):
                problems.append(f"{field_name}: åŒ…å«Infå€¼")
                
            # æ£€æŸ¥è¿‡å¤§æ•°å€¼
            max_abs = np.max(np.abs(data_array))
            if max_abs > self.max_abs_value:
                problems.append(f"{field_name}: è¿‡å¤§æ•°å€¼ (max_abs={max_abs:.2e})")
                
            # æ£€æŸ¥å¼‚å¸¸æ ‡å‡†å·®
            if data_array.size > 1:
                data_std = np.std(data_array)
                if data_std > self.max_std:
                    problems.append(f"{field_name}: å¼‚å¸¸æ ‡å‡†å·® (std={data_std:.2f})")
                    
        elif isinstance(data, dict):
            for sub_key, sub_data in data.items():
                sub_problems = self._check_numerical_validity(sub_data, f"{field_name}.{sub_key}", episode_path)
                problems.extend(sub_problems)
                
        return problems
        
    def check_episode(self, episode_path: str) -> Dict[str, Any]:
        """
        æ£€æŸ¥å•ä¸ªepisodeæ–‡ä»¶
        
        Returns:
            Dict: æ£€æŸ¥ç»“æœ {
                'path': str,
                'valid': bool, 
                'problems': List[str],
                'stats': Dict[str, Any]
            }
        """
        try:
            # è¯»å–parquetæ–‡ä»¶
            df = pd.read_parquet(episode_path)
            
            result = {
                'path': episode_path,
                'valid': True,
                'problems': [],
                'stats': {
                    'num_steps': len(df),
                    'columns': list(df.columns)
                }
            }
            
            # æ£€æŸ¥æ¯ä¸ªå…³é”®å­—æ®µ
            for i, row in df.iterrows():
                for field in self.check_fields:
                    if field in row:
                        field_problems = self._check_numerical_validity(
                            row[field], field, f"{episode_path}:step_{i}"
                        )
                        if field_problems:
                            result['problems'].extend([f"Step {i} - {p}" for p in field_problems])
                            result['valid'] = False
                            
                # æ£€æŸ¥observationä¸­çš„stateå’Œimages
                if 'observation.state' in row:
                    state_problems = self._check_numerical_validity(
                        row['observation.state'], 'observation.state', f"{episode_path}:step_{i}"
                    )
                    if state_problems:
                        result['problems'].extend([f"Step {i} - {p}" for p in state_problems])
                        result['valid'] = False
                        
                # æ£€æŸ¥action
                if 'action' in row:
                    action_problems = self._check_numerical_validity(
                        row['action'], 'action', f"{episode_path}:step_{i}"
                    )
                    if action_problems:
                        result['problems'].extend([f"Step {i} - {p}" for p in action_problems])
                        result['valid'] = False
                        
            return result
            
        except Exception as e:
            return {
                'path': episode_path,
                'valid': False,
                'problems': [f"æ— æ³•è¯»å–æ–‡ä»¶: {e}"],
                'stats': {}
            }
            
    def find_dataset_files(self) -> List[str]:
        """æ‰¾åˆ°æ•°æ®é›†ä¸­æ‰€æœ‰çš„parquetæ–‡ä»¶"""
        dataset_paths = []
        
        # ä»RLé…ç½®ä¸­è·å–æ•°æ®è·¯å¾„
        data_config = self.rl_config.data
        if hasattr(data_config, 'repo_id'):
            # LeRobotæ•°æ®é›†è·¯å¾„
            cache_dir = Path.home() / '.cache' / 'huggingface' / 'hub'
            possible_paths = [
                cache_dir / f"datasets--lerobot--{data_config.repo_id}" / "snapshots",
                Path("/era-ai/lm/dataset/huggingface_cache/lerobot") / data_config.repo_id / "data"
            ]
            
            for base_path in possible_paths:
                if base_path.exists():
                    # æŸ¥æ‰¾æ‰€æœ‰parquetæ–‡ä»¶
                    parquet_files = list(base_path.rglob("*.parquet"))
                    dataset_paths.extend([str(f) for f in parquet_files])
                    logger.info(f"åœ¨ {base_path} ä¸­æ‰¾åˆ° {len(parquet_files)} ä¸ªæ–‡ä»¶")
                    
        return sorted(dataset_paths)
        
    def check_all_episodes(self, max_episodes: int = None) -> Dict[str, Any]:
        """
        æ£€æŸ¥æ‰€æœ‰episode
        
        Args:
            max_episodes: æœ€å¤§æ£€æŸ¥episodeæ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰
            
        Returns:
            Dict: å®Œæ•´æ£€æŸ¥ç»“æœ
        """
        episode_files = self.find_dataset_files()
        
        if not episode_files:
            logger.error("æœªæ‰¾åˆ°ä»»ä½•æ•°æ®æ–‡ä»¶ï¼")
            return {'valid_episodes': [], 'problematic_episodes': [], 'summary': {}}
            
        if max_episodes:
            episode_files = episode_files[:max_episodes]
            logger.info(f"é™åˆ¶æ£€æŸ¥å‰ {max_episodes} ä¸ªæ–‡ä»¶")
            
        logger.info(f"å¼€å§‹æ£€æŸ¥ {len(episode_files)} ä¸ªepisodeæ–‡ä»¶...")
        
        valid_episodes = []
        problematic_episodes = []
        
        for i, episode_path in enumerate(episode_files):
            if (i + 1) % 50 == 0:
                logger.info(f"è¿›åº¦: {i+1}/{len(episode_files)} ({100*(i+1)/len(episode_files):.1f}%)")
                
            result = self.check_episode(episode_path)
            
            if result['valid']:
                valid_episodes.append(result)
            else:
                problematic_episodes.append(result)
                logger.warning(f"å‘ç°é—®é¢˜æ–‡ä»¶: {episode_path}")
                for problem in result['problems'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé—®é¢˜
                    logger.warning(f"  - {problem}")
                if len(result['problems']) > 5:
                    logger.warning(f"  ... è¿˜æœ‰ {len(result['problems']) - 5} ä¸ªé—®é¢˜")
                    
        summary = {
            'total_episodes': len(episode_files),
            'valid_episodes': len(valid_episodes), 
            'problematic_episodes': len(problematic_episodes),
            'success_rate': len(valid_episodes) / len(episode_files) if episode_files else 0
        }
        
        return {
            'valid_episodes': valid_episodes,
            'problematic_episodes': problematic_episodes,
            'summary': summary
        }
        
    def delete_problematic_episodes(self, problematic_episodes: List[Dict], dry_run: bool = True):
        """
        åˆ é™¤æœ‰é—®é¢˜çš„episodeæ–‡ä»¶
        
        Args:
            problematic_episodes: æœ‰é—®é¢˜çš„episodeåˆ—è¡¨
            dry_run: æ˜¯å¦ä¸ºè¯•è¿è¡Œï¼ˆä¸å®é™…åˆ é™¤ï¼‰
        """
        if not problematic_episodes:
            logger.info("æ²¡æœ‰éœ€è¦åˆ é™¤çš„æ–‡ä»¶")
            return
            
        logger.info(f"{'[DRY RUN] ' if dry_run else ''}å‡†å¤‡åˆ é™¤ {len(problematic_episodes)} ä¸ªé—®é¢˜æ–‡ä»¶:")
        
        deleted_count = 0
        for episode_info in problematic_episodes:
            file_path = episode_info['path']
            logger.info(f"{'[DRY RUN] ' if dry_run else ''}åˆ é™¤: {file_path}")
            
            if not dry_run:
                try:
                    Path(file_path).unlink()
                    deleted_count += 1
                    logger.info(f"âœ… å·²åˆ é™¤: {file_path}")
                except Exception as e:
                    logger.error(f"âŒ åˆ é™¤å¤±è´¥ {file_path}: {e}")
                    
        if not dry_run:
            logger.info(f"âœ… æˆåŠŸåˆ é™¤ {deleted_count}/{len(problematic_episodes)} ä¸ªæ–‡ä»¶")


def main():
    parser = argparse.ArgumentParser(description="æ£€æŸ¥å¹¶æ¸…ç†æ•°æ®é›†å¼‚å¸¸episode")
    parser.add_argument("--config", required=True, help="é…ç½®åç§° (å¦‚: rl_fold_box)")
    parser.add_argument("--fix", action="store_true", help="åˆ é™¤æœ‰é—®é¢˜çš„æ–‡ä»¶ï¼ˆä¸åŠ æ­¤å‚æ•°åˆ™ä¸ºè¯•è¿è¡Œï¼‰")
    parser.add_argument("--dry-run", action="store_true", help="è¯•è¿è¡Œï¼Œä¸åˆ é™¤æ–‡ä»¶ï¼ˆé»˜è®¤è¡Œä¸ºï¼‰")
    parser.add_argument("--max-episodes", type=int, help="é™åˆ¶æ£€æŸ¥çš„episodeæ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ£€æŸ¥å™¨
    checker = DatasetIntegrityChecker(args.config)
    
    # æ£€æŸ¥æ‰€æœ‰episodes
    logger.info("=" * 80)
    logger.info("å¼€å§‹æ•°æ®é›†å®Œæ•´æ€§æ£€æŸ¥...")
    logger.info("=" * 80)
    
    results = checker.check_all_episodes(max_episodes=args.max_episodes)
    
    # è¾“å‡ºæ‘˜è¦
    summary = results['summary']
    logger.info("=" * 80)
    logger.info("æ£€æŸ¥å®Œæˆï¼æ‘˜è¦:")
    logger.info(f"  æ€»æ–‡ä»¶æ•°: {summary['total_episodes']}")
    logger.info(f"  æ­£å¸¸æ–‡ä»¶: {summary['valid_episodes']}")
    logger.info(f"  é—®é¢˜æ–‡ä»¶: {summary['problematic_episodes']}")
    logger.info(f"  æˆåŠŸç‡: {summary['success_rate']*100:.1f}%")
    logger.info("=" * 80)
    
    if results['problematic_episodes']:
        logger.info("é—®é¢˜æ–‡ä»¶è¯¦æƒ…:")
        for episode_info in results['problematic_episodes']:
            logger.info(f"\nğŸ“ {episode_info['path']}")
            for problem in episode_info['problems'][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªé—®é¢˜
                logger.info(f"   âš ï¸  {problem}")
            if len(episode_info['problems']) > 3:
                logger.info(f"   ... è¿˜æœ‰ {len(episode_info['problems']) - 3} ä¸ªé—®é¢˜")
        
        # åˆ é™¤å¤„ç†
        if args.fix and not args.dry_run:
            logger.info("\n" + "=" * 80)
            logger.info("å¼€å§‹åˆ é™¤é—®é¢˜æ–‡ä»¶...")
            checker.delete_problematic_episodes(results['problematic_episodes'], dry_run=False)
        else:
            logger.info("\n" + "=" * 80)
            logger.info("è¯•è¿è¡Œæ¨¡å¼ - å¦‚éœ€å®é™…åˆ é™¤ï¼Œè¯·æ·»åŠ  --fix å‚æ•°")
            checker.delete_problematic_episodes(results['problematic_episodes'], dry_run=True)
    else:
        logger.info("ğŸ‰ æ‰€æœ‰æ–‡ä»¶éƒ½æ­£å¸¸ï¼")


if __name__ == "__main__":
    main()