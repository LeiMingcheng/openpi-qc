"""
Compute normalization statistics for ACRLPD datasets.

This script computes the normalization statistics for ACRLPD training data,
compatible with OpenPI's normalization architecture.
"""

import logging
import sys
from pathlib import Path
from typing import Dict, Any
import numpy as np
import tqdm

import openpi.training.config as _config
import openpi.training.data_loader as _data_loader
import openpi.transforms as transforms
from openpi.shared import normalize as _normalize

logger = logging.getLogger(__name__)


class RemoveStrings(transforms.DataTransformFn):
    """Remove string fields that are not needed for normalization stats."""
    def __call__(self, x: dict) -> dict:
        return {k: v for k, v in x.items() if not np.issubdtype(np.asarray(v).dtype, np.str_)}


def compute_acrlpd_norm_stats(
    rl_config: Any,  # RLTrainConfig (avoiding circular import)
    output_dir: Path,
    max_samples: int = 2000,  # é™ä½é»˜è®¤æ ·æœ¬æ•°ï¼Œæé«˜ç¨³å®šæ€§
    batch_size: int = 32      # é™ä½é»˜è®¤batchå¤§å°
) -> Dict[str, _normalize.NormStats]:
    """
    Compute normalization statistics using OpenPI native methods.
    
    Args:
        rl_config: RLTrainConfig configuration
        output_dir: Directory to save norm_stats.json
        max_samples: Maximum number of samples to use for computing stats
        batch_size: Batch size for processing
    
    Returns:
        Dictionary of normalization statistics
    """
    
    logger.info("ğŸš€ ä½¿ç”¨OpenPIåŸç”Ÿæ–¹æ³•è®¡ç®—normalization statistics")
    
    # åˆ›å»ºdata configï¼ˆä½¿ç”¨OpenPIæ ‡å‡†æ–¹æ³•ï¼‰
    print("ğŸ” åˆ›å»ºOpenPI DataConfig...")
    data_config = rl_config.data.create(rl_config.assets_dirs, rl_config.model)
    
    repo_id = data_config.repo_id
    logger.info(f"ğŸ“Š æ•°æ®é›†: {repo_id}")
    print(f"âœ… DataConfigåˆ›å»ºæˆåŠŸ: {repo_id}")
    
    # åˆ›å»ºdatasetï¼ˆä½¿ç”¨OpenPIæ ‡å‡†torch datasetï¼Œä½†è·³è¿‡è§†é¢‘å¤„ç†ï¼‰
    print("ğŸ” åˆ›å»ºtorch datasetï¼ˆè·³è¿‡è§†é¢‘å¤„ç†ï¼‰...")
    
    # ä¸ºnorm statsè®¡ç®—åˆ›å»ºä¸€ä¸ªåªåŒ…å«å¿…è¦æ•°æ®çš„è½»é‡ç‰ˆdataset
    if data_config.repo_id is None:
        raise ValueError("Data config must have a repo_id")
    
    # åˆ›å»ºç®€åŒ–çš„LeRobot datasetï¼Œç”¨äºnorm statsè®¡ç®—
    from lerobot.common.datasets.lerobot_dataset import LeRobotDataset
    
    print("ğŸ” åˆ›å»ºç®€åŒ–LeRobot dataset...")
    
    # ğŸ”§ CRITICAL FIX: Apply torch.stack Column compatibility patch first
    import torch
    if not hasattr(torch, '_openpi_column_patch'):
        logger.info("ğŸ”§ åº”ç”¨torch.stack Columnå…¼å®¹æ€§patch for norm stats")
        original_stack = torch.stack
        
        def patched_stack(tensors, dim=0, *, out=None):
            # æ£€æŸ¥æ˜¯å¦æ˜¯HF Datasetçš„Columnå¯¹è±¡
            if hasattr(tensors, '__class__') and 'Column' in tensors.__class__.__name__:
                # å°†Columnè½¬æ¢ä¸ºtensoråˆ—è¡¨
                tensor_list = [torch.as_tensor(item) for item in tensors]
                return original_stack(tensor_list, dim=dim, out=out)
            else:
                return original_stack(tensors, dim=dim, out=out)
        
        torch.stack = patched_stack
        torch._openpi_column_patch = True
    
    try:
        # å…ˆå°è¯•ä¸ä½¿ç”¨delta_timestampsï¼Œé¿å…æ—¶é—´æˆ³é—®é¢˜
        lerobot_dataset = LeRobotDataset(
            data_config.repo_id,
            tolerance_s=1e-4,
            video_backend="pyav",  # ä½¿ç”¨PyAV backend
            skip_problematic_episodes=True
        )
        print("âœ… PyAV backend åˆå§‹åŒ–æˆåŠŸ")
        
        # ç›´æ¥ä½¿ç”¨åŸç”ŸLeRobot datasetï¼Œä¸éœ€è¦OpenPIåŒ…è£…
        dataset = lerobot_dataset
        
    except Exception as e:
        print(f"âš ï¸ è§†é¢‘backendå¤±è´¥: {e}")
        # å†æ¬¡å°è¯•ï¼Œè¿™æ¬¡å®Œå…¨è·³è¿‡è§†é¢‘å¤„ç†
        print("ğŸ” å°è¯•è·³è¿‡æ‰€æœ‰è§†é¢‘å¤„ç†...")
        try:
            lerobot_dataset = LeRobotDataset(
                data_config.repo_id,
                tolerance_s=1e-4,
                skip_problematic_episodes=True
                # ä¸æŒ‡å®švideo_backendï¼Œä½¿ç”¨é»˜è®¤æˆ–è·³è¿‡
            )
            dataset = lerobot_dataset
            print("âœ… è·³è¿‡è§†é¢‘å¤„ç†æˆåŠŸ")
        except Exception as e2:
            raise RuntimeError(f"æ— æ³•åˆ›å»ºæ•°æ®é›†: {e2}") from e2
    
    print(f"âœ… Datasetåˆ›å»ºæˆåŠŸï¼Œæ€»é•¿åº¦: {len(dataset)}")
    
    # ç®€åŒ–çš„æ‰¹å¤„ç†æ–¹å¼ï¼Œç›´æ¥ä»dataseté‡‡æ ·è€Œä¸ä½¿ç”¨å¤æ‚çš„DataLoader
    logger.info(f"ğŸ“Š å¼€å§‹ç›´æ¥é‡‡æ ·æ•°æ®è®¡ç®—ç»Ÿè®¡ä¿¡æ¯")
    logger.info(f"   - æ•°æ®é›†å¤§å°: {len(dataset)}")
    logger.info(f"   - æœ€å¤§æ ·æœ¬æ•°: {max_samples}")
    
    # åˆå§‹åŒ–ç»Ÿè®¡æ”¶é›†å™¨
    keys = ["state", "actions"]  
    stats = {key: _normalize.RunningStats() for key in keys}
    
    logger.info(f"ğŸ”„ å¼€å§‹è®¡ç®—normalization stats for keys: {keys}")
    
    # ğŸ”‘ å®Œå…¨æŒ‰ç…§OpenPIåŸç”Ÿæ–¹æ³•é‡å†™
    
    # ğŸ”‘ å…³é”®ä¿®å¤ï¼šå¯¹äºnorm statsï¼Œåªä½¿ç”¨åŸºç¡€transformsï¼Œè·³è¿‡å¤æ‚çš„model transforms
    print("ğŸ” åº”ç”¨ç®€åŒ–çš„transformsï¼ˆä»…ç”¨äºnorm statsè®¡ç®—ï¼‰...")
    
    # åªä½¿ç”¨repack_transformsï¼Œè·³è¿‡å¯èƒ½å¯¼è‡´ç»´åº¦é—®é¢˜çš„data_transforms
    essential_transforms = [
        *data_config.repack_transforms.inputs,
        RemoveStrings(),  # ç§»é™¤å­—ç¬¦ä¸²å­—æ®µ
    ]
    
    dataset = _data_loader.TransformedDataset(dataset, essential_transforms)
    
    # è®¡ç®—batchæ•°é‡
    if max_samples is not None and max_samples < len(dataset):
        num_batches = max_samples // batch_size
        shuffle = True
    else:
        num_batches = len(dataset) // batch_size
        shuffle = False
    
    # åˆ›å»ºDataLoaderï¼ˆOpenPIæ ‡å‡†æ–¹å¼ï¼‰
    data_loader = _data_loader.TorchDataLoader(
        dataset,
        local_batch_size=batch_size,
        num_workers=0,  # å•è¿›ç¨‹é¿å…è§†é¢‘é—®é¢˜
        shuffle=shuffle,
        num_batches=num_batches,
    )
    
    # ğŸ”‘ OpenPIåŸç”Ÿç»Ÿè®¡è®¡ç®—å¾ªç¯ï¼ˆå¸¦debugä¿¡æ¯ï¼‰
    batch_count = 0
    for batch in tqdm.tqdm(data_loader, total=num_batches, desc="Computing norm stats"):
        batch_count += 1
        
        # Debugå‰å‡ ä¸ªbatchçš„è¯¦ç»†ä¿¡æ¯
        if batch_count <= 3:
            print(f"\nğŸ” DEBUG Batch {batch_count}:")
            print(f"  - Batch keys: {list(batch.keys())}")
            for key in keys:
                if key in batch:
                    batch_data = batch[key]
                    print(f"  - {key}: type={type(batch_data)}, len={len(batch_data) if hasattr(batch_data, '__len__') else 'no len'}")
                    if hasattr(batch_data, '__len__') and len(batch_data) > 0:
                        first_elem = batch_data[0]
                        print(f"    - [0]: type={type(first_elem)}, shape={getattr(first_elem, 'shape', 'no shape')}")
        
        for key in keys:
            if key in batch:
                try:
                    # ğŸ”‘ å®Œå…¨æŒ‰ç…§OpenPIæ–¹å¼ï¼šbatch[key][0]ç„¶åreshape
                    values = np.asarray(batch[key][0])
                    reshaped_values = values.reshape(-1, values.shape[-1])
                    
                    # Debugç»´åº¦ä¿¡æ¯
                    if batch_count <= 3:
                        print(f"  - {key} å¤„ç†æˆåŠŸ: original shape={values.shape} -> reshaped shape={reshaped_values.shape}")
                    
                    stats[key].update(reshaped_values)
                    
                except Exception as e:
                    # è¯¦ç»†çš„é”™è¯¯debugä¿¡æ¯
                    print(f"\nâŒ ERROR processing {key} in batch {batch_count}:")
                    print(f"  - Exception: {e}")
                    try:
                        values = np.asarray(batch[key][0])
                        print(f"  - values shape: {values.shape}")
                        print(f"  - values dtype: {values.dtype}")
                        print(f"  - values.shape[-1]: {values.shape[-1]}")
                        print(f"  - reshape target: (-1, {values.shape[-1]})")
                        if stats[key]._mean is not None:
                            print(f"  - current stats mean shape: {stats[key]._mean.shape}")
                    except Exception as e2:
                        print(f"  - Could not extract debug info: {e2}")
                    raise
    
    # è®¡ç®—æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯ï¼ˆä½¿ç”¨OpenPIæ ‡å‡†æ–¹æ³•ï¼‰
    logger.info("ğŸ”„ è®¡ç®—æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯...")
    
    norm_stats = {}
    for key, running_stats in stats.items():
        logger.info(f"å¤„ç† '{key}': {running_stats._count} ä¸ªæ ·æœ¬")
        
        if running_stats._count > 1:
            try:
                final_stats = running_stats.get_statistics()
                norm_stats[key] = final_stats
                
                logger.info(f"âœ… '{key}' ç»Ÿè®¡ä¿¡æ¯:")
                logger.info(f"  - shape: {final_stats.mean.shape}")
                logger.info(f"  - mean range: [{final_stats.mean.min():.6f}, {final_stats.mean.max():.6f}]")
                logger.info(f"  - std range: [{final_stats.std.min():.6f}, {final_stats.std.max():.6f}]")
                
            except ValueError as e:
                logger.warning(f"è®¡ç®— '{key}' ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        else:
            logger.warning(f"'{key}' æ ·æœ¬æ•°ä¸è¶³: åªæœ‰ {running_stats._count} ä¸ªæ ·æœ¬")
    
    if not norm_stats:
        raise RuntimeError("æœªèƒ½è®¡ç®—å‡ºä»»ä½•æœ‰æ•ˆçš„ç»Ÿè®¡ä¿¡æ¯ï¼è¯·æ£€æŸ¥æ•°æ®åŠ è½½å™¨ã€‚")
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯ï¼ˆä½¿ç”¨OpenPIæ ‡å‡†æ–¹æ³•ï¼‰
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"ğŸ’¾ ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ°: {output_dir}")
    _normalize.save(output_dir, norm_stats)
    
    # è¾“å‡ºæ€»ç»“
    print(f"\nğŸ‰ æˆåŠŸè®¡ç®—å¹¶ä¿å­˜normalization statistics!")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯æ¦‚è¦:")
    
    for key, stats_obj in norm_stats.items():
        print(f"  ğŸ”¹ {key}:")
        print(f"    - Shape: {stats_obj.mean.shape}")
        print(f"    - Mean range: [{stats_obj.mean.min():.6f}, {stats_obj.mean.max():.6f}]")
        print(f"    - Std range: [{stats_obj.std.min():.6f}, {stats_obj.std.max():.6f}]")
        if hasattr(stats_obj, 'q01'):
            print(f"    - Q01 range: [{stats_obj.q01.min():.6f}, {stats_obj.q01.max():.6f}]")
            print(f"    - Q99 range: [{stats_obj.q99.min():.6f}, {stats_obj.q99.max():.6f}]")
    
    return norm_stats


def main():
    """Main function for CLI usage."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Compute ACRLPD normalization statistics",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument("--repo-id", required=True, 
                       help="LeRobot dataset repository ID (local or remote)")
    parser.add_argument("--output-dir", required=True, 
                       help="Output directory for norm_stats.json") 
    parser.add_argument("--max-samples", type=int, default=10000, 
                       help="Maximum samples to process")
    parser.add_argument("--batch-size", type=int, default=32, 
                       help="Batch size for processing")
    parser.add_argument("--action-horizon", type=int, default=10, 
                       help="Action horizon for sequence generation")
    parser.add_argument("--tolerance-s", type=float, default=1e-4,
                       help="Tolerance for timestamp synchronization")
    parser.add_argument("--skip-problematic-episodes", action="store_true",
                       help="Skip episodes with timestamp issues")
    parser.add_argument("--verbose", "-v", action="store_true",
                       help="Enable verbose logging")
    
    args = parser.parse_args()
    
    # Configure logging
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Check if we have a local dataset path instead of remote repo_id
    local_dataset_path = Path(f"/era-ai/lm/dataset/huggingface_cache/lerobot/{args.repo_id}")
    if local_dataset_path.exists():
        logger.info(f"âœ“ Found local dataset at: {local_dataset_path}")
        # Use local path as repo_id for LeRobot dataset loading
        actual_repo_id = str(local_dataset_path)
    else:
        logger.info(f"Using remote repo_id: {args.repo_id}")
        actual_repo_id = args.repo_id

    logger.info(f"ğŸš€ Starting ACRLPD normalization statistics computation")
    logger.info(f"ğŸ“Š Configuration:")
    logger.info(f"  - repo_id: {args.repo_id} -> {actual_repo_id}")
    logger.info(f"  - output_dir: {args.output_dir}")
    logger.info(f"  - max_samples: {args.max_samples}")
    logger.info(f"  - batch_size: {args.batch_size}")
    logger.info(f"  - action_horizon: {args.action_horizon}")
    
    # Use unified config system but override repo_id with command line arg
    sys.path.insert(0, str(Path(__file__).parent.parent))
    from config import get_config
    
    logger.info("Loading rl_fold_box config...")
    rl_config = get_config("rl_fold_box")
    
    # ğŸ”‘ CRITICAL FIX: Override repo_id with command line argument
    print(f"ğŸ”„ STEP 1: Original config repo_id: {rl_config.data.repo_id}")
    print(f"ğŸ”„ STEP 2: Overriding with command line repo_id: {actual_repo_id}")
    
    # Create a modified config with the correct repo_id
    import dataclasses
    from openpi.training import config as openpi_config
    
    # Get the original data config factory
    original_data_factory = rl_config.data
    print(f"ğŸ”„ STEP 3: Got original data factory")
    
    # Create a new LeRobotAlohaDataConfig with the correct repo_id
    modified_data_config = openpi_config.LeRobotAlohaDataConfig(
        repo_id=actual_repo_id,  # Use command line repo_id
        default_prompt=original_data_factory.default_prompt,
        adapt_to_pi=original_data_factory.adapt_to_pi,
        assets=original_data_factory.assets,
        repack_transforms=original_data_factory.repack_transforms,
        base_config=original_data_factory.base_config
    )
    print(f"ğŸ”„ STEP 4: Created modified data config with repo_id: {modified_data_config.repo_id}")
    
    # Create modified RL config with new data config and NO weight loader (for norm stats only)
    rl_config = dataclasses.replace(
        rl_config, 
        data=modified_data_config,
        weight_loader=None  # Skip weight loading for norm stats computation
    )
    print(f"ğŸ”„ STEP 5: Replaced RL config, new repo_id: {rl_config.data.repo_id}")
    print(f"ğŸ”„ STEP 6: Disabled weight loader for norm stats computation")
    
    print(f"âœ“ Using dataset: {actual_repo_id}")
    
    try:
        # Compute and save statistics using unified config
        compute_acrlpd_norm_stats(
            rl_config=rl_config,
            output_dir=Path(args.output_dir),
            max_samples=args.max_samples,
            batch_size=args.batch_size
        )
        
        logger.info("ğŸ‰ ACRLPD normalization statistics computation completed successfully!")
        
    except Exception as e:
        logger.error(f"âŒ Failed to compute normalization statistics: {e}")
        raise


if __name__ == "__main__":
    main()