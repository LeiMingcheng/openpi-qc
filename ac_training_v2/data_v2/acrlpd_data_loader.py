"""
ACRLPDDataLoader: QC-ACTæ¶æ„ + OpenPIæ ¼å¼è¾“å‡ºçš„æ•°æ®åŠ è½½å™¨

åŸºäºqc_ACTçš„å®Œå…¨éšæœºéå†æ¶æ„ï¼Œä¸“ä¸ºå†…å­˜å—é™ç¯å¢ƒè®¾è®¡ï¼š
- å†…å­˜æ± é¢„åŠ è½½ï¼šæ¯epochéšæœºåŠ è½½Nä¸ªepisodes
- å®Œå…¨éšæœºé‡‡æ ·ï¼šå¿½ç•¥DataLoaderç´¢å¼•ï¼Œç»Ÿè®¡æ€§éå†å…¨æ•°æ®é›†
- LeRobotè¾“å…¥ â†’ OpenPIæ ¼å¼è¾“å‡º
- Ï€â‚€å˜æ¢ç®¡é“é›†æˆ
- é«˜æ€§èƒ½å†…å­˜è®¿é—®ï¼Œé¿å…é¢‘ç¹I/O
"""

import logging
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional, Iterator, Union
import dataclasses
from enum import Enum

import jax
import jax.numpy as jnp
import numpy as np
import torch
from lerobot.common.datasets.lerobot_dataset import LeRobotDataset, LeRobotDatasetMetadata
import lerobot.common.datasets.lerobot_dataset as lerobot_dataset

import openpi.models.model as _model
import openpi.transforms as _transforms
from openpi.shared import array_typing as at
from openpi.shared import normalize as _normalize
import openpi.training.data_loader as openpi_data_loader
import openpi.training.config as openpi_config

logger = logging.getLogger(__name__)
# ç¡®ä¿loggerèƒ½æ­£ç¡®è¾“å‡ºåˆ°console
logger.setLevel(logging.INFO)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(levelname)s:%(name)s:%(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)


# ===============================================================================
# BACKWARD COMPATIBILITY CLASSES FOR STAGE 1
# These classes are preserved for Stage 1 (H5 â†’ LeRobot conversion) compatibility
# ===============================================================================

class SamplingStrategy(Enum):
    """Backward compatibility: Sampling strategies for Stage 1."""
    UNIFORM = "uniform"
    BALANCED_REWARD = "balanced_reward" 
    PRIORITY_POSITIVE = "priority_positive"
    BOOTSTRAP = "bootstrap"


@dataclasses.dataclass
class SamplingConfig:
    """Backward compatibility: Sampling configuration for Stage 1."""
    strategy: SamplingStrategy = SamplingStrategy.UNIFORM
    positive_weight: float = 1.0
    negative_weight: float = 1.0
    min_positive_ratio: float = 0.3
    max_positive_ratio: float = 0.7
    bootstrap_ratio: float = 0.1


@dataclasses.dataclass 
class RewardProcessingConfig:
    """Backward compatibility: Reward processing configuration for Stage 1."""
    use_sparse_rewards: bool = True
    use_dense_rewards: bool = False
    reward_shaping: bool = False
    discount_factor: float = 0.99
    reward_scale: float = 1.0
    success_bonus: float = 10.0
    failure_penalty: float = -1.0

# ===============================================================================


# ACRLPDDataConfigå·²åˆ é™¤ - ç°åœ¨ç›´æ¥ä½¿ç”¨RLTrainConfig


class ACRLPDDataLoader:
    """
    Q-chunking RLæ•°æ®åŠ è½½å™¨ï¼ˆåŸºäºqc_ACTæ¶æ„ï¼‰
    
    æ ¸å¿ƒç‰¹æ€§ï¼š
    - Q-chunkingæ”¯æŒï¼šè¿”å›RLè®­ç»ƒæ‰€éœ€çš„å®Œæ•´transitionæ ¼å¼
    - åŠ¨ä½œåºåˆ—ç”Ÿæˆï¼šHæ­¥åŠ¨ä½œåºåˆ—å’Œepisodeè¾¹ç•Œå¤„ç†
    - Bootstrapæœºåˆ¶ï¼šnext_observationsç”¨äºtarget Qè®¡ç®—
    - å†…å­˜æ± ç®¡ç†ï¼šæ¯epochéšæœºåŠ è½½episodesåˆ°å†…å­˜
    - å®Œå…¨éšæœºé‡‡æ ·ï¼šç¬¦åˆå¼ºåŒ–å­¦ä¹ è¦æ±‚
    - Ï€â‚€å…¼å®¹ï¼šObservationæ ¼å¼å…¼å®¹OpenPIæ¨¡å‹
    
    è¾“å‡ºæ ¼å¼ï¼ˆQ-chunkingæ ‡å‡†ï¼‰ï¼š
    {
        'observations': Ï€â‚€_Observation,      # å½“å‰è§‚æµ‹
        'next_observations': Ï€â‚€_Observation,  # Bootstrapç”¨
        'actions': [B, H, action_dim],       # åŠ¨ä½œåºåˆ—
        'rewards': [B, H],                   # å¥–åŠ±åºåˆ—
        'masks': [B, H],                     # Bootstrap mask
        'valid': [B, H],                     # åŠ¨ä½œæœ‰æ•ˆæ€§
        'terminals': [B, H],                 # æ­¥éª¤ç»ˆæ­¢æ ‡å¿—
        'next_terminal': [B],                # ä¸‹ä¸€çŠ¶æ€terminal
        'sequence_mask': [B]                 # åºåˆ—æœ‰æ•ˆæ€§
    }
    """
    
    def __init__(
        self,
        rl_config: Any,  # RLTrainConfig (é¿å…å¾ªç¯å¯¼å…¥)
        batch_size: int = 128,
        episodes_per_memory_pool: int = 64,  # å†…å­˜æ± å¤§å°
        shuffle: bool = True,
        seed: int = 42,
        tolerance_s: float = 1e-4,  # æ—¶é—´æˆ³å®¹é”™é˜ˆå€¼
        device_sharding: Optional[jax.sharding.Sharding] = None,
        rank: int = 0,
        world_size: int = 1,
        skip_norm_stats: bool = False  # acrlpd_data_converter æ–°å¢ï¼šè·³è¿‡norm statsåŠ è½½ï¼Œé¿å…å¾ªç¯ä¾èµ–
    ):
        """
        åˆå§‹åŒ–ç»Ÿä¸€çš„ACRLPDæ•°æ®åŠ è½½å™¨
        
        Args:
            rl_config: RLTrainConfigç»Ÿä¸€é…ç½®
            batch_size: æ‰¹æ¬¡å¤§å°
            episodes_per_memory_pool: å†…å­˜æ± å¤§å°ï¼ˆæ¯epochåŠ è½½çš„episodesæ•°é‡ï¼‰
            shuffle: æ˜¯å¦å¯ç”¨éšæœºæ€§ï¼ˆå®é™…æ€»æ˜¯éšæœºçš„ï¼‰
            seed: éšæœºç§å­
            tolerance_s: æ—¶é—´æˆ³å®¹é”™é˜ˆå€¼ï¼ˆç§’ï¼‰
            device_sharding: JAXè®¾å¤‡åˆ†ç‰‡é…ç½®
            rank: åˆ†å¸ƒå¼è®­ç»ƒGPUç¼–å·
            world_size: æ€»GPUæ•°é‡
            skip_norm_stats: è·³è¿‡norm statsåŠ è½½ï¼Œç”¨äºnorm statsè®¡ç®—åœºæ™¯
        """
        self.rl_config = rl_config
        self.batch_size = batch_size
        self.episodes_per_memory_pool = episodes_per_memory_pool
        self.seed = seed
        self.tolerance_s = tolerance_s
        self.rank = rank
        self.world_size = world_size
        self.skip_norm_stats = skip_norm_stats
        
        # ä»ç»Ÿä¸€é…ç½®ä¸­æå–å…³é”®å‚æ•°
        self.qchunking_config = rl_config.qchunking
        self.acrlpd_config = rl_config.acrlpd
        
        # ä»OpenPIæ•°æ®é…ç½®ä¸­è·å–repo_id
        data_config_factory = rl_config.data
        if hasattr(data_config_factory, 'repo_id'):
            self.repo_id = data_config_factory.repo_id
        else:
            # å¯¹äºå¤æ‚çš„DataConfigFactoryï¼Œéœ€è¦åˆ›å»ºåè·å–
            temp_data_config = data_config_factory.create(
                assets_dirs=rl_config.assets_dirs,
                model_config=rl_config.model
            )
            self.repo_id = temp_data_config.repo_id
        
        if not self.repo_id:
            raise ValueError("Cannot determine repo_id from RLTrainConfig.data")
        
        # qc_ACTé£æ ¼çš„å†…å­˜æ± 
        self.memory_pool_episodes = []      # å½“å‰å†…å­˜æ± ä¸­çš„å®Œæ•´episodesæ•°æ®
        self.memory_pool_lengths = []       # æ¯ä¸ªepisodeçš„é•¿åº¦ï¼ˆtransitionæ•°é‡ï¼‰
        self.total_pool_transitions = 0     # å†…å­˜æ± ä¸­æ€»transitionæ•°
        
        # å…¨å±€æ•°æ®é›†ä¿¡æ¯
        self.all_episode_info = []          # æ‰€æœ‰episodesçš„ä¿¡æ¯ï¼ˆè¾¹ç•Œç­‰ï¼‰
        self.current_epoch_seed = 0         # å½“å‰epochç§å­
        
        # éšæœºçŠ¶æ€
        self.rng = np.random.RandomState(seed)
        
        # è®¾å¤‡åˆ†ç‰‡
        if device_sharding is None:
            device_sharding = jax.sharding.NamedSharding(
                jax.sharding.Mesh(jax.devices(), ("B",)),
                jax.sharding.PartitionSpec("B")
            )
        self.device_sharding = device_sharding
        
        # åˆå§‹åŒ–æ­¥éª¤
        print(f"acrlpd_data_converter DATA LOADER DEBUG: repo_id={self.repo_id}")
        print(f"   - episodes_per_memory_pool={episodes_per_memory_pool}")
        print(f"   - batch_size={batch_size}")
        logger.info(f"åˆå§‹åŒ–ç»Ÿä¸€ACRLPDæ•°æ®åŠ è½½å™¨: repo_id={self.repo_id}, "
                   f"episodes_per_memory_pool={episodes_per_memory_pool}, batch_size={batch_size}")
        
        self._discover_all_episodes()       # å‘ç°æ‰€æœ‰episodes
        self._setup_transforms()            # è®¾ç½®OpenPIå˜æ¢ç®¡é“
        self._load_current_memory_pool()    # åŠ è½½åˆå§‹å†…å­˜æ± 
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'repo_id': self.repo_id,
            'total_episodes': len(self.all_episode_info),
            'episodes_per_memory_pool': episodes_per_memory_pool,
            'current_pool_transitions': self.total_pool_transitions,
            'batches_served': 0,
            'current_epoch': 0,
            'config_type': 'RLTrainConfig (unified)'
        }
        
        logger.info(f"âœ“ ç»Ÿä¸€æ•°æ®åŠ è½½å™¨åˆå§‹åŒ–å®Œæˆ: å‘ç°{len(self.all_episode_info)}ä¸ªepisodes, "
                   f"å†…å­˜æ± å®¹é‡{episodes_per_memory_pool}, å½“å‰åŠ è½½{len(self.memory_pool_episodes)}ä¸ªepisodes, "
                   f"æ€»è®¡{self.total_pool_transitions}ä¸ªtransitions")
    
    def _discover_all_episodes(self):
        """å‘ç°LeRobotæ•°æ®é›†ä¸­çš„æ‰€æœ‰episodes"""
        
        # è·å–æ•°æ®é›†å…ƒæ•°æ®
        try:
            self.dataset_meta = LeRobotDatasetMetadata(self.repo_id, local_files_only=True)
        except TypeError:
            # å¦‚æœLeRobotDatasetMetadataä¸æ”¯æŒlocal_files_onlyï¼Œå›é€€åˆ°é»˜è®¤æ–¹å¼
            self.dataset_meta = LeRobotDatasetMetadata(self.repo_id)
        
        # acrlpd_data_converter DEBUG: Print the repo_id being used to create LeRobot dataset
        print(f"acrlpd_data_converter LEROBOT DATASET: Creating dataset with repo_id='{self.repo_id}'")
        print(f"   - Type: {type(self.repo_id)}")
        print(f"   - Absolute path exists: {Path(self.repo_id).exists() if not self.repo_id.startswith('/') else 'checking...'}")
        
        # acrlpd_data_converter ä¿®å¤1ï¼šå¼ºåˆ¶ç¦ç”¨torchcodecï¼Œç¡®ä¿ä½¿ç”¨pyav/torchvision
        import lerobot.common.datasets.video_utils as video_utils
        
        if not hasattr(video_utils, '_openpi_patched'):
            logger.info("acrlpd_data_converter åº”ç”¨PyAV/torchvision patchä»¥ä¿®å¤torchcodecé—®é¢˜")
            original_decode_video_frames = video_utils.decode_video_frames
            
            def patched_decode_video_frames(video_path, timestamps, tolerance_s, backend="pyav"):
                # ä½¿ç”¨torchvision+pyav backendï¼Œé¿å…torchcodecé—®é¢˜
                try:
                    return video_utils.decode_video_frames_torchvision(video_path, timestamps, tolerance_s, backend="pyav")
                except Exception as e:
                    logger.error(f"PyAV/torchvisionè§£ç å¤±è´¥: {e}")
                    raise e
            
            video_utils.decode_video_frames = patched_decode_video_frames
            video_utils._openpi_patched = True
        
        # acrlpd_data_converter ä¿®å¤2ï¼štorch.stack(Column)å…¼å®¹æ€§é—®é¢˜ - ç»è¿‡éªŒè¯çš„ä¿®å¤
        import torch
        if not hasattr(torch, '_openpi_column_patch'):
            logger.info("acrlpd_data_converter åº”ç”¨torch.stack Columnå…¼å®¹æ€§patch")
            original_stack = torch.stack
            
            def patched_stack(tensors, dim=0, *, out=None):
                # æ£€æŸ¥æ˜¯å¦æ˜¯HF Datasetçš„Columnå¯¹è±¡
                if hasattr(tensors, '__class__') and 'Column' in tensors.__class__.__name__:
                    # å°†Columnè½¬æ¢ä¸ºtensoråˆ—è¡¨
                    tensor_list = [torch.as_tensor(item) for item in tensors]
                    return original_stack(tensor_list, dim=dim, out=out)
                else:
                    return original_stack(tensors, dim=dim, out=out)
            
            torch.stack = patched_stack
            torch._openpi_column_patch = True
        
        # åˆ›å»ºLeRobotæ•°æ®é›† - æ·»åŠ å‚æ•°è·³è¿‡æœ‰é—®é¢˜çš„episodes
        self.lerobot_dataset = lerobot_dataset.LeRobotDataset(
            self.repo_id,
            delta_timestamps={
                key: [t / self.dataset_meta.fps for t in range(self.qchunking_config.horizon_length)] 
                for key in ["action"]  # ç®€åŒ–ä¸ºæ ‡å‡†action key
            },
            tolerance_s=self.tolerance_s,  # ä½¿ç”¨å¯é…ç½®çš„tolerance
            video_backend="pyav",  # ä¸OpenPIä¿æŒä¸€è‡´
            skip_problematic_episodes=True  # è·³è¿‡æœ‰æ—¶é—´åŒæ­¥é—®é¢˜çš„episodes
        )
        
        # acrlpd_data_converter DEBUG: Print dataset info after creation
        print(f"acrlpd_data_converter LEROBOT DATASET CREATED:")
        print(f"   - Dataset repo_id: {getattr(self.lerobot_dataset, 'repo_id', 'N/A')}")
        print(f"   - Dataset root: {getattr(self.lerobot_dataset, 'root', 'N/A')}")
        print(f"   - Dataset length: {len(self.lerobot_dataset)}")
        if hasattr(self.lerobot_dataset, 'episode_data_index'):
            print(f"   - Episode data index keys: {len(self.lerobot_dataset.episode_data_index)} episodes")
        
        # æå–episodeè¾¹ç•Œä¿¡æ¯
        if hasattr(self.lerobot_dataset, 'episode_data_index'):
            episode_index = self.lerobot_dataset.episode_data_index
            
            print(f"acrlpd_data_converter EPISODE_DATA_INDEX DEBUG:")
            print(f"   - Type: {type(episode_index)}")
            print(f"   - Length: {len(episode_index)}")
            print(f"   - Keys: {list(episode_index.keys())}...")
            for key, value in list(episode_index.items())[:1]:
                print(f"   - Sample: key={key} (type: {type(key)})")
                print(f"           value={value} (type: {type(value)})")
            
            # acrlpd_data_converter FIXED: Handle correct LeRobot episode_data_index format
            if 'from' in episode_index and 'to' in episode_index:
                # Correct LeRobot format: {'from': tensor([starts...]), 'to': tensor([ends...])}
                starts = episode_index['from']
                ends = episode_index['to']
                
                print(f"acrlpd_data_converter CORRECTED EPISODE PARSING:")
                print(f"   - Episodes found: {len(starts)}")
                print(f"   - Total transitions: {ends[-1].item() if len(ends) > 0 else 0}")
                print(f"   - First 3 episodes: starts={starts[:3].tolist()}, ends={ends[:3].tolist()}")
                
                # Create episode info for each episode
                for i in range(len(starts)):
                    episode_start = int(starts[i].item())
                    episode_end = int(ends[i].item())
                    episode_length = episode_end - episode_start
                    
                    if episode_length > 0:
                        self.all_episode_info.append({
                            'episode_id': i,
                            'start_idx': episode_start,
                            'end_idx': episode_end,
                            'length': episode_length
                        })
                        
            else:
                # Fallback: Legacy format handling (for compatibility)
                logger.warning("Using legacy episode_data_index format")
                for episode_id, episode_info in episode_index.items():
                    try:
                        if hasattr(episode_info, '__getitem__') and isinstance(episode_info, dict):
                            episode_start = episode_info['from']
                            episode_end = episode_info['to']
                        elif hasattr(episode_info, 'from_'):
                            episode_start = episode_info.from_
                            episode_end = episode_info.to
                        elif torch.is_tensor(episode_info) and episode_info.numel() >= 2:
                            episode_start = episode_info[0]
                            episode_end = episode_info[1]
                        else:
                            logger.warning(f"Unknown episode_info structure for episode {episode_id}: {type(episode_info)}")
                            continue
                        
                        # è½¬æ¢ä¸ºPython int
                        if torch.is_tensor(episode_start):
                            episode_start = int(episode_start.item())
                        if torch.is_tensor(episode_end):
                            episode_end = int(episode_end.item())
                        
                        episode_start = int(episode_start)
                        episode_end = int(episode_end)
                        episode_length = episode_end - episode_start
                        
                        if episode_length > 0:
                            self.all_episode_info.append({
                                'episode_id': episode_id,
                                'start_idx': episode_start,
                                'end_idx': episode_end,
                                'length': episode_length
                            })
                            
                    except Exception as e:
                        logger.warning(f"Failed to process episode {episode_id}: {e}")
                        continue
        else:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šå°è¯•é€šè¿‡éå†æ£€æµ‹episodeè¾¹ç•Œ
            logger.warning("No episode_data_index found, using fallback detection")
            # è¿™é‡Œå¯ä»¥å®ç°å¤‡ç”¨çš„episodeæ£€æµ‹é€»è¾‘
            # æš‚æ—¶å‡è®¾æ•´ä¸ªæ•°æ®é›†æ˜¯ä¸€ä¸ªå¤§episode
            dataset_len = len(self.lerobot_dataset)
            self.all_episode_info.append({
                'episode_id': 0,
                'start_idx': 0,
                'end_idx': dataset_len,
                'length': dataset_len
            })
        
        total_transitions = sum(ep['length'] for ep in self.all_episode_info)
        print(f"acrlpd_data_converter EPISODE DISCOVERY RESULTS:")
        print(f"   - Total episodes found: {len(self.all_episode_info)}")
        print(f"   - Total transitions: {total_transitions}")
        print(f"   - First 3 episodes:")
        for i, ep_info in enumerate(self.all_episode_info[:3]):
            print(f"     Episode {i}: ID={ep_info.get('episode_id', 'N/A')}, length={ep_info.get('length', 'N/A')}")
        
        logger.info(f"âœ“ å‘ç° {len(self.all_episode_info)} ä¸ªepisodesï¼Œæ€»è®¡ {total_transitions} ä¸ªtransitions")
        
        if len(self.all_episode_info) == 0:
            raise RuntimeError("No episodes found in dataset")
    
    def _setup_transforms(self):
        """è®¾ç½®OpenPIå˜æ¢ç®¡é“ï¼ˆå»¶è¿ŸåŠ è½½ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        
        print(f"acrlpd_data_converter TRANSFORM SETUP: Starting lazy initialization...")
        
        # å»¶è¿Ÿdata_configåˆ›å»ºï¼Œä»…å­˜å‚¨factory
        self.data_config_factory = self.rl_config.data
        self.data_config = None
        print(f"acrlpd_data_converter TRANSFORM SETUP: Stored data config factory for lazy loading")
        
        # é¢„è®¾transformä¸ºNoneï¼ŒçœŸæ­£éœ€è¦æ—¶æ‰åŠ è½½
        self.repack_transforms = None
        self.data_transforms = None  
        self.model_transforms = None
        self.norm_stats = None
        
        # æ ‡è®°æ˜¯å¦å·²åˆå§‹åŒ–transforms
        self._transforms_initialized = False
        
        logger.info("âœ“ Transformç³»ç»Ÿå»¶è¿Ÿåˆå§‹åŒ–å®Œæˆï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰")
    
    def _ensure_transforms_initialized(self):
        """å»¶è¿ŸåŠ è½½ï¼šä»…åœ¨çœŸæ­£éœ€è¦æ—¶æ‰åˆå§‹åŒ–transforms"""
        
        if self._transforms_initialized:
            return
            
        print(f"acrlpd_data_converter LAZY LOADING: First-time transform initialization...")
        
        # åˆ›å»ºå®é™…çš„DataConfig
        try:
            print(f"acrlpd_data_converter LAZY LOADING: Creating data config...")
            
            self.data_config = self.data_config_factory.create(
                assets_dirs=self.rl_config.assets_dirs,
                model_config=self.rl_config.model
            )
            print(f"acrlpd_data_converter LAZY LOADING: Data config created successfully")
        except Exception as e:
            print(f"ERROR: data_configåˆ›å»ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise e
        
        # æå–å˜æ¢ç®¡é“
        print(f"acrlpd_data_converter LAZY LOADING: Extracting transform pipelines...")
        self.repack_transforms = self.data_config.repack_transforms
        self.data_transforms = self.data_config.data_transforms
        self.model_transforms = self.data_config.model_transforms
        
        # å½’ä¸€åŒ–ç»Ÿè®¡ä¿¡æ¯
        if self.skip_norm_stats:
            print(f"acrlpd_data_converter LAZY LOADING: Skipping norm_stats loading (norm computation mode)")
            self.norm_stats = None
        else:
            print(f"acrlpd_data_converter LAZY LOADING: Loading norm_stats...")
            try:
                self.norm_stats = self.data_config.norm_stats
                print(f"acrlpd_data_converter LAZY LOADING: Got norm_stats successfully")
            except Exception as e:
                print(f"acrlpd_data_converter LAZY LOADING: Failed to load norm_stats: {e}")
                print(f"acrlpd_data_converter LAZY LOADING: Setting norm_stats to None for now...")
                self.norm_stats = None
        
        self._transforms_initialized = True
        print(f"acrlpd_data_converter LAZY LOADING: Transform initialization completed")
        logger.info("âœ“ Transformç³»ç»Ÿå»¶è¿ŸåŠ è½½å®Œæˆ")
    
    def _load_current_memory_pool(self):
        """qc_ACTé£æ ¼ï¼šéšæœºåŠ è½½episodes_per_memory_poolä¸ªepisodesåˆ°å†…å­˜æ± ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        
        if self.rank == 0:
            logger.info(f" åŠ è½½å†…å­˜æ±  (epoch {self.current_epoch_seed})")
        
        total_start_time = time.time()
        step_times = {}  # è¯¦ç»†è®¡æ—¶å„ä¸ªæ­¥éª¤
        
        # Step 1: Episodeé€‰æ‹©
        step_start = time.time()
        # åŸºäºepochç§å­å’ŒGPU rankåˆ›å»ºç‹¬ç«‹éšæœºçŠ¶æ€
        pool_rng = np.random.RandomState(self.current_epoch_seed + self.rank * 1000)
        
        # éšæœºé€‰æ‹©episodes
        if len(self.all_episode_info) >= self.episodes_per_memory_pool:
            selected_indices = pool_rng.choice(
                len(self.all_episode_info), 
                size=self.episodes_per_memory_pool, 
                replace=False  # ä¸é‡å¤é€‰æ‹©
            )
        else:
            # å¦‚æœæ€»episodeæ•°ä¸è¶³ï¼Œå…¨éƒ¨åŠ è½½
            selected_indices = list(range(len(self.all_episode_info)))
            logger.warning(f"æ€»episodesæ•°({len(self.all_episode_info)})å°äºå†…å­˜æ± å¤§å°({self.episodes_per_memory_pool})ï¼Œå…¨éƒ¨åŠ è½½")
        
        step_times['episode_selection'] = time.time() - step_start
        
        # Step 2: å¹¶è¡ŒåŠ è½½episodesï¼ˆPhase 2Aä¼˜åŒ–ï¼‰
        step_start = time.time()
        
        # æ¸…ç©ºæ—§å†…å­˜æ± 
        self.memory_pool_episodes.clear()
        self.memory_pool_lengths.clear()
        
        # acrlpd_data_converter Phase 2A: å¹¶è¡Œæ‰¹é‡åŠ è½½episodes
        loaded_episodes = 0
        failed_episodes = 0
        
        # ä½¿ç”¨å¹¶è¡ŒåŠ è½½æ›¿ä»£ä¸²è¡ŒåŠ è½½
        episode_results = self._parallel_load_episodes(selected_indices)
        
        # å¤„ç†åŠ è½½ç»“æœ
        for episode_data in episode_results:
            if episode_data is not None and len(episode_data) > 0:
                self.memory_pool_episodes.append(episode_data)
                self.memory_pool_lengths.append(len(episode_data))
                loaded_episodes += 1
            else:
                failed_episodes += 1
        
        step_times['parallel_loading'] = time.time() - step_start
        
        self.total_pool_transitions = sum(self.memory_pool_lengths)
        total_load_time = time.time() - total_start_time
        
        if self.rank == 0:
            logger.info(f"âœ“ å†…å­˜æ± åŠ è½½å®Œæˆ (epoch {self.current_epoch_seed}): "
                       f"æˆåŠŸåŠ è½½{loaded_episodes}ä¸ªepisodes, å¤±è´¥{failed_episodes}ä¸ª, "
                       f"æ€»è®¡{self.total_pool_transitions}ä¸ªtransitions, è€—æ—¶{total_load_time:.2f}s")
            
            # acrlpd_data_converter è¯¦ç»†æ€§èƒ½åˆ†ææ—¥å¿—
            logger.info(f"acrlpd_data_converter Phase 2Aæ€§èƒ½åˆ†æ:")
            for step_name, duration in step_times.items():
                percentage = (duration / total_load_time) * 100
                logger.info(f"   - {step_name}: {duration:.2f}s ({percentage:.1f}%)")
            
            # æ€§èƒ½æŒ‡æ ‡
            avg_time_per_episode = total_load_time / len(selected_indices) if len(selected_indices) > 0 else 0
            avg_time_per_transition = total_load_time / self.total_pool_transitions if self.total_pool_transitions > 0 else 0
            logger.info(f"ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡: å¹³å‡æ¯episode {avg_time_per_episode:.2f}s, å¹³å‡æ¯transition {avg_time_per_transition*1000:.1f}ms")
        
        if self.total_pool_transitions == 0:
            raise RuntimeError("Memory pool is empty after loading")
    
    def _parallel_load_episodes(self, selected_indices):
        """acrlpd_data_converter Phase 2A: å¹¶è¡ŒåŠ è½½å¤šä¸ªepisodes"""
        import concurrent.futures
        import time
        
        # acrlpd_data_converter é«˜æ€§èƒ½æœåŠ¡å™¨ä¼˜åŒ–ï¼šå……åˆ†åˆ©ç”¨64+æ ¸å¿ƒå’Œå……è¶³å†…å­˜
        # æ ¹æ®episodeæ•°é‡å’ŒæœåŠ¡å™¨èƒ½åŠ›é€‰æ‹©æœ€ä¼˜å¹¶è¡Œåº¦
        total_episodes = len(selected_indices)
        max_workers = min(64, total_episodes)  # å¤§æ•°æ®é›†ï¼šä½¿ç”¨æœ€å¤š64çº¿ç¨‹


        episode_results = []
        
        if max_workers > 1 and len(selected_indices) > 1:
            # acrlpd_data_converter é«˜æ€§èƒ½å¹¶è¡ŒåŠ è½½
            logger.info(f"acrlpd_data_converter å¯åŠ¨{max_workers}çº¿ç¨‹å¹¶è¡ŒåŠ è½½{total_episodes}ä¸ªepisodes...")
            parallel_start_time = time.time()
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰åŠ è½½ä»»åŠ¡
                future_to_idx = {}
                for episode_idx in selected_indices:
                    episode_info = self.all_episode_info[episode_idx]
                    future = executor.submit(self._load_episode_optimized, episode_info)
                    future_to_idx[future] = episode_idx
                
                # æ”¶é›†ç»“æœï¼ˆä¿æŒé¡ºåºï¼‰å¹¶æ˜¾ç¤ºè¿›åº¦
                results_dict = {}
                completed_episodes = 0
                total_samples = 0
                
                for future in concurrent.futures.as_completed(future_to_idx):
                    episode_idx = future_to_idx[future]
                    try:
                        episode_data = future.result()
                        results_dict[episode_idx] = episode_data
                        if episode_data:
                            total_samples += len(episode_data)
                    except Exception as e:
                        logger.warning(f"åŠ è½½episode {episode_idx}å¤±è´¥: {e}")
                        results_dict[episode_idx] = None
                    
                    completed_episodes += 1
                    
                    # æ¯å®Œæˆ10%æ˜¾ç¤ºè¯¦ç»†è¿›åº¦å’Œæ€§èƒ½æŒ‡æ ‡
                    if completed_episodes % max(1, total_episodes // 10) == 0 or completed_episodes == total_episodes:
                        elapsed = time.time() - parallel_start_time
                        progress_pct = (completed_episodes / total_episodes) * 100
                        avg_time_per_episode = elapsed / completed_episodes
                        eta = avg_time_per_episode * (total_episodes - completed_episodes)
                        episodes_per_sec = completed_episodes / max(elapsed, 0.001)
                        
                        logger.info(f"acrlpd_data_converter è¿›åº¦: {completed_episodes}/{total_episodes} ({progress_pct:.1f}%), "
                                  f"ç´¯è®¡{total_samples}ä¸ªsamples, "
                                  f"ç”¨æ—¶{elapsed:.1f}s, ETA:{eta:.1f}s, "
                                  f"é€Ÿåº¦{episodes_per_sec:.2f}episodes/s, å¹³å‡{avg_time_per_episode:.2f}s/episode")
                
                # æŒ‰åŸå§‹é¡ºåºè¿”å›ç»“æœ
                episode_results = [results_dict.get(idx, None) for idx in selected_indices]
                
                # acrlpd_data_converter æœ€ç»ˆæ€§èƒ½æ€»ç»“
                total_parallel_time = time.time() - parallel_start_time
                success_count = sum(1 for r in episode_results if r is not None)
                failure_count = total_episodes - success_count
                final_sample_count = sum(len(r) for r in episode_results if r is not None)
                
                logger.info(f"acrlpd_data_converter å¹¶è¡ŒåŠ è½½å®Œæˆ: {success_count}/{total_episodes}æˆåŠŸ ({failure_count}å¤±è´¥), "
                          f"æ€»è®¡{final_sample_count}ä¸ªsamples, "
                          f"æ€»è€—æ—¶{total_parallel_time:.2f}s, "
                          f"æœ€ç»ˆé€Ÿåº¦{success_count/max(total_parallel_time,0.001):.2f}episodes/s, "
                          f"æ ·æœ¬é€Ÿåº¦{final_sample_count/max(total_parallel_time,0.001):.1f}samples/s")
        else:
            # ä¸²è¡ŒåŠ è½½ï¼ˆfallbackï¼‰
            for episode_idx in selected_indices:
                episode_info = self.all_episode_info[episode_idx]
                try:
                    episode_data = self._load_episode_optimized(episode_info)
                    episode_results.append(episode_data)
                except Exception as e:
                    logger.warning(f"ä¸²è¡ŒåŠ è½½episode {episode_idx}å¤±è´¥: {e}")
                    episode_results.append(None)
        
        return episode_results
    
    def _load_episode_optimized(self, episode_info: dict) -> List[dict]:
        """acrlpd_data_converter Phase 2A: ä¼˜åŒ–çš„å•ä¸ªepisodeåŠ è½½æ–¹æ³•"""
        episode_data = []
        start_idx = episode_info['start_idx']
        end_idx = episode_info['end_idx']
        
        # acrlpd_data_converter æ‰¹é‡è®¿é—®ä¼˜åŒ–ï¼šå°è¯•æ‰¹é‡åŠ è½½æ•´ä¸ªepisode
        try:
            batch_indices = list(range(start_idx, end_idx))
            
            # acrlpd_data_converter æ·±å…¥æ¢ç´¢LeRobotæ‰¹é‡åŠ è½½çš„å¯èƒ½æ€§
            # è™½ç„¶åˆ—è¡¨ç´¢å¼•ä¸æ”¯æŒï¼Œä½†å¯èƒ½æœ‰å…¶ä»–æ‰¹é‡è®¿é—®æ–¹æ³•
            
            batch_loaded_data = None
            
            # æ–¹æ³•1: æ·±å…¥æ¢ç´¢LeRobotæ•°æ®é›†çš„å†…éƒ¨ç»“æ„
            try:
                # é¦–æ¬¡è¿è¡Œæ—¶è¯¦ç»†åˆ†ææ•°æ®é›†ç»“æ„
                if not hasattr(self, '_dataset_structure_analyzed'):
                    self._dataset_structure_analyzed = True
                    
                    # åˆ†ææ‰€æœ‰å¯èƒ½çš„æ•°æ®è®¿é—®å±æ€§
                    potential_data_attrs = []
                    for attr_name in ['hf_dataset', '_hf_dataset', 'dataset', '_dataset', 
                                    'data', '_data', 'episodes', '_episodes']:
                        if hasattr(self.lerobot_dataset, attr_name):
                            attr_obj = getattr(self.lerobot_dataset, attr_name)
                            potential_data_attrs.append((attr_name, type(attr_obj).__name__))
                    
                    logger.info(f"acrlpd_data_converter LeRobotå†…éƒ¨æ•°æ®å±æ€§: {potential_data_attrs}")
                
                # acrlpd_data_converter å…³é”®ä¿®å¤ï¼šä½¿ç”¨LeRobotDatasetçš„æ­£ç¡®__getitem__æ–¹æ³•è€Œä¸æ˜¯ç»•è¿‡å®ƒ
                # è¿™ç¡®ä¿äº†è§†é¢‘è§£ç é€»è¾‘è¢«æ­£ç¡®æ‰§è¡Œ
                try:
                    # acrlpd_data_converter é«˜æ€§èƒ½ä¼˜åŒ–åŠ è½½ï¼šå‡å°‘è§†é¢‘è§£ç å¼€é”€ + è¯¦ç»†è®¡æ—¶
                    episode_id = episode_info.get('episode_id', 'unknown')
                    episode_start_time = time.time()
                    episode_length = len(batch_indices)
                    
                    batch_loaded_data = []
                    logger.info(f"acrlpd_data_converter Episode {episode_id}:  {episode_length}å¸§")
                    # æ‰¹é‡åŠ è½½å¹¶è®¡æ—¶
                    load_count = 0
                    for idx in batch_indices:
                        try:
                            # å…³é”®ï¼šä½¿ç”¨LeRobotDatasetçš„__getitem__æ–¹æ³•ï¼Œè§¦å‘è§†é¢‘è§£ç 
                            sample = self.lerobot_dataset[idx]
                            batch_loaded_data.append(sample)
                            load_count += 1
                        except Exception as e:
                            logger.debug(f"LeRobotDataset[{idx}]åŠ è½½å¤±è´¥: {e}")
                            continue
                    
                    episode_load_time = time.time() - episode_start_time
                    fps = load_count / max(episode_load_time, 0.001)
                    logger.info(f"acrlpd_data_converter Episode {episode_id}: åŠ è½½{load_count}å¸§, è€—æ—¶{episode_load_time:.2f}s, é€Ÿåº¦{fps:.1f}å¸§/s")
                except Exception as e:
                    logger.debug(f"LeRobotDataset.__getitem__æ‰¹é‡åŠ è½½å¤±è´¥: {e}")
                    batch_loaded_data = None
                                
            except Exception as e:
                logger.debug(f"åº•å±‚æ•°æ®é›†æ‰¹é‡è®¿é—®å¤±è´¥: {e}")
            
            # æ–¹æ³•2: å¤šçº¿ç¨‹LeRobotDatasetè®¿é—®ï¼ˆå¦‚æœä¸»æ–¹æ³•å¤±è´¥ä¸”æ•°æ®é‡å¤§ï¼‰
            if batch_loaded_data is None and len(batch_indices) > 4:  # åªåœ¨è¶³å¤Ÿå¤šæ•°æ®æ—¶ä½¿ç”¨å¤šçº¿ç¨‹
                try:
                    import concurrent.futures
                    
                    def fetch_single_sample(idx):
                        # ç¡®ä¿ä½¿ç”¨LeRobotDatasetçš„__getitem__æ–¹æ³•
                        return self.lerobot_dataset[idx]
                    
                    logger.info(f"acrlpd_data_converter ä½¿ç”¨å¤šçº¿ç¨‹LeRobotDataset.__getitem__æ–¹æ³•...")
                    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                        future_to_idx = {executor.submit(fetch_single_sample, idx): idx for idx in batch_indices}
                        batch_loaded_data = [None] * len(batch_indices)
                        
                        for future in concurrent.futures.as_completed(future_to_idx):
                            idx = future_to_idx[future]
                            original_idx = batch_indices.index(idx)
                            try:
                                batch_loaded_data[original_idx] = future.result()
                            except Exception as e:
                                logger.warning(f"å¤šçº¿ç¨‹æ ·æœ¬{idx}å¤±è´¥: {e}")
                                batch_loaded_data[original_idx] = None
                    
                    # è¿‡æ»¤Noneå€¼
                    batch_loaded_data = [sample for sample in batch_loaded_data if sample is not None]
                except Exception as e:
                    logger.debug(f"å¤šçº¿ç¨‹LeRobotDatasetè®¿é—®å¤±è´¥: {e}")
                    batch_loaded_data = None
            
            # å¦‚æœæ‰¹é‡åŠ è½½æˆåŠŸï¼Œè¿”å›ç»“æœ
            if batch_loaded_data and len(batch_loaded_data) > 0:
                return batch_loaded_data
            
            # å›é€€æ–¹æ³•ï¼šé€ä¸ªä½¿ç”¨LeRobotDataset.__getitem__è®¿é—®ï¼ˆç¡®ä¿è§†é¢‘è§£ç ï¼‰
            episode_data = [None] * len(batch_indices)
            
            for i, transition_idx in enumerate(batch_indices):
                try:
                    python_idx = int(transition_idx)
                    # å…³é”®ï¼šä½¿ç”¨LeRobotDatasetçš„__getitem__æ–¹æ³•ï¼Œç¡®ä¿è§†é¢‘è§£ç 
                    sample = self.lerobot_dataset[python_idx]
                    episode_data[i] = sample
                except Exception as e:
                    logger.debug(f"LeRobotDataset[{transition_idx}]åŠ è½½å¤±è´¥: {e}")
                    episode_data[i] = None
            
            # è¿‡æ»¤æ‰å¤±è´¥çš„transitions
            episode_data = [sample for sample in episode_data if sample is not None]
            
        except Exception as e:
            logger.warning(f"Episode {episode_info['episode_id']}æ‰¹é‡åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°åŸæ–¹æ³•: {e}")
            # å®Œå…¨å›é€€åˆ°åŸæ¥çš„æ–¹æ³•
            episode_data = self._load_complete_episode_fallback(episode_info)
        
        # æ˜¾ç¤ºepisodeåŠ è½½ç»“æœ
        if episode_data:
            logger.info(f"âœ… Episode {episode_info['episode_id']}: {len(episode_data)} samples")
        else:
            logger.warning(f"acrlpd_data_converter Episode {episode_info['episode_id']}: 0 samples (failed)")
            
        return episode_data
    
    def _load_complete_episode_fallback(self, episode_info: dict) -> List[dict]:
        """å®Œå…¨å›é€€çš„episodeåŠ è½½æ–¹æ³•ï¼ˆç¡®ä¿ä½¿ç”¨LeRobotDataset.__getitem__ï¼‰"""
        episode_data = []
        start_idx = episode_info['start_idx']
        end_idx = episode_info['end_idx']
        
        for transition_idx in range(start_idx, end_idx):
            try:
                python_idx = int(transition_idx)
                # å…³é”®ï¼šä½¿ç”¨LeRobotDatasetçš„__getitem__æ–¹æ³•ï¼Œç¡®ä¿è§†é¢‘è§£ç 
                sample = self.lerobot_dataset[python_idx]
                episode_data.append(sample)
            except Exception as e:
                logger.debug(f"LeRobotDataset[{transition_idx}] in episode {episode_info['episode_id']}å¤±è´¥: {e}")
                continue
        
        # æ˜¾ç¤ºepisodeåŠ è½½ç»“æœï¼ˆfallbackæ–¹æ³•ï¼‰
        if episode_data:
            logger.info(f"âœ… Episode {episode_info['episode_id']} (fallback): {len(episode_data)} samples")
        else:
            logger.warning(f"acrlpd_data_converter Episode {episode_info['episode_id']} (fallback): 0 samples (failed)")
            
        return episode_data
    
    def _load_complete_episode(self, episode_info: dict) -> List[dict]:
        """ä»LeRobotæ•°æ®é›†åŠ è½½å®Œæ•´episodeæ•°æ®ï¼ˆä¿æŒå‘åå…¼å®¹ï¼Œç°åœ¨ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        # acrlpd_data_converter Phase 2A: ä½¿ç”¨ä¼˜åŒ–çš„åŠ è½½æ–¹æ³•
        return self._load_episode_optimized(episode_info)
    
    def sample_batch(self) -> Dict[str, jnp.ndarray]:
        """åŸºäºqc_ACTæ ‡å‡†çš„Q-chunking batchç”Ÿæˆ"""
        
        if self.total_pool_transitions == 0:
            raise RuntimeError("Memory pool is empty. Call refresh_memory_pool() first.")
        
        # Step 1: æ™ºèƒ½é‡‡æ · - ç¡®ä¿å¯ä»¥æ„å»ºå®Œæ•´action sequence
        valid_starts = self._sample_valid_sequence_starts(
            self.batch_size, 
            self.qchunking_config.horizon_length
        )
        
        # Step 2: ä¸ºæ¯ä¸ªèµ·å§‹ç‚¹æ„å»ºå®Œæ•´çš„Q-chunking transition
        batch_transitions = []
        for ep_idx, start_idx in valid_starts:
            transition = self._build_qc_transition_from_episode(
                ep_idx, start_idx, self.qchunking_config.horizon_length
            )
            batch_transitions.append(transition)
        
        # Step 3: æ‰¹æ¬¡æ•´ç† - å°†åˆ—è¡¨è½¬æ¢ä¸ºJAX arrays
        batch_dict = self._collate_qc_batch(batch_transitions)
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats['batches_served'] += 1
        
        return batch_dict
    
    def _sample_valid_sequence_starts(self, batch_size: int, horizon_length: int) -> List[Tuple[int, int]]:
        """acrlpd_data_converter CHUNK-AWAREé‡‡æ ·ï¼šæ™ºèƒ½è¯†åˆ«chunkå‹æ•°æ®ï¼Œåªä»æœ‰å›¾åƒçš„chunkèµ·å§‹ç‚¹å¼€å§‹"""
        
        valid_positions = []
        
        # æ”¶é›†æ‰€æœ‰å¯èƒ½çš„èµ·å§‹ä½ç½®ï¼ˆæ”¯æŒchunkå‹å’Œæ­£å¸¸æ•°æ®ï¼‰
        for ep_idx, ep_length in enumerate(self.memory_pool_lengths):
            episode_data = self.memory_pool_episodes[ep_idx]
            
            # acrlpd_data_converter CHECK: æ£€æµ‹æ˜¯å¦ä¸ºchunkå‹æ•°æ®ï¼ˆé€šè¿‡metadataæ£€æŸ¥ï¼‰
            if len(episode_data) > 0:
                sample_frame = episode_data[0]
                is_chunked_episode = not sample_frame.get('has_image', True)  # å¦‚æœç¬¬ä¸€å¸§æ²¡æœ‰å›¾åƒï¼Œå¯èƒ½æ˜¯chunkå‹
                
                # è¿›ä¸€æ­¥éªŒè¯ï¼šæ£€æŸ¥is_chunk_startæ¨¡å¼
                chunk_starts = [sample.get('is_chunk_start', True) for sample in episode_data]
                has_images = [sample.get('has_image', True) for sample in episode_data]
                is_chunked_episode = not all(has_images)  # å¦‚æœä¸æ˜¯æ‰€æœ‰å¸§éƒ½æœ‰å›¾åƒï¼Œåˆ™ä¸ºchunkå‹
                
                if is_chunked_episode:
                    # acrlpd_data_converter CHUNK-TYPE: åªä»æœ‰å›¾åƒä¸”ä¸ºchunkå¼€å§‹çš„ä½ç½®é‡‡æ ·
                    logger.debug(f"Episode {ep_idx}: Detected chunk-type data")
                    for pos in range(ep_length):
                        if pos < len(episode_data):
                            sample = episode_data[pos]
                            # å¿…é¡»åŒæ—¶æ»¡è¶³ï¼šæœ‰å›¾åƒ AND æ˜¯chunkå¼€å§‹
                            if sample.get('has_image', False) and sample.get('is_chunk_start', False):
                                valid_positions.append((ep_idx, pos))
                                logger.debug(f"  Valid chunk start: frame {pos}")
                else:
                    # acrlpd_data_converter NORMAL-TYPE: åŸæœ‰é€»è¾‘ï¼Œä»»ä½•ä½ç½®éƒ½å¯ä»¥å¼€å§‹
                    for pos in range(ep_length):
                        valid_positions.append((ep_idx, pos))
        
        if len(valid_positions) == 0:
            raise RuntimeError("No valid starting positions found - check chunk data or metadata")
        
        logger.debug(f"Found {len(valid_positions)} valid sequence starting positions")
        
        # éšæœºé‡‡æ ·
        selected_indices = self.rng.choice(
            len(valid_positions), 
            size=batch_size, 
            replace=True if len(valid_positions) < batch_size else False
        )
        
        return [valid_positions[i] for i in selected_indices]
    
    def _find_next_image_frame(self, episode_data: List[dict], start_idx: int) -> Optional[int]:
        """acrlpd_data_converter CHUNK-HELPER: å¯»æ‰¾ä»start_idxå¼€å§‹çš„ä¸‹ä¸€ä¸ªæœ‰å›¾åƒçš„å¸§"""
        for idx in range(start_idx, len(episode_data)):
            if episode_data[idx].get('has_image', True):
                return idx
        return None
    
    def _build_qc_transition_from_episode(self, ep_idx: int, start_idx: int, horizon_length: int) -> Dict[str, Any]:
        """åŸºäºqc_ACTçš„å®Œæ•´transitionæ„å»ºé€»è¾‘"""
        
        episode_data = self.memory_pool_episodes[ep_idx]
        episode_length = len(episode_data)
        
        # === 1. å½“å‰è§‚æµ‹æ„å»º ===
        current_sample = episode_data[start_idx]
        current_obs = self._sample_to_pi0_observation(current_sample)
        
        # === 2. åŠ¨ä½œåºåˆ—æ„å»ºï¼ˆqc_ACTæ ¸å¿ƒé€»è¾‘ï¼‰===
        action_dim = self._get_action_dim(current_sample)
        actions = np.zeros((horizon_length, action_dim), dtype=np.float32)
        rewards = np.zeros(horizon_length, dtype=np.float32)
        masks = np.zeros(horizon_length, dtype=np.float32)
        valid = np.zeros(horizon_length, dtype=np.float32)
        terminals = np.zeros(horizon_length, dtype=np.bool_)
        
        # æ„å»ºåºåˆ—çš„æ¯ä¸€æ­¥
        for i in range(horizon_length):
            step_idx = start_idx + i
            
            if step_idx < episode_length:
                # === åœ¨episodeèŒƒå›´å†… ===
                actions[i] = self._extract_action_from_sample(episode_data[step_idx])
                
                # qc_ACTé£æ ¼å¥–åŠ±ï¼šé€šå¸¸åœ¨æœ€åä¸€æ­¥ç»™å¥–åŠ±
                if i == horizon_length - 1:
                    rewards[i] = self._calculate_step_reward(episode_data, step_idx, i, horizon_length)
                else:
                    rewards[i] = 0.0  # ä¸­é—´æ­¥éª¤ä¾èµ–bootstrap
                    
                masks[i] = 1.0      # æœ‰æ•ˆçš„mask
                valid[i] = 1.0      # æœ‰æ•ˆçš„åŠ¨ä½œ
                terminals[i] = (step_idx == episode_length - 1)  # episodeæœ€åä¸€æ­¥
                
            else:
                # === è¶…å‡ºepisodeè¾¹ç•Œï¼Œä½¿ç”¨padding ===
                if i > 0:
                    actions[i] = actions[i-1]  # é‡å¤æœ€åä¸€ä¸ªæœ‰æ•ˆåŠ¨ä½œ
                else:
                    actions[i] = self._extract_action_from_sample(episode_data[start_idx])
                    
                rewards[i] = 0.0    # Paddingæ­¥éª¤å¥–åŠ±ä¸º0
                masks[i] = 0.0      # æ— æ•ˆçš„mask  
                valid[i] = 0.0      # æ— æ•ˆçš„åŠ¨ä½œ
                terminals[i] = True # è¶…è¾¹ç•Œå³ä¸ºterminal
        
        # === 3. Next Observationsæ„å»ºï¼ˆBootstrapå¿…éœ€ï¼‰===
        next_idx = start_idx + horizon_length
        next_is_terminal = False
        
        if next_idx < episode_length:
            # ä¸‹ä¸€å¸§åœ¨episodeå†…
            next_sample = episode_data[next_idx]
            
            # acrlpd_data_converter CHUNK-TYPE HANDLING: æ£€æŸ¥next_sampleæ˜¯å¦æœ‰å›¾åƒ
            if not next_sample.get('has_image', True):
                # Chunkå‹æ•°æ®ï¼šnext_idxä½ç½®æ²¡æœ‰å›¾åƒï¼Œå¯»æ‰¾ä¸‹ä¸€ä¸ªæœ‰å›¾åƒçš„chunkå¼€å§‹
                next_with_image_idx = self._find_next_image_frame(episode_data, next_idx)
                if next_with_image_idx is not None and next_with_image_idx < episode_length:
                    next_sample = episode_data[next_with_image_idx]
                    next_obs = self._sample_to_pi0_observation(next_sample)
                    next_is_terminal = (next_with_image_idx == episode_length - 1)
                else:
                    # æ²¡æœ‰æ‰¾åˆ°ä¸‹ä¸€ä¸ªæœ‰å›¾åƒçš„å¸§ï¼Œä½¿ç”¨current_obs
                    next_obs = current_obs
                    next_is_terminal = True
            else:
                # æ­£å¸¸æƒ…å†µï¼šnext_sampleæœ‰å›¾åƒ
                next_obs = self._sample_to_pi0_observation(next_sample)
                next_is_terminal = (next_idx == episode_length - 1)
        else:
            # è¶…å‡ºè¾¹ç•Œï¼Œä½¿ç”¨terminalå¤„ç†
            next_obs = current_obs  # Terminal state convention
            next_is_terminal = True
        
        # === 4. æ„å»ºå®Œæ•´transitionï¼ˆqc_ACTæ ¼å¼ï¼‰===
        return {
            'observations': current_obs,
            'next_observations': next_obs,
            'actions': actions,                    # [H, action_dim]
            'rewards': rewards,                    # [H]
            'masks': masks,                       # [H] - bootstrap mask
            'valid': valid,                       # [H] - action validity
            'terminals': terminals,               # [H] - step terminals  
            'next_terminal': next_is_terminal,    # scalar - next state terminal
            'sequence_mask': np.ones(1, dtype=np.bool_)[0]  # åºåˆ—çº§æœ‰æ•ˆæ€§
        }
    
    def _map_global_to_episode_transition(self, global_idx: int) -> Tuple[int, int]:
        """å°†å…¨å±€ç´¢å¼•æ˜ å°„åˆ°(episode_idx, transition_idx)"""
        
        if global_idx >= self.total_pool_transitions or global_idx < 0:
            raise IndexError(f"Global index {global_idx} out of range [0, {self.total_pool_transitions})")
        
        cumulative = 0
        for episode_idx, episode_length in enumerate(self.memory_pool_lengths):
            if global_idx < cumulative + episode_length:
                transition_idx = global_idx - cumulative
                return episode_idx, transition_idx
            cumulative += episode_length
        
        # ä¸åº”è¯¥åˆ°è¾¾è¿™é‡Œ
        raise IndexError(f"Failed to map global index {global_idx} to episode transition")
    
    def _sample_to_pi0_observation(self, sample: dict) -> _model.Observation:
        """å°†å•ä¸ªLeRobotæ ·æœ¬è½¬æ¢ä¸ºÏ€â‚€ Observationæ ¼å¼"""
        
        # åº”ç”¨OpenPIå˜æ¢ç®¡é“
        transformed_sample = self._apply_openpi_transforms(sample)
        
        # è½¬æ¢ä¸ºÏ€â‚€æ ¼å¼
        observation, _ = self._to_openpi_format([transformed_sample])
        
        # è¿”å›å•ä¸ªè§‚æµ‹ï¼ˆå»æ‰batchç»´åº¦ï¼‰
        return jax.tree.map(lambda x: x[0], observation)
    
    def _get_action_dim(self, sample: dict) -> int:
        """è·å–åŠ¨ä½œç»´åº¦ï¼ˆä¼˜å…ˆä½¿ç”¨æ¨¡å‹é…ç½®ï¼‰"""
        
        # é¦–å…ˆå°è¯•ä»æ¨¡å‹é…ç½®è·å–
        if hasattr(self.rl_config.model, 'action_dim'):
            return self.rl_config.model.action_dim
        
        # æ ‡å‡†é”®åå°è¯•
        for action_key in ["action", "actions", "observation.action"]:
            if action_key in sample:
                action = sample[action_key]
                if isinstance(action, (list, np.ndarray)):
                    action_array = np.array(action)
                    if action_array.ndim >= 2:  # [H, action_dim]
                        return action_array.shape[-1]
                    else:  # [action_dim]
                        return len(action_array)
        
        # é»˜è®¤å€¼ï¼ˆALOHAæœºå™¨äººï¼‰
        logger.warning("Could not determine action dimension, using default 14")
        return 14
    
    def _extract_action_from_sample(self, sample: dict) -> np.ndarray:
        """ä»å•ä¸ªæ ·æœ¬ä¸­æå–åŠ¨ä½œï¼ˆä½¿ç”¨OpenPI transformåçš„æ•°æ®ï¼‰"""
        
        # **å…³é”®ä¿®å¤ï¼šé¦–å…ˆåº”ç”¨OpenPI transformsï¼Œç¡®ä¿14ç»´â†’32ç»´è½¬æ¢**
        transformed_sample = self._apply_openpi_transforms(sample)
        
        # ä»å·²è½¬æ¢çš„æ ·æœ¬ä¸­æå–actionsï¼ˆåº”è¯¥å·²ç»æ˜¯32ç»´ï¼‰
        if "actions" in transformed_sample:
            action = transformed_sample["actions"]
            # å¤„ç†torch.Tensor
            if hasattr(action, 'numpy'):
                action = action.numpy()
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            if isinstance(action, (list, np.ndarray)):
                action_array = np.array(action, dtype=np.float32)
                # LeRobot actionæ ¼å¼ï¼š[action_dim] æˆ– [horizon, action_dim]
                if action_array.ndim >= 2:  # [H, action_dim] - å–ç¬¬ä¸€ä¸ªåŠ¨ä½œ
                    action_array = action_array[0]
                
                return action_array
        
        # å¤‡ç”¨ï¼šå°è¯•ä»transformed_sampleä¸­çš„å…¶ä»–é”®
        for action_key in ["action", "observation.action"]:
            if action_key in transformed_sample:
                action = transformed_sample[action_key]
                if hasattr(action, 'numpy'):
                    action = action.numpy()
                if isinstance(action, (list, np.ndarray)):
                    action_array = np.array(action, dtype=np.float32)
                    if action_array.ndim >= 2:
                        action_array = action_array[0]
                    
                    return action_array
        
        # å¦‚æœæ‰¾ä¸åˆ°åŠ¨ä½œï¼Œæ‰“å°è°ƒè¯•ä¿¡æ¯å¹¶è¿”å›é›¶åŠ¨ä½œ
        if not hasattr(self, '_debug_logged'):
            logger.warning(f"Could not extract action from transformed sample. Available keys: {list(transformed_sample.keys())}")
            if "actions" in transformed_sample:
                action_data = transformed_sample["actions"]
                logger.warning(f"Actions data type: {type(action_data)}, shape: {getattr(action_data, 'shape', 'no shape')}")
            self._debug_logged = True
        
        logger.warning("Could not extract action from sample, returning zero action")
        # ä½¿ç”¨æ¨¡å‹çš„action_dimï¼ˆåº”è¯¥æ˜¯32ç»´ï¼‰
        model_action_dim = getattr(self.rl_config.model, 'action_dim', 32)
        return np.zeros(model_action_dim, dtype=np.float32)
    
    def _calculate_step_reward(self, episode_data: List[dict], step_idx: int, seq_pos: int, horizon_length: int) -> float:
        """è®¡ç®—å•æ­¥å¥–åŠ±ï¼ˆå¯æ ¹æ®å…·ä½“ä»»åŠ¡å®šåˆ¶ï¼‰"""
        
        # åŸºç¡€å®ç°ï¼šä» LeRobot æ•°æ®ä¸­æå–å¥–åŠ±
        if 'reward' in episode_data[step_idx]:
            return float(episode_data[step_idx]['reward'])
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šåŸºäºepisodeæˆåŠŸ/å¤±è´¥çš„ç¨€ç–å¥–åŠ±
        if seq_pos == horizon_length - 1:  # ä»…åœ¨åºåˆ—æœ€åç»™å¥–åŠ±
            # å¯ä»¥åŸºäºepisode metadataåˆ¤æ–­æˆåŠŸ/å¤±è´¥
            return 1.0  # ç®€åŒ–ä¸ºå›ºå®šå¥–åŠ±
        
        return 0.0
    
    def _apply_openpi_transforms(self, raw_sample: dict) -> dict:
        """å¯¹å•ä¸ªæ ·æœ¬åº”ç”¨OpenPIå˜æ¢ç®¡é“ï¼ˆå»¶è¿ŸåŠ è½½ç‰ˆæœ¬ï¼‰"""
        
        # acrlpd_data_converter å…³é”®ï¼šç¡®ä¿transformsåœ¨ä½¿ç”¨å‰å·²åˆå§‹åŒ–
        self._ensure_transforms_initialized()
        
        transformed_sample = raw_sample.copy()
        
        # åº”ç”¨ä¸‰é˜¶æ®µå˜æ¢ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        try:
            if self.repack_transforms:
                if hasattr(self.repack_transforms, '__call__'):
                    transformed_sample = self.repack_transforms(transformed_sample)
                elif hasattr(self.repack_transforms, 'inputs'):  # ä¿®å¤ï¼šGroupå¯¹è±¡ä½¿ç”¨inputså±æ€§
                    for transform in self.repack_transforms.inputs:
                        transformed_sample = transform(transformed_sample)
                elif hasattr(self.repack_transforms, 'transforms'):  # å›é€€å…¼å®¹æ€§
                    for transform in self.repack_transforms.transforms:
                        transformed_sample = transform(transformed_sample)
                        
            if self.data_transforms:
                # Apply data transforms correctly
                if hasattr(self.data_transforms, 'inputs'):
                    for i, transform in enumerate(self.data_transforms.inputs):
                        transformed_sample = transform(transformed_sample)
                elif hasattr(self.data_transforms, '__call__'):
                    transformed_sample = self.data_transforms(transformed_sample)
                else:
                    logger.warning(f"Unknown data_transforms type: {type(self.data_transforms)}")
                        
            if self.model_transforms:
                if hasattr(self.model_transforms, '__call__'):
                    transformed_sample = self.model_transforms(transformed_sample)
                elif hasattr(self.model_transforms, 'inputs'):
                    for i, transform in enumerate(self.model_transforms.inputs):
                        transformed_sample = transform(transformed_sample)
                elif hasattr(self.model_transforms, 'transforms'):
                    for transform in self.model_transforms.transforms:
                        transformed_sample = transform(transformed_sample)
        except Exception as e:
            import traceback
            logger.error(f"Transform application failed, using raw sample: {e}")
            logger.error(f"Full traceback: {traceback.format_exc()}")
            logger.error(f"Raw sample keys: {raw_sample.keys() if isinstance(raw_sample, dict) else type(raw_sample)}")
            
            # acrlpd_data_converter è¯¦ç»†è°ƒè¯•ï¼šæ£€æŸ¥å›¾åƒæ•°æ®æ˜¯å¦å­˜åœ¨
            if isinstance(raw_sample, dict):
                image_keys = [k for k in raw_sample.keys() if 'image' in k.lower()]
                logger.error(f"Available image keys: {image_keys}")
                
                # æ£€æŸ¥observationç»“æ„
                if 'observation' in raw_sample:
                    obs = raw_sample['observation']
                    logger.error(f"observation type: {type(obs)}")
                    if hasattr(obs, 'keys'):
                        logger.error(f"observation keys: {list(obs.keys())}")
                    elif isinstance(obs, dict):
                        logger.error(f"observation dict keys: {obs.keys()}")
            
            transformed_sample = raw_sample
        
        # åº”ç”¨å½’ä¸€åŒ–ï¼ˆå¦‚æœæœ‰norm_statsï¼‰
        if self.norm_stats:
            transformed_sample = self._apply_normalization(transformed_sample)
        
        return transformed_sample
    
    def _apply_normalization(self, sample: dict) -> dict:
        """åº”ç”¨å½’ä¸€åŒ–ç»Ÿè®¡"""
        
        # è¿™é‡Œåº”è¯¥ä½¿ç”¨openpiçš„normalizeæ¨¡å—
        # å…·ä½“å®ç°å–å†³äºnorm_statsçš„ç»“æ„
        try:
            if hasattr(_normalize, 'apply') and callable(_normalize.apply):
                return _normalize.apply(sample, self.norm_stats)
            else:
                # å¤‡ç”¨çš„æ‰‹åŠ¨å½’ä¸€åŒ–
                return self._manual_normalization(sample)
        except Exception as e:
            logger.warning(f"Normalization failed: {e}, returning original sample")
            return sample
    
    def _manual_normalization(self, sample: dict) -> dict:
        """æ‰‹åŠ¨å½’ä¸€åŒ–å¤‡ç”¨æ–¹æ¡ˆ"""
        
        # ç®€å•çš„çŠ¶æ€å½’ä¸€åŒ–ç¤ºä¾‹
        normalized_sample = sample.copy()
        
        if "observation.state" in sample and "qpos" in self.norm_stats:
            state = np.array(sample["observation.state"])
            qpos_stats = self.norm_stats["qpos"]
            
            if hasattr(qpos_stats, 'mean') and hasattr(qpos_stats, 'std'):
                normalized_state = (state - qpos_stats.mean) / qpos_stats.std
                normalized_sample["observation.state"] = normalized_state
        
        return normalized_sample
    
    def _collate_qc_batch(self, transitions: List[Dict[str, Any]]) -> Dict[str, jnp.ndarray]:
        """å°†Q-chunking transitionåˆ—è¡¨æ•´ç†æˆJAX batchæ ¼å¼"""
        
        if not transitions:
            raise ValueError("Empty transition list")
        
        # å‡†å¤‡æ‰¹æ¬¡å­—å…¸
        batch = {}
        
        # === å¤„ç†Ï€â‚€ Observationå¯¹è±¡ ===
        observations = []
        next_observations = []
        
        for trans in transitions:
            observations.append(trans['observations'])
            next_observations.append(trans['next_observations'])
        
        # ä½¿ç”¨OpenPIçš„Observationæ‰¹æ¬¡åŒ–
        batch['observations'] = self._batch_pi0_observations(observations)
        batch['next_observations'] = self._batch_pi0_observations(next_observations)
        
        # === å¤„ç†å…¶ä»–æ•°ç»„å­—æ®µ ===
        array_keys = ['actions', 'rewards', 'masks', 'valid', 'terminals']
        for key in array_keys:
            arrays = [trans[key] for trans in transitions]
            batch[key] = jnp.array(arrays)  # [B, H, ...] or [B, H]
        
        # === å¤„ç†æ ‡é‡å­—æ®µ ===  
        scalar_keys = ['next_terminal', 'sequence_mask']
        for key in scalar_keys:
            scalars = [trans[key] for trans in transitions]
            batch[key] = jnp.array(scalars)  # [B]
        
        return batch
    
    def _batch_pi0_observations(self, observations: List[_model.Observation]) -> _model.Observation:
        """æ‰¹æ¬¡åŒ–Ï€â‚€ Observationå¯¹è±¡"""
        
        if not observations:
            raise ValueError("Empty observation list")
        
        # acrlpd_data_converter è°ƒè¯•ï¼šæ£€æŸ¥ä¼ å…¥çš„observationsç±»å‹
        logger.debug(f"æ‰¹æ¬¡åŒ–observations: æ•°é‡={len(observations)}")
        for i, obs in enumerate(observations[:2]):  # åªæ£€æŸ¥å‰2ä¸ª
            logger.debug(f"  obs[{i}] type: {type(obs)}")
            if hasattr(obs, 'keys'):
                logger.debug(f"  obs[{i}] keys: {list(obs.keys())}")
        
        # acrlpd_data_converter é˜²æŠ¤æªæ–½ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯Columnå¯¹è±¡
        for i, obs in enumerate(observations):
            if hasattr(obs, '__class__') and 'Column' in str(type(obs)):
                logger.error(f"acrlpd_data_converter æ£€æµ‹åˆ°Columnå¯¹è±¡ obs[{i}]: {type(obs)}")
                raise TypeError(f"Cannot batch Column objects. Transform failed for observation {i}. "
                               f"Expected _model.Observation, got {type(obs)}")
        
        # ä½¿ç”¨JAX treeæ“ä½œæ‰¹æ¬¡åŒ–
        def stack_observations(*obs_list):
            return jax.tree.map(lambda *arrays: jnp.stack(arrays, axis=0), *obs_list)
        
        return stack_observations(*observations)
    
    def _collate_batch(self, samples: List[dict]) -> dict:
        """å°†æ ·æœ¬åˆ—è¡¨æ•´ç†æˆæ‰¹æ¬¡æ ¼å¼"""
        
        if not samples:
            raise ValueError("Cannot collate empty sample list")
        
        # ä½¿ç”¨JAXçš„treeå·¥å…·è¿›è¡Œæ‰¹æ¬¡æ•´ç†
        def stack_arrays(*arrays):
            arrays = [np.asarray(arr) for arr in arrays]
            return np.stack(arrays, axis=0)
        
        return jax.tree.map(stack_arrays, *samples)
    
    def _to_openpi_format(self, batch_samples: List[dict]) -> Tuple[_model.Observation, jnp.ndarray]:
        """è½¬æ¢ä¸ºOpenPIæ ‡å‡†æ ¼å¼ï¼ˆä»…ç”¨äº_sample_to_pi0_observationï¼‰"""
        
        # æ‰¹æ¬¡æ•´ç†
        batch = self._collate_batch(batch_samples)
        
        # æå–åŠ¨ä½œåºåˆ—ï¼ˆåœ¨è¿™é‡Œä¸é‡è¦ï¼Œä¸»è¦æ˜¯ä¸ºObservationæœåŠ¡ï¼‰
        actions = jnp.zeros((len(batch_samples), 1))  # å ä½ç¬¦
        
        # æ„å»ºå›¾åƒå­—å…¸
        image_dict = {}
        image_mask_dict = {}
        
        # æ£€æŸ¥LeRobotæ ‡å‡†å›¾åƒæ ¼å¼
        for key in batch.keys():
            if key.startswith("observation.images."):
                camera_name = key.replace("observation.images.", "")
                image_dict[camera_name] = jnp.array(batch[key])
                batch_size = batch[key].shape[0]
                # å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶ç¡®ä¿image_masksåªæœ‰batchç»´åº¦ [batch_size,]
                image_mask_dict[camera_name] = jnp.ones((batch_size,), dtype=jnp.bool_)
        
        # å¤‡ç”¨ï¼šç›´æ¥çš„"image"æ ¼å¼
        if not image_dict and "image" in batch:
            if isinstance(batch["image"], dict):
                for cam_name, cam_data in batch["image"].items():
                    image_dict[cam_name] = jnp.array(cam_data)
                    batch_size = cam_data.shape[0]
                    # å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶ç¡®ä¿image_masksåªæœ‰batchç»´åº¦ [batch_size,]
                    image_mask_dict[cam_name] = jnp.ones((batch_size,), dtype=jnp.bool_)
            else:
                # å‡è®¾æ˜¯å•ç›¸æœº
                image_dict["camera"] = jnp.array(batch["image"])
                batch_size = batch["image"].shape[0]
                # å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶ç¡®ä¿image_masksåªæœ‰batchç»´åº¦ [batch_size,]
                image_mask_dict["camera"] = jnp.ones((batch_size,), dtype=jnp.bool_)
        
        # å¦‚æœä»ç„¶æ²¡æœ‰å›¾åƒï¼Œåˆ›å»ºå ä½ç¬¦
        if not image_dict:
            batch_size = len(batch_samples)
            image_dict["placeholder"] = jnp.zeros((batch_size, 224, 224, 3), dtype=jnp.float32)
            # å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶ç¡®ä¿å ä½ç¬¦image_masksåªæœ‰batchç»´åº¦ [batch_size,]
            image_mask_dict["placeholder"] = jnp.ones((batch_size,), dtype=jnp.bool_)
        
        # æ„å»ºçŠ¶æ€
        state_data = None
        for state_key in ["observation.state", "state", "qpos"]:
            if state_key in batch:
                state_data = jnp.array(batch[state_key])
                break
        
        if state_data is None:
            batch_size = len(batch_samples)
            state_data = jnp.zeros((batch_size, 14), dtype=jnp.float32)  # å‡è®¾14ç»´çŠ¶æ€
        
        # OpenPI's AlohaInputs transform handles dimension padding automatically
        
        # æ„å»ºè§‚æµ‹å­—å…¸
        observation_dict = {
            "image": image_dict,
            "image_mask": image_mask_dict,
            "state": state_data,
        }
        
        # æ·»åŠ è¯­è¨€ç›¸å…³å­—æ®µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if "tokenized_prompt" in batch:
            observation_dict["tokenized_prompt"] = jnp.array(batch["tokenized_prompt"])
        if "tokenized_prompt_mask" in batch:
            observation_dict["tokenized_prompt_mask"] = jnp.array(batch["tokenized_prompt_mask"])
        
        # åˆ›å»ºÏ€â‚€ Observationå¯¹è±¡
        observation = _model.Observation.from_dict(observation_dict)
        
        return observation, actions
    
    def refresh_memory_pool(self, epoch_seed: int):
        """qc_ACTé£æ ¼ï¼šåˆ·æ–°å†…å­˜æ± åˆ°æ–°epoch"""
        
        old_epoch = self.current_epoch_seed
        self.current_epoch_seed = epoch_seed
        
        # é‡æ–°åŠ è½½å†…å­˜æ± 
        self._load_current_memory_pool()
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats['current_epoch'] = epoch_seed
        
        if self.rank == 0:
            logger.info(f"âœ“ å†…å­˜æ± å·²åˆ·æ–°: epoch {old_epoch} â†’ {epoch_seed}")
    
    def __getitem__(self, idx: int) -> Dict[str, jnp.ndarray]:
        """qc_ACTæ ¸å¿ƒï¼šå®Œå…¨å¿½ç•¥idxå‚æ•°ï¼Œè¿”å›Q-chunkingæ ¼å¼"""
        
        # å¿½ç•¥ä¼ å…¥çš„idxï¼Œç›´æ¥è°ƒç”¨sample_batchè·å–éšæœºæ‰¹æ¬¡
        batch_data = self.sample_batch()
        
        # è¿”å›æ‰¹æ¬¡ä¸­çš„ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼ˆä½†ä¿æŒæ‰¹æ¬¡ç»´åº¦ï¼‰
        single_sample = {}
        for key, value in batch_data.items():
            if isinstance(value, jnp.ndarray) and value.ndim > 0:
                single_sample[key] = value[0:1]  # ä¿æŒ[1, ...] å½¢çŠ¶
            else:
                single_sample[key] = value
        
        return single_sample
    
    def __len__(self) -> int:
        """è¿”å›å†…å­˜æ± ä¸­çš„transitionæ€»æ•°"""
        return self.total_pool_transitions
    
    def create_batch_iterator(self) -> Iterator[Dict[str, jnp.ndarray]]:
        """åˆ›å»ºæ— é™Q-chunking batchè¿­ä»£å™¨"""
        while True:
            yield self.sample_batch()
    
    def __iter__(self) -> Iterator[Dict[str, jnp.ndarray]]:
        """åˆ›å»º Q-chunking è¿­ä»£å™¨ï¼Œæ”¯æŒforå¾ªç¯"""
        return self.create_batch_iterator()
    
    def get_dataset_statistics(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            **self.stats,
            'total_episodes_in_dataset': len(self.all_episode_info),
            'episodes_in_memory_pool': len(self.memory_pool_episodes),
            'transitions_in_memory_pool': self.total_pool_transitions,
            'memory_pool_utilization': len(self.memory_pool_episodes) / self.episodes_per_memory_pool * 100,
            'model_action_dim': self.rl_config.model.action_dim,
            'qchunking_horizon': self.qchunking_config.horizon_length,
            'batch_size': self.batch_size,
            'config_name': self.rl_config.name
        }


# æ·»åŠ timeå¯¼å…¥
import time


def create_acrlpd_data_loader(
    rl_config: Any,  # RLTrainConfig
    batch_size: int = 128,
    episodes_per_memory_pool: int = 64,
    skip_norm_stats: bool = False,
    **kwargs
) -> ACRLPDDataLoader:
    """
    åˆ›å»ºç»Ÿä¸€çš„ACRLPDæ•°æ®åŠ è½½å™¨
    
    Args:
        rl_config: RLTrainConfigç»Ÿä¸€é…ç½®
        batch_size: æ‰¹æ¬¡å¤§å°
        episodes_per_memory_pool: å†…å­˜æ± å¤§å°
        skip_norm_stats: è·³è¿‡norm statsåŠ è½½ï¼ˆç”¨äºè®¡ç®—norm statsæ—¶ï¼‰
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        ACRLPDDataLoaderå®ä¾‹
    """
    
    return ACRLPDDataLoader(
        rl_config=rl_config,
        batch_size=batch_size,
        episodes_per_memory_pool=episodes_per_memory_pool,
        skip_norm_stats=skip_norm_stats,
        **kwargs
    )


def load_acrlpd_norm_stats(
    repo_id: str,
    norm_stats_dir: Optional[Path] = None
) -> Optional[at.PyTree[_normalize.NormStats]]:
    """
    åŠ è½½é¢„è®¡ç®—çš„å½’ä¸€åŒ–ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        repo_id: æ•°æ®é›†repository ID
        norm_stats_dir: è‡ªå®šä¹‰ç»Ÿè®¡ç›®å½•è·¯å¾„
        
    Returns:
        åŠ è½½çš„å½’ä¸€åŒ–ç»Ÿè®¡ä¿¡æ¯æˆ–None
    """
    if norm_stats_dir is None:
        dataset_name = Path(repo_id).name if "/" in repo_id else repo_id
        norm_stats_dir = Path(f"/tmp/acrlpd_norm_stats/{dataset_name}")
    
    if norm_stats_dir.exists():
        try:
            logger.info(f" Loading norm_stats from: {norm_stats_dir}")
            norm_stats = _normalize.load(norm_stats_dir)
            logger.info(f"âœ“ Successfully loaded norm_stats: {list(norm_stats.keys())}")
            return norm_stats
        except Exception as e:
            logger.warning(f"Failed to load norm_stats from {norm_stats_dir}: {e}")
            return None
    else:
        logger.warning(f"Norm_stats directory not found: {norm_stats_dir}")
        logger.info(f" Run compute_acrlpd_norm_stats.py first to generate norm_stats")
        return None


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import argparse
    
    parser = argparse.ArgumentParser(description="Test qc_ACT ACRLPDDataLoader")
    parser.add_argument("--repo-id", type=str, required=True, help="LeRobot dataset repo ID")
    parser.add_argument("--batch-size", type=int, default=8, help="Batch size")
    parser.add_argument("--episodes-per-memory-pool", type=int, default=4, help="Memory pool size")
    parser.add_argument("--num-batches", type=int, default=3, help="Number of test batches")
    
    args = parser.parse_args()
    
    try:
        # éœ€è¦ä½¿ç”¨RLTrainConfigï¼Œè¿™é‡Œéœ€è¦ä»configå¯¼å…¥
        print("acrlpd_data_converter è¯·ä½¿ç”¨æ–°çš„ç»Ÿä¸€é…ç½®ç³»ç»Ÿæµ‹è¯•:")
        print("   cd /dev/shm/lmc/openpi/ac_training")
        print("   /era-ai/conda_envs/openpi/bin/uv run python data/acrlpd_data_loader.py --config rl_aloha_fold")
        exit(0)
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        stats = data_loader.get_dataset_statistics()
        print(" Dataset Statistics:")
        for key, value in stats.items():
            print(f"  {key}: {value}")
        
        # æµ‹è¯•Q-chunking batché‡‡æ ·
        print(f"\n Testing {args.num_batches} Q-chunking batches:")
        for i in range(args.num_batches):
            batch_dict = data_loader.sample_batch()
            
            print(f"Batch {i}:")
            print(f"  Q-chunking format keys: {list(batch_dict.keys())}")
            
            # æ£€æŸ¥è§‚æµ‹
            if 'observations' in batch_dict:
                obs = batch_dict['observations']
                print(f"  Observations type: {type(obs)}")
                if hasattr(obs, 'state') and obs.state is not None:
                    print(f"    State shape: {obs.state.shape}")
                if hasattr(obs, 'image') and obs.image is not None:
                    for cam_name, cam_data in obs.image.items():
                        print(f"    {cam_name} shape: {cam_data.shape}")
            
            # æ£€æŸ¥Q-chunkingå…³é”®å­—æ®µ
            for key in ['actions', 'rewards', 'masks', 'valid', 'terminals']:
                if key in batch_dict:
                    print(f"  {key} shape: {batch_dict[key].shape}")
            
            for key in ['next_terminal', 'sequence_mask']:
                if key in batch_dict:
                    print(f"  {key} shape: {batch_dict[key].shape}")
        
        # æµ‹è¯•epochåˆ·æ–°
        print("\n Testing epoch refresh:")
        old_pool_size = data_loader.total_pool_transitions
        data_loader.refresh_memory_pool(1)
        new_pool_size = data_loader.total_pool_transitions
        print(f"Pool size before refresh: {old_pool_size}")
        print(f"Pool size after refresh: {new_pool_size}")
        
        print("\n Q-chunking RL ACRLPDDataLoader test completed successfully!")
        
    except Exception as e:
        print(f" Test failed: {e}")
        import traceback
        traceback.print_exc()