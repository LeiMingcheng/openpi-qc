"""
ACRLPDDataLoaderV2: åŸºäºOpenPIçš„é«˜æ•ˆRLæ•°æ®åŠ è½½å™¨

åœ¨OpenPIé«˜æ•ˆæ•°æ®åŠ è½½åŸºç¡€ä¸Šï¼Œæ·»åŠ Q-chunking RLè®­ç»ƒéœ€è¦çš„åŠŸèƒ½ï¼š
- ä½¿ç”¨OpenPIçš„LeRobotDataset + delta_timestampsæœºåˆ¶
- ä¿æŒOpenPIçš„transforms pipeline
- æ·»åŠ RLç‰¹æœ‰å­—æ®µï¼šreward, next_observations, masks, terminals  
- è¾“å‡ºæ ‡å‡†Q-chunkingæ ¼å¼ï¼Œä¸AC Trainingè®­ç»ƒå¾ªç¯å…¼å®¹

æ¶æ„ï¼š
LeRobotDataset â†’ OpenPI transforms â†’ RLQChunkingTransform â†’ Q-chunking output
"""

import logging
from typing import Dict, List, Any, Optional
import jax.numpy as jnp
import numpy as np
import torch

# OpenPI imports  
import openpi.training.data_loader as openpi_data_loader
import openpi.training.config as openpi_config
import openpi.transforms as _transforms

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(levelname)s:%(name)s:%(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)


# RLQChunkingTransformç±»å·²å®Œå…¨åˆ é™¤ - æ‰€æœ‰è½¬æ¢é€»è¾‘ç§»è‡³_collate_fnå®ç°é›¶å¤åˆ¶é«˜æ•ˆå¤„ç†


class ACRLPDDataLoaderV2:
    """åŸºäºOpenPIçš„é«˜æ•ˆRLæ•°æ®åŠ è½½å™¨"""
    
    def __init__(
        self,
        rl_config: Any,  # RLTrainConfig
        batch_size: int = 128,
        seed: int = 42,
        positive_batch_ratio: float = 0.1,  # ä¿ç•™å…¼å®¹æ€§ï¼Œä½†ä¸ä½¿ç”¨
        tolerance_s: float = 1e-4,
        debug_mode: bool = False
    ):
        """
        åˆå§‹åŒ–åŸºäºOpenPIçš„RLæ•°æ®åŠ è½½å™¨
        
        Args:
            rl_config: AC Trainingçš„RLTrainConfig  
            batch_size: æ‰¹æ¬¡å¤§å°
            seed: éšæœºç§å­
            positive_batch_ratio: (å…¼å®¹æ€§å‚æ•°ï¼Œä¸ä½¿ç”¨å¤æ‚åˆ†ç±»)
            tolerance_s: OpenPIæ—¶é—´æˆ³å®¹é”™
            debug_mode: è°ƒè¯•æ¨¡å¼
        """
        self.rl_config = rl_config
        self.batch_size = batch_size
        self.seed = seed
        self.debug_mode = debug_mode
        
        # ä»RL configæå–å…³é”®å‚æ•°
        self.repo_id = self._extract_repo_id(rl_config.data)
        self.action_horizon = rl_config.model.action_horizon
        self.action_dim = rl_config.qchunking.action_dim
        
        logger.info(f"=== ACRLPDDataLoaderV2 (åŸºäºOpenPI) ===")
        logger.info(f"Repo ID: {self.repo_id}")
        logger.info(f"Action Horizon: {self.action_horizon}")
        logger.info(f"Action Dim: {self.action_dim}")
        logger.info(f"Batch Size: {batch_size}")
        
        # 1. ä½¿ç”¨OpenPIæ ‡å‡†æ–¹å¼åˆ›å»ºæ•°æ®é›†
        self.openpi_dataset = self._create_openpi_dataset(tolerance_s)
        
        # 2. RLè½¬æ¢å™¨å·²åˆ é™¤ - ç›´æ¥åœ¨_collate_fnä¸­å®ç°é›¶å¤åˆ¶è½¬æ¢
        
        # 3. åˆ›å»ºPyTorch DataLoader (ä½¿ç”¨OpenPIæ–¹å¼)
        self.dataloader = self._create_pytorch_dataloader()
        
        logger.info(f"âœ… ACRLPDDataLoaderV2 åˆå§‹åŒ–å®Œæˆ (åŸºäºOpenPI)")
    
    def _extract_repo_id(self, data_config) -> str:
        """ä»data configä¸­æå–repo_id"""
        if hasattr(data_config, 'repo_id'):
            return data_config.repo_id
        else:
            raise ValueError("æ— æ³•ä»data configä¸­æå–repo_id")
    
    def _create_openpi_dataset(self, tolerance_s: float):
        """ä½¿ç”¨OpenPIæ ‡å‡†æ–¹å¼åˆ›å»ºæ•°æ®é›†"""
        
        logger.info("ä½¿ç”¨OpenPIæ ‡å‡†æ–¹å¼åˆ›å»ºtorchæ•°æ®é›†...")
        
        # 1. ä»DataConfigFactoryåˆ›å»ºå®é™…çš„DataConfigå®ä¾‹
        # LeRobotAlohaDataConfigæ˜¯factoryï¼Œéœ€è¦è°ƒç”¨create()ç”Ÿæˆå®é™…é…ç½®
        assets_dirs = self.rl_config.assets_dirs  # TrainConfigçš„assets_dirså±æ€§
        actual_data_config = self.rl_config.data.create(assets_dirs, self.rl_config.model)
        logger.info(f"âœ… DataConfigFactoryåˆ›å»ºå®é™…DataConfigæˆåŠŸï¼Œprompt_from_task={actual_data_config.prompt_from_task}")
        
        # 2. åˆ›å»ºåŸºç¡€æ•°æ®é›† - ä½¿ç”¨å®é™…DataConfig
        base_dataset = openpi_data_loader.create_torch_dataset(
            data_config=actual_data_config,  # ä½¿ç”¨factoryåˆ›å»ºçš„å®é™…DataConfig
            action_horizon=self.action_horizon,
            model_config=self.rl_config.model,
            tolerance_s=tolerance_s,
            skip_problematic_episodes=True
        )
        
        # 3. åº”ç”¨OpenPI transforms (repack + data + normalize + model)
        transformed_dataset = openpi_data_loader.transform_dataset(
            base_dataset, 
            actual_data_config,  # ä½¿ç”¨å®é™…DataConfigè¿›è¡Œtransform
            skip_norm_stats=False  # ä½¿ç”¨å½’ä¸€åŒ–ç»Ÿè®¡
        )
        
        logger.info(f"âœ… OpenPIæ•°æ®é›†åˆ›å»ºå¹¶å˜æ¢æˆåŠŸï¼Œæ•°æ®é›†é•¿åº¦: {len(transformed_dataset)}")
        return transformed_dataset
    
    def _create_pytorch_dataloader(self):
        """åˆ›å»ºPyTorch DataLoader"""
        
        # ä½¿ç”¨OpenPIæ–¹å¼åˆ›å»ºDataLoader
        dataloader = torch.utils.data.DataLoader(
            self.openpi_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=0,  # JAXç¯å¢ƒä¸‹é€šå¸¸è®¾ä¸º0
            drop_last=True
            # ğŸš€ æ€§èƒ½ä¿®å¤ï¼šä½¿ç”¨PyTorché»˜è®¤collate_fnï¼Œæ¢å¤OpenPIé«˜æ•ˆbatching
        )
        
        logger.info(f"âœ… PyTorch DataLoaderåˆ›å»ºæˆåŠŸ")
        return dataloader
    
    def _collate_fn_DISABLED_FOR_PERFORMANCE(self, batch):
        """
        è¶…é«˜æ•ˆcollateå‡½æ•°ï¼šé›¶å¤åˆ¶OpenPIâ†’Q-chunkingè½¬æ¢
        
        è¾“å…¥: OpenPIåŸå§‹æ ·æœ¬åˆ—è¡¨ [{image: {...}, state: [...], actions: [...], reward: float}]
        è¾“å‡º: Q-chunkingæ‰¹æ¬¡ {image: [B,...], actions: [B,H,A], rewards: [B,H], ...}
        """
        
        if len(batch) == 0:
            return {}
        
        batch_size = len(batch)
        
        # 1. é¢„è®¡ç®—RLå­—æ®µæ¨¡æ¿ï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
        masks_template = jnp.ones(self.action_horizon, dtype=jnp.float32)
        terminals_template = jnp.zeros(self.action_horizon, dtype=jnp.float32)
        
        collated = {}
        
        # æ•°å€¼æ£€æŸ¥å‡½æ•°ï¼šé˜²æ­¢å¼‚å¸¸æ•°æ®
        def _check_numerical_validity(name: str, data):
            """æ£€æŸ¥æ•°æ®çš„æ•°å€¼æœ‰æ•ˆæ€§ï¼Œé˜²æ­¢NaN/Inf/è¿‡å¤§æ•°å€¼"""
            if isinstance(data, (jnp.ndarray, np.ndarray)):
                if jnp.any(jnp.isnan(data)) or jnp.any(jnp.isinf(data)):
                    raise ValueError(f"å‘ç°NaNæˆ–Infå€¼åœ¨å­—æ®µ: {name}")
                if jnp.max(jnp.abs(data)) > 1e6:  # åˆç†çš„æ•°å€¼èŒƒå›´
                    logger.warning(f"å­—æ®µ {name} åŒ…å«è¿‡å¤§æ•°å€¼: max_abs={float(jnp.max(jnp.abs(data)))}")
            elif isinstance(data, dict):
                for sub_key, sub_data in data.items():
                    _check_numerical_validity(f"{name}.{sub_key}", sub_data)
        
        # 2. ğŸš€ NEW: å¤„ç†LeRobotæ—¶é—´åºåˆ—æ•°æ®æ ¼å¼
        # åŸºäºRepackTransformæ˜ å°„ï¼šobservation.* -> å¯¹åº”OpenPIå­—æ®µ
        processed_keys = set()
        
        
        
        for key in batch[0].keys():
            processed_keys.add(key)
            
            if key == 'reward':
                # RLæ•°æ®å¿…é¡»æœ‰rewardå­—æ®µï¼ŒAC Trainingä¸“ä¸ºRLè®¾è®¡
                reward_values = jnp.array([float(sample['reward']) for sample in batch], dtype=jnp.float32)
                
                # Q-chunkingå¥–åŠ±åˆ†å¸ƒï¼šåºåˆ—æœ«å°¾ç»™å¥–åŠ±ï¼Œä¸­é—´æ­¥éª¤ä¸º0ï¼ˆä¸åŸAC Trainingä¸€è‡´ï¼‰
                rewards = jnp.zeros((batch_size, self.action_horizon), dtype=jnp.float32)
                rewards = rewards.at[:, -1].set(reward_values)  # åªåœ¨åºåˆ—æœ€åä¸€æ­¥ç»™å¥–åŠ±
                collated['rewards'] = rewards
                
            elif key == 'state':
                # state: [batch_size, 2, state_dim] -> [current_state, next_state]
                state_values = [sample[key] for sample in batch]  # List[[2, state_dim]]
                collated['state'] = jnp.stack([s[0] for s in state_values], axis=0)      # [B, state_dim] - current
                collated['next_state'] = jnp.stack([s[1] for s in state_values], axis=0) # [B, state_dim] - next
                
            elif key == 'image':
                # images: {cam_name: [batch_size, 2, H, W, C]} -> {cam_name: [current_img, next_img]}
                images_values = [sample[key] for sample in batch]  # List[{cam: [2, H, W, C]}]
                
                collated['image'] = {}
                collated['next_image'] = {}
                
                # ğŸ”§ RLä¿®å¤: æ·»åŠ image_maskå¤„ç†ï¼Œç»´åº¦éœ€è¦åŒ¹é…æ—¶é—´åºåˆ—æ ¼å¼
                collated['image_mask'] = {}
                collated['next_image_mask'] = {}  # ğŸ”§ ä¿®å¤ï¼šåˆå§‹åŒ–next_image_maskå­—å…¸
                
                # è·å–æ‰€æœ‰ç›¸æœºåç§°ï¼ˆä»ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼‰
                cam_names = images_values[0].keys()
                # ğŸš€ ä¼˜åŒ–ï¼šå•æ¬¡å¾ªç¯å¤„ç†æ‰€æœ‰ç›¸æœºæ•°æ®å’Œmasks
                for cam_name in cam_names:
                    # æ¯ä¸ªç›¸æœº: [B, 2, H, W, C]
                    cam_data = [sample[key][cam_name] for sample in batch]
                    collated['image'][cam_name] = jnp.stack([img[0] for img in cam_data], axis=0)      # [B, H, W, C] - current  
                    collated['next_image'][cam_name] = jnp.stack([img[1] for img in cam_data], axis=0) # [B, H, W, C] - next
                    
                    # ğŸš€ ä¼˜åŒ–ï¼šå•æ¬¡åˆ›å»ºcurrentå’Œnextçš„image_mask
                    mask = jnp.ones(batch_size, dtype=jnp.bool_)  # [B] - æ ‡å‡†OpenPIæ ¼å¼
                    collated['image_mask'][cam_name] = mask
                    collated['next_image_mask'][cam_name] = mask  # å¤ç”¨same mask
                    
            elif key == 'actions':
                # actions: [batch_size, action_horizon, action_dim] - ä¿æŒåŸæœ‰å¤„ç†
                values = [sample[key] for sample in batch]
                collated[key] = jnp.stack(values, axis=0)
                
            else:
                # å…¶ä»–å­—æ®µï¼šæŒ‰åŸæœ‰é€»è¾‘å¤„ç†
                values = [sample[key] for sample in batch]
                if isinstance(values[0], dict):
                    collated[key] = {}
                    for sub_key in values[0].keys():
                        sub_values = [sample[key][sub_key] for sample in batch]
                        collated[key][sub_key] = jnp.stack(sub_values, axis=0)
                elif isinstance(values[0], (jnp.ndarray, np.ndarray)):
                    collated[key] = jnp.stack(values, axis=0)
                else:
                    collated[key] = values if len(set(str(v) for v in values)) > 1 else values[0]
        
        # 2.5. éªŒè¯RLå¿…éœ€å­—æ®µå­˜åœ¨
        if 'reward' not in processed_keys:
            logger.info(f"å½“å‰å­—æ®µï¼š{batch[0].keys()}")
            raise ValueError("RLæ•°æ®å¿…é¡»åŒ…å«rewardå­—æ®µï¼AC Trainingä¸“ä¸ºRLè®­ç»ƒè®¾è®¡ï¼Œä¸æ”¯æŒSFTæ•°æ®ã€‚")
        
        # 3. æ·»åŠ å›ºå®šRLå­—æ®µï¼ˆå‘é‡åŒ–æ“ä½œï¼‰
        collated['masks'] = jnp.tile(masks_template[None, :], (batch_size, 1))
        collated['terminals'] = jnp.tile(terminals_template[None, :], (batch_size, 1))
        
        # 5. æ•°å€¼æœ‰æ•ˆæ€§æ£€æŸ¥ï¼ˆé˜²æ­¢å¼‚å¸¸æ•°æ®ï¼‰
        try:
            for key, value in collated.items():
                _check_numerical_validity(key, value)
        except ValueError as e:
            # æ•°å€¼å¼‚å¸¸æ—¶æŠ›å‡ºå¼‚å¸¸ï¼Œè®©sample_batché‡æ–°é‡‡æ ·
            raise ValueError(f"Batchæ•°å€¼å¼‚å¸¸: {e}")
        
        return collated
    
    def sample_batch(self) -> Dict[str, jnp.ndarray]:
        """é‡‡æ ·ä¸€ä¸ªbatchï¼Œå…¼å®¹AC Trainingæ¥å£"""
        
         # ä»DataLoaderè·å–ä¸‹ä¸€ä¸ªbatch
        batch_iter = iter(self.dataloader)
        batch = next(batch_iter)
        
        return batch
    
    def __iter__(self):
        """è¿­ä»£å™¨æ¥å£"""
        return iter(self.dataloader)
    
    def __len__(self):
        """è¿”å›æ•°æ®é›†é•¿åº¦"""  
        return len(self.dataloader)


def create_acrlpd_data_loader_v2(
    rl_config: Any,
    batch_size: int = 128,
    seed: int = 42,
    tolerance_s: float = 1e-4,
    debug_mode: bool = False,
    **kwargs
) -> ACRLPDDataLoaderV2:
    """
    åˆ›å»ºACRLPDDataLoaderV2å®ä¾‹ (å…¼å®¹AC Trainingæ¥å£)
    
    Args:
        rl_config: RLTrainConfigç»Ÿä¸€é…ç½®
        batch_size: æ‰¹æ¬¡å¤§å°  
        seed: éšæœºç§å­
        tolerance_s: OpenPIæ—¶é—´æˆ³å®¹é”™
        debug_mode: è°ƒè¯•æ¨¡å¼
        **kwargs: å…¶ä»–å…¼å®¹æ€§å‚æ•°
        
    Returns:
        ACRLPDDataLoaderV2å®ä¾‹
    """
    
    return ACRLPDDataLoaderV2(
        rl_config=rl_config,
        batch_size=batch_size,
        seed=seed,
        tolerance_s=tolerance_s,
        debug_mode=debug_mode
    )