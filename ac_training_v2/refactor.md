# AC Training V2 ç³»ç»Ÿæ€§é‡æ„æ–¹æ¡ˆ

**åŸºäºå…¨é¢æ¶æ„åˆ†æçš„ç³»ç»Ÿé‡æ„è®¡åˆ’**

## ğŸš¨ ç°çŠ¶åˆ†æ

### æ¶æ„æ–­è£‚å±æœº
ac_training_v2ç›®å‰å­˜åœ¨ä¸¥é‡çš„æ¥å£æ–­è£‚å’Œæ¶æ„æ··ä¹±ï¼š

**âœ… agents_v2å·²é‡æ„å®Œæˆ**ï¼š
- åˆ é™¤ACRLPDPi0Configï¼Œç»Ÿä¸€ä½¿ç”¨config.py
- æŸå¤±è®¡ç®—é€»è¾‘å†…åŒ–åˆ°Agentç±»
- å‚æ•°åŒ–æ„é€ å‡½æ•°ï¼Œæ¸…æ™°çš„æ¥å£

**âŒ å…¶ä»–æ¨¡å—ä»ä¸ºv1å¤åˆ¶**ï¼š
- training_v2ä»å¯¼å…¥`agents.acrlpd_pi0_agent`ï¼ŒæœŸæœ›æ—§æ¥å£
- scriptså¯¼å…¥æ—§æ¨¡å—ï¼Œä¾èµ–å·²åˆ é™¤çš„ACRLPDPi0Config
- åŠŸèƒ½é‡å¤ï¼ŒèŒè´£è¾¹ç•Œæ¨¡ç³Š

### å…·ä½“é—®é¢˜
1. **æ¥å£ä¸åŒ¹é…**ï¼štraining_v2æœŸæœ›`ACRLPDPi0Config`ï¼Œä½†agents_v2å·²åˆ é™¤
2. **ä¾èµ–é”™ä¹±**ï¼šscriptså¯¼å…¥agentsè€Œéagents_v2
3. **åŠŸèƒ½é‡å¤**ï¼štraining_v2å’Œagents_v2å­˜åœ¨é‡å¤çš„æŸå¤±è®¡ç®—é€»è¾‘
4. **èŒè´£æ··ä¹±**ï¼šç¼ºä¹æ¸…æ™°çš„æ¨¡å—è¾¹ç•Œå®šä¹‰

## ğŸ¯ ç›®æ ‡æ¶æ„è®¾è®¡

### è®¾è®¡åŸåˆ™
1. **æ¸…æ™°èŒè´£åˆ†å·¥**ï¼šagentsè´Ÿè´£ç®—æ³•ï¼Œtrainingè´Ÿè´£åŸºç¡€è®¾æ–½
2. **ç®€åŒ–æ¥å£**ï¼šç»Ÿä¸€çš„å‚æ•°ä¼ é€’å’Œè°ƒç”¨æ–¹å¼
3. **æ¶ˆé™¤é‡å¤**ï¼šåˆ é™¤é‡å¤çš„é…ç½®ã€æŸå¤±è®¡ç®—ç­‰åŠŸèƒ½
4. **ç‹¬ç«‹æ€§**ï¼šä¸è€ƒè™‘å‘åå…¼å®¹ï¼Œå®Œå…¨ç‹¬ç«‹çš„ç³»ç»Ÿ

### ç›®æ ‡ç›®å½•ç»“æ„ä¸è¯¦ç»†åŠŸèƒ½å®šä¹‰

```
ac_training_v2/
â”œâ”€â”€ agents_v2/           # âœ… æ ¸å¿ƒç®—æ³•æ¨¡å—ï¼ˆå·²é‡æ„å®Œæˆï¼‰
â”‚   â”œâ”€â”€ acrlpd_pi0_agent.py      # Agentç±»ï¼Œå†…åŒ–æŸå¤±è®¡ç®—
â”‚   â”œâ”€â”€ critic_networks.py       # Criticç½‘ç»œå®šä¹‰
â”‚   â””â”€â”€ loss_functions.py        # ç®€åŒ–çš„å·¥å…·å‡½æ•°
â”œâ”€â”€ training_v2/         # ğŸ”„ è®­ç»ƒåŸºç¡€è®¾æ–½ï¼ˆéœ€è¦å®Œå…¨é‡æ„ï¼‰
â”‚   â”œâ”€â”€ trainer.py              # ç®€åŒ–è®­ç»ƒå™¨ï¼Œé€‚é…agents_v2
â”‚   â”œâ”€â”€ fsdp_support.py         # FSDPåˆ†å¸ƒå¼è®­ç»ƒ
â”‚   â””â”€â”€ checkpointing.py        # Checkpointç®¡ç†
â”œâ”€â”€ data_v2/            # ğŸ“¦ æ•°æ®å¤„ç†ï¼ˆä¿æŒç›¸å¯¹ç‹¬ç«‹ï¼‰
â”‚   â””â”€â”€ acrlpd_data_loader.py   # LeRobotâ†’OpenPIæ ¼å¼è½¬æ¢
â”œâ”€â”€ scripts/            # ğŸ”„ è®­ç»ƒè„šæœ¬ï¼ˆæ›´æ–°æ¥å£ï¼‰
â”‚   â””â”€â”€ train_acrlpd_v2.py      # ç»Ÿä¸€è®­ç»ƒå…¥å£
â”œâ”€â”€ config.py           # âœ… ç»Ÿä¸€é…ç½®ç³»ç»Ÿ
â””â”€â”€ utils/              # ğŸ› ï¸ å·¥å…·å‡½æ•°
    â”œâ”€â”€ memory_monitor.py       # GPUå†…å­˜ç›‘æ§å·¥å…·
    â””â”€â”€ pytree_checker.py       # PyTreeç»“æ„éªŒè¯
```

## ğŸ“‹ è¯¦ç»†æ¨¡å—åŠŸèƒ½å®šä¹‰

### agents_v2/acrlpd_pi0_agent.py âœ…å·²é‡æ„å®Œæˆ
**èŒè´£**ï¼šACRLPDç®—æ³•æ ¸å¿ƒé€»è¾‘ï¼ŒÏ€â‚€æ¨¡å‹é›†æˆï¼ŒæŸå¤±è®¡ç®—

#### æ ¸å¿ƒç±»å®šä¹‰
```python
class ACRLPDPi0Agent:
    """ACRLPD + Ï€â‚€ é›†æˆAgentï¼Œå·²å®Œæˆé‡æ„"""
    
    def __init__(
        self,
        # æ ¸å¿ƒæ¨¡å‹ç»„ä»¶
        pi0_model: Any,                     # Ï€â‚€æ‰©æ•£æ¨¡å‹
        critic_networks: CriticNetworks,    # Criticç½‘ç»œensemble
        
        # Q-learningæ ¸å¿ƒå‚æ•°ï¼ˆä»configä¸­è§£åŒ…ï¼‰
        num_action_samples: int,            # Best-of-Né‡‡æ ·æ•°é‡
        horizon_length: int,                # Q-chunkingåºåˆ—é•¿åº¦  
        real_action_dim: int,               # çœŸå®åŠ¨ä½œç»´åº¦
        discount: float = 0.99,             # æŠ˜æ‰£å› å­
        target_update_rate: float = 0.005,  # Targetç½‘ç»œæ›´æ–°ç‡
        
        # æŸå¤±æƒé‡
        bc_loss_weight: float = 0.1,        # BCæ­£åˆ™åŒ–æƒé‡
        
        # EMAé…ç½®
        use_ema: bool = True,               # å¯ç”¨EMA
        pi0_ema_decay: float = 0.999,       # Ï€â‚€ EMAè¡°å‡ç‡
        critic_ema_decay: float = 0.99,     # Critic EMAè¡°å‡ç‡
        
        # ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå¤–éƒ¨åˆ›å»ºä¼ å…¥ï¼‰
        pi0_opt_state: optax.OptState,      # Ï€â‚€ä¼˜åŒ–å™¨çŠ¶æ€
        critic_opt_state: optax.OptState,   # Criticä¼˜åŒ–å™¨çŠ¶æ€
        temp_opt_state: Optional[optax.OptState] = None,  # æ¸©åº¦ä¼˜åŒ–å™¨çŠ¶æ€
        
        # å…¶ä»–é…ç½®
        **kwargs
    ):
```

#### æ ¸å¿ƒæ–¹æ³•æ¥å£
```python
# === æŸå¤±è®¡ç®—æ–¹æ³•ï¼ˆå·²å†…åŒ–ï¼‰ ===
def compute_critic_loss(
    self, 
    batch: Dict[str, jnp.ndarray], 
    rng: jax.random.PRNGKey
) -> Tuple[jnp.ndarray, Dict[str, Any]]:
    """è®¡ç®—CriticæŸå¤±ï¼ˆQ-learning + Targetç½‘ç»œï¼‰"""

def compute_actor_loss(
    self, 
    batch: Dict[str, jnp.ndarray], 
    rng: jax.random.PRNGKey
) -> Tuple[jnp.ndarray, Dict[str, Any]]:
    """è®¡ç®—ActoræŸå¤±ï¼ˆAWR + Best-of-Né‡‡æ ·ï¼‰"""

def compute_bc_loss(
    self, 
    batch: Dict[str, jnp.ndarray], 
    rng: jax.random.PRNGKey
) -> Tuple[jnp.ndarray, Dict[str, Any]]:
    """è®¡ç®—Behavior Cloningæ­£åˆ™åŒ–æŸå¤±"""

def compute_loss(
    self, 
    batch: Dict[str, jnp.ndarray], 
    rng: jax.random.PRNGKey
) -> Tuple[jnp.ndarray, Dict[str, Any]]:
    """ç»Ÿä¸€æŸå¤±è®¡ç®—æ¥å£ï¼ˆç»„åˆæ‰€æœ‰æŸå¤±ï¼‰"""

# === è®­ç»ƒæ­¥éª¤æ–¹æ³• ===
def train_step(
    self, 
    batch: Dict[str, jnp.ndarray], 
    rng: jax.random.PRNGKey
) -> Tuple['ACRLPDPi0Agent', Dict[str, Any]]:
    """å•æ­¥è®­ç»ƒæ›´æ–°ï¼ˆåŒ…å«æ¢¯åº¦è®¡ç®—å’Œå‚æ•°æ›´æ–°ï¼‰"""

# === OpenPIå…¼å®¹æ€§æ–¹æ³• ===
def create_train_state(self) -> openpi_training.TrainState:
    """åˆ›å»ºOpenPIå…¼å®¹çš„è®­ç»ƒçŠ¶æ€"""

def create_openpi_train_state(self) -> openpi_training.TrainState:
    """åˆ›å»ºä»…åŒ…å«Ï€â‚€çš„OpenPIæ ¼å¼checkpoint"""

def to_train_state(self) -> openpi_training.TrainState:
    """è½¬æ¢ä¸ºOpenPI TrainStateæ ¼å¼"""

# === çŠ¶æ€ç®¡ç†æ–¹æ³• ===
def update_target_networks(self) -> 'ACRLPDPi0Agent':
    """æ›´æ–°Targetç½‘ç»œï¼ˆè½¯æ›´æ–°ï¼‰"""

def update_ema_parameters(self) -> 'ACRLPDPi0Agent':
    """æ›´æ–°EMAå‚æ•°"""
```

#### å·¥å‚å‡½æ•°æ¥å£
```python
def create_acrlpd_pi0_agent_from_rl_config(
    rl_config: RLTrainConfig,
    rng: jax.random.PRNGKey,
    pi0_opt_state: Optional[optax.OptState] = None,
    critic_opt_state: Optional[optax.OptState] = None,
    temp_opt_state: Optional[optax.OptState] = None
) -> ACRLPDPi0Agent:
    """ä»RLTrainConfigåˆ›å»ºAgentå®ä¾‹ï¼ˆç»Ÿä¸€å…¥å£ï¼‰"""

def create_acrlpd_pi0_agent(
    pi0_model: Any,
    critic_networks: CriticNetworks,
    **kwargs
) -> ACRLPDPi0Agent:
    """ç›´æ¥å‚æ•°åˆ›å»ºAgentå®ä¾‹"""
```

### training_v2/trainer.py ğŸ”„éœ€è¦å®Œå…¨é‡æ„
**èŒè´£**ï¼šè®­ç»ƒå¾ªç¯ç®¡ç†ï¼ŒFSDPé›†æˆï¼Œæ€§èƒ½ç›‘æ§

#### æ ¸å¿ƒç±»å®šä¹‰
```python
class ACRLPDTrainer:
    """ç®€åŒ–çš„ACRLPDè®­ç»ƒå™¨ï¼Œé€‚é…agents_v2æ¥å£"""
    
    def __init__(
        self,
        agent: ACRLPDPi0Agent,              # agents_v2çš„Agentå®ä¾‹
        dataloader: ACRLPDDataLoader,       # æ•°æ®åŠ è½½å™¨
        rl_config: RLTrainConfig,           # ç»Ÿä¸€é…ç½®
        training_config: ACRLPDTrainingConfig,  # è®­ç»ƒä¸“ç”¨é…ç½®
        eval_fn: Optional[Callable] = None, # è¯„ä¼°å‡½æ•°
        
        # FSDPç›¸å…³å‚æ•°
        mesh: Optional[jax.sharding.Mesh] = None,
        data_sharding: Optional[jax.sharding.Sharding] = None,
        replicated_sharding: Optional[jax.sharding.Sharding] = None,
        
        # å…¨å±€ä¼˜åŒ–å™¨ï¼ˆè§£å†³pytreeä¸€è‡´æ€§ï¼‰
        global_pi0_tx: Optional[optax.GradientTransformation] = None,
        global_critic_tx: Optional[optax.GradientTransformation] = None
    ):
```

#### æ ¸å¿ƒæ–¹æ³•æ¥å£
```python
# === è®­ç»ƒå¾ªç¯æ–¹æ³• ===
def train(self) -> ACRLPDPi0Agent:
    """ä¸»è®­ç»ƒå¾ªç¯ï¼Œè¿”å›è®­ç»ƒå®Œæˆçš„Agent"""

def train_epoch(
    self, 
    epoch: int, 
    num_steps: int
) -> Dict[str, float]:
    """å•ä¸ªepochçš„è®­ç»ƒå¾ªç¯"""

def train_step_wrapper(
    self,
    agent: ACRLPDPi0Agent,
    batch: Dict[str, jnp.ndarray],
    rng: jax.random.PRNGKey
) -> Tuple[ACRLPDPi0Agent, Dict[str, Any]]:
    """è®­ç»ƒæ­¥éª¤åŒ…è£…å™¨ï¼ˆç”¨äºJITç¼–è¯‘ï¼‰"""

# === FSDPæ”¯æŒæ–¹æ³• ===
def setup_fsdp(
    self,
    mesh: jax.sharding.Mesh,
    data_sharding: jax.sharding.Sharding
) -> None:
    """è®¾ç½®FSDPåˆ†å¸ƒå¼è®­ç»ƒ"""

def create_fsdp_train_step(self) -> Callable:
    """åˆ›å»ºFSDPå…¼å®¹çš„è®­ç»ƒæ­¥éª¤å‡½æ•°"""

# === Checkpointæ–¹æ³• ===
def save_checkpoint(
    self, 
    step: int, 
    agent: ACRLPDPi0Agent,
    checkpoint_dir: str
) -> None:
    """ä¿å­˜è®­ç»ƒcheckpoint"""

def save_openpi_checkpoint(
    self, 
    step: int, 
    agent: ACRLPDPi0Agent,
    checkpoint_dir: str
) -> None:
    """ä¿å­˜Ï€â‚€ OpenPIæ ¼å¼checkpoint"""

def load_checkpoint(
    self, 
    checkpoint_path: str
) -> ACRLPDPi0Agent:
    """åŠ è½½è®­ç»ƒcheckpoint"""

# === è¯„ä¼°æ–¹æ³• ===
def evaluate(
    self, 
    agent: ACRLPDPi0Agent, 
    eval_steps: int = 100
) -> Dict[str, float]:
    """æ¨¡å‹è¯„ä¼°"""

# === ç›‘æ§å’Œæ—¥å¿—æ–¹æ³• ===
def log_training_metrics(
    self, 
    step: int, 
    metrics: Dict[str, Any]
) -> None:
    """è®°å½•è®­ç»ƒæŒ‡æ ‡"""

def log_memory_usage(
    self, 
    step: int, 
    phase: str = "training"
) -> None:
    """è®°å½•GPUå†…å­˜ä½¿ç”¨"""
```

### training_v2/fsdp_support.py ğŸ”„éœ€è¦åˆ›å»º
**èŒè´£**ï¼šFSDPåˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒï¼Œå†…å­˜ä¼˜åŒ–

#### æ ¸å¿ƒå‡½æ•°æ¥å£
```python
def create_fsdp_mesh(
    fsdp_devices: int,
    data_parallel_devices: Optional[int] = None
) -> jax.sharding.Mesh:
    """åˆ›å»ºFSDPè®­ç»ƒmesh"""

def create_fsdp_sharding_strategy(
    mesh: jax.sharding.Mesh,
    model_config: Any
) -> Tuple[jax.sharding.Sharding, jax.sharding.Sharding]:
    """åˆ›å»ºFSDPåˆ†ç‰‡ç­–ç•¥ï¼ˆæ•°æ®åˆ†ç‰‡ + å¤åˆ¶åˆ†ç‰‡ï¼‰"""

def init_fsdp_train_state(
    rl_config: RLTrainConfig,
    mesh: jax.sharding.Mesh,
    rng: jax.random.PRNGKey,
    global_optimizers: Dict[str, optax.GradientTransformation]
) -> Tuple[ACRLPDPi0Agent, jax.sharding.Sharding]:
    """åˆå§‹åŒ–FSDPè®­ç»ƒçŠ¶æ€"""

def create_fsdp_train_step_fn(
    agent: ACRLPDPi0Agent,
    mesh: jax.sharding.Mesh,
    data_sharding: jax.sharding.Sharding
) -> Callable:
    """åˆ›å»ºFSDPä¼˜åŒ–çš„è®­ç»ƒæ­¥éª¤å‡½æ•°"""

def validate_fsdp_sharding(
    train_state: Any,
    mesh: jax.sharding.Mesh
) -> bool:
    """éªŒè¯FSDPåˆ†ç‰‡æ˜¯å¦æ­£ç¡®å·¥ä½œ"""

def optimize_fsdp_memory(
    batch_size: int,
    model_config: Any,
    available_devices: int
) -> Dict[str, int]:
    """FSDPå†…å­˜ä¼˜åŒ–å»ºè®®"""
```

### training_v2/checkpointing.py ğŸ”„éœ€è¦é‡æ„
**èŒè´£**ï¼šCheckpointä¿å­˜å’ŒåŠ è½½ï¼ŒOpenPIæ ¼å¼å…¼å®¹

#### æ ¸å¿ƒå‡½æ•°æ¥å£
```python
def save_acrlpd_checkpoint(
    agent: ACRLPDPi0Agent,
    step: int,
    checkpoint_dir: str,
    save_openpi: bool = True,
    save_full: bool = True
) -> Dict[str, str]:
    """ä¿å­˜ACRLPD checkpointï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰"""

def save_openpi_pi0_checkpoint(
    agent: ACRLPDPi0Agent,
    checkpoint_path: str
) -> str:
    """ä¿å­˜Ï€â‚€æ¨¡å‹çš„OpenPIå…¼å®¹checkpoint"""

def load_acrlpd_checkpoint(
    checkpoint_path: str,
    rl_config: RLTrainConfig,
    rng: jax.random.PRNGKey
) -> Tuple[ACRLPDPi0Agent, int]:
    """åŠ è½½ACRLPD checkpoint"""

def load_pi0_weights_from_openpi(
    openpi_checkpoint_path: str,
    target_agent: ACRLPDPi0Agent
) -> ACRLPDPi0Agent:
    """ä»OpenPI checkpointåŠ è½½Ï€â‚€æƒé‡"""

def create_checkpoint_metadata(
    rl_config: RLTrainConfig,
    training_metrics: Dict[str, Any],
    step: int
) -> Dict[str, Any]:
    """åˆ›å»ºcheckpointå…ƒæ•°æ®"""

def validate_checkpoint_compatibility(
    checkpoint_path: str,
    expected_config: RLTrainConfig
) -> bool:
    """éªŒè¯checkpointä¸é…ç½®çš„å…¼å®¹æ€§"""
```

### data_v2/acrlpd_data_loader.py âœ…å·²å­˜åœ¨ï¼Œéœ€è¦æ¥å£ä¼˜åŒ–
**èŒè´£**ï¼šLeRobotâ†’OpenPIæ•°æ®æ ¼å¼è½¬æ¢ï¼ŒQ-chunkingæ•°æ®ç”Ÿæˆ

#### æ ¸å¿ƒç±»æ¥å£ï¼ˆå·²å®ç°ï¼Œéœ€è¦æ¥å£æ ‡å‡†åŒ–ï¼‰
```python
class ACRLPDDataLoader:
    """Q-chunking RLæ•°æ®åŠ è½½å™¨ï¼Œç»Ÿä¸€æ¥å£ç‰ˆæœ¬"""
    
    def __init__(
        self,
        rl_config: RLTrainConfig,           # ç»Ÿä¸€é…ç½®æ¥å£
        batch_size: int = 128,
        episodes_per_memory_pool: int = 64,
        skip_norm_stats: bool = False,
        device_sharding: Optional[jax.sharding.Sharding] = None,
        **kwargs
    ):
    
    # === Q-chunkingæ•°æ®ç”Ÿæˆ ===
    def sample_batch(self) -> Dict[str, jnp.ndarray]:
        """ç”ŸæˆQ-chunkingæ ¼å¼batch"""
    
    def refresh_memory_pool(self, epoch_seed: int) -> None:
        """åˆ·æ–°å†…å­˜æ± ï¼ˆæ–°epochï¼‰"""
    
    def get_dataset_statistics(self) -> Dict[str, Any]:
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""

# === å·¥å‚å‡½æ•° ===
def create_acrlpd_data_loader(
    rl_config: RLTrainConfig,
    batch_size: int = 128,
    episodes_per_memory_pool: int = 64,
    skip_norm_stats: bool = False,
    **kwargs
) -> ACRLPDDataLoader:
    """åˆ›å»ºç»Ÿä¸€çš„ACRLPDæ•°æ®åŠ è½½å™¨"""
```

### scripts/train_acrlpd_v2.py ğŸ”„éœ€è¦é‡æ„
**èŒè´£**ï¼šè®­ç»ƒè„šæœ¬å…¥å£ï¼Œå‘½ä»¤è¡Œæ¥å£ï¼Œå®éªŒç®¡ç†

#### æ ¸å¿ƒå‡½æ•°æ¥å£
```python
def create_argument_parser() -> argparse.ArgumentParser:
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""

def load_rl_config(args: argparse.Namespace) -> RLTrainConfig:
    """ä»å‘½ä»¤è¡Œå‚æ•°åŠ è½½RLTrainConfig"""

def setup_jax_environment(
    rl_config: RLTrainConfig,
    args: argparse.Namespace
) -> Tuple[jax.sharding.Mesh, jax.sharding.Sharding, jax.sharding.Sharding]:
    """è®¾ç½®JAXç¯å¢ƒå’ŒFSDPé…ç½®"""

def create_training_components(
    rl_config: RLTrainConfig,
    mesh: jax.sharding.Mesh,
    data_sharding: jax.sharding.Sharding,
    rng: jax.random.PRNGKey
) -> Tuple[ACRLPDPi0Agent, ACRLPDDataLoader, ACRLPDTrainer]:
    """åˆ›å»ºè®­ç»ƒç»„ä»¶ï¼ˆAgent, DataLoader, Trainerï¼‰"""

def run_training_experiment(
    trainer: ACRLPDTrainer,
    rl_config: RLTrainConfig,
    args: argparse.Namespace
) -> ACRLPDPi0Agent:
    """è¿è¡Œå®Œæ•´è®­ç»ƒå®éªŒ"""

def main() -> None:
    """ä¸»å…¥å£å‡½æ•°"""

# === è¾…åŠ©å‡½æ•° ===
def log_gpu_memory(step_name: str) -> Dict[str, float]:
    """GPUå†…å­˜ä½¿ç”¨ç›‘æ§"""

def verify_fsdp_sharding(train_state: Any, step_name: str) -> None:
    """éªŒè¯FSDPåˆ†ç‰‡æ•ˆæœ"""

def analyze_fsdp_effectiveness(usage_data: List[float]) -> None:
    """åˆ†æFSDPåˆ†ç‰‡æ•ˆæœ"""
```

### config.py âœ…å·²å®Œæˆ
**èŒè´£**ï¼šç»Ÿä¸€é…ç½®ç³»ç»Ÿï¼Œé¢„å®šä¹‰é…ç½®

#### æ ¸å¿ƒæ¥å£ï¼ˆå·²å®ç°ï¼‰
```python
@dataclasses.dataclass(frozen=True)
class RLTrainConfig(openpi_config.TrainConfig):
    """ç»Ÿä¸€çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒé…ç½®"""
    acrlpd: ACRLPDHyperparams
    qchunking: QChunkingConfig
    # ... å…¶ä»–å­—æ®µ

def get_config(config_name: str) -> RLTrainConfig:
    """è·å–é¢„å®šä¹‰é…ç½®"""

def list_configs() -> Dict[str, str]:
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨é…ç½®"""

def cli() -> RLTrainConfig:
    """å‘½ä»¤è¡Œé…ç½®é€‰æ‹©æ¥å£"""
```

### utils/memory_monitor.py âœ…å·²å­˜åœ¨
**èŒè´£**ï¼šGPUå†…å­˜ç›‘æ§å’Œåˆ†æ

#### æ ¸å¿ƒæ¥å£ï¼ˆå·²å®ç°ï¼‰
```python
class GPUMemoryMonitor:
    """GPUå†…å­˜ä½¿ç”¨ç›‘æ§å™¨"""
    
    def checkpoint_memory(
        self, 
        name: str, 
        train_state: Optional[Any] = None
    ) -> None:
        """åˆ›å»ºå†…å­˜ä½¿ç”¨æ£€æŸ¥ç‚¹"""
    
    def analyze_train_state_memory(
        self, 
        train_state: Any
    ) -> Dict[str, Dict[str, float]]:
        """åˆ†æè®­ç»ƒçŠ¶æ€çš„è¯¦ç»†å†…å­˜ç»„æˆ"""
    
    def get_memory_report(self) -> str:
        """ç”Ÿæˆè¯¦ç»†çš„å†…å­˜ä½¿ç”¨æŠ¥å‘Š"""

# === å…¨å±€å‡½æ•° ===
def log_memory_usage(
    step: int, 
    train_state: Any = None, 
    phase: str = "training"
) -> None:
    """ä¾¿æ·çš„å†…å­˜ä½¿ç”¨è®°å½•å‡½æ•°"""

def enable_memory_monitoring() -> None:
    """å¯ç”¨å†…å­˜ç›‘æ§ç³»ç»Ÿ"""
```

### utils/pytree_checker.py ğŸ”„éœ€è¦åˆ›å»º
**èŒè´£**ï¼šPyTreeç»“æ„éªŒè¯ï¼ŒFSDPå…¼å®¹æ€§æ£€æŸ¥

#### æ ¸å¿ƒå‡½æ•°æ¥å£
```python
def diagnose_pytree_structure(
    pytree: Any, 
    name: str = "pytree"
) -> Dict[str, Any]:
    """è¯Šæ–­PyTreeç»“æ„"""

def check_optimizer_consistency(
    opt1: optax.GradientTransformation,
    opt2: optax.GradientTransformation,
    name1: str = "opt1",
    name2: str = "opt2"
) -> bool:
    """æ£€æŸ¥ä¼˜åŒ–å™¨ä¸€è‡´æ€§"""

def validate_fsdp_compatibility(
    train_state: Any,
    mesh: jax.sharding.Mesh,
    data_sharding: jax.sharding.Sharding
) -> bool:
    """éªŒè¯FSDPå…¼å®¹æ€§"""

def check_pytree_leaf_dtypes(
    pytree: Any,
    expected_dtype: jnp.dtype = jnp.bfloat16
) -> Dict[str, List[str]]:
    """æ£€æŸ¥PyTreeå¶å­èŠ‚ç‚¹æ•°æ®ç±»å‹"""

def compare_pytree_structures(
    tree1: Any,
    tree2: Any,
    name1: str = "tree1", 
    name2: str = "tree2"
) -> Dict[str, Any]:
    """æ¯”è¾ƒä¸¤ä¸ªPyTreeçš„ç»“æ„å·®å¼‚"""
```

## ğŸ“‹ é‡æ„é˜¶æ®µè§„åˆ’

### Stage 1: Training-Agentsæ¥å£ç»Ÿä¸€ (High Priority)
**ç›®æ ‡**ï¼šä½¿training_v2å®Œå…¨é€‚é…agents_v2çš„æ–°æ¶æ„

- **1.1 æ›´æ–°training_v2å¯¼å…¥å’Œæ¥å£**
  - ä¿®æ”¹training_v2å¯¼å…¥agents_v2è€Œéagents
  - é€‚é…agents_v2çš„å‚æ•°åŒ–æ„é€ å‡½æ•°
  - è°ƒç”¨agents_v2çš„å†…åŒ–æŸå¤±è®¡ç®—æ–¹æ³•

- **1.2 ç®€åŒ–trainingèŒè´£**
  - åˆ é™¤training_v2ä¸­é‡å¤çš„æŸå¤±è®¡ç®—é€»è¾‘
  - ä¸“æ³¨äºè®­ç»ƒå¾ªç¯ã€FSDPæ”¯æŒã€checkpointç®¡ç†
  - é€šè¿‡ç®€å•æ¥å£è°ƒç”¨agents_v2æ–¹æ³•

### Stage 2: Checkpointæœºåˆ¶ç»Ÿä¸€ (Medium Priority)
**ç›®æ ‡**ï¼šå¤ç”¨ç°æœ‰çš„OpenPIå…¼å®¹checkpointæœºåˆ¶

- **2.1 åˆ†æç°æœ‰æœºåˆ¶**
  - agentsä¸­çš„`create_train_state()`å·²æ”¯æŒOpenPIæ ¼å¼
  - æ— éœ€é‡æ–°è®¾è®¡ï¼Œç›´æ¥ç§»æ¤åˆ°agents_v2

- **2.2 ç®€åŒ–checkpointä¿å­˜**
  - åˆ é™¤å¤æ‚çš„å¤šæ ¼å¼æ”¯æŒ
  - ä¸“æ³¨Ï€â‚€ OpenPIæ ¼å¼å’Œå®Œæ•´ACRLPDæ ¼å¼

### Stage 3: Scriptsæ¥å£æ›´æ–° (Medium Priority)
**ç›®æ ‡**ï¼šæ›´æ–°è®­ç»ƒè„šæœ¬ä½¿ç”¨æ–°çš„æ¥å£

- **3.1 æ›´æ–°importså’Œå·¥å‚å‡½æ•°è°ƒç”¨**
- **3.2 é€‚é…æ–°çš„å‚æ•°ä¼ é€’æ–¹å¼**
- **3.3 ç»Ÿä¸€é…ç½®ç³»ç»Ÿä½¿ç”¨**

### Stage 4: æ¶æ„æ¸…ç†ä¼˜åŒ– (Low Priority)
**ç›®æ ‡**ï¼šæœ€ç»ˆæ¸…ç†å’Œä¼˜åŒ–

- **4.1 åˆ é™¤æ®‹ç•™çš„é‡å¤ä»£ç **
- **4.2 ä¼˜åŒ–importä¾èµ–å…³ç³»**
- **4.3 æ–‡æ¡£å’Œæµ‹è¯•å®Œå–„**

## ğŸ”§ æ¥å£è®¾è®¡è§„èŒƒå’Œæ•°æ®æµ

### æ ¸å¿ƒæ•°æ®æ ¼å¼æ ‡å‡†

#### Q-chunking Batchæ ¼å¼
```python
# ACRLPDDataLoader.sample_batch() è¾“å‡ºæ ¼å¼
batch: Dict[str, jnp.ndarray] = {
    # Ï€â‚€è§‚æµ‹æ ¼å¼ï¼ˆæ”¯æŒå¤šç›¸æœºï¼‰
    'observations': _model.Observation = {
        'image': Dict[str, jnp.ndarray],      # {"cam_high": [B,224,224,3], ...}
        'image_mask': Dict[str, jnp.ndarray], # {"cam_high": [B,], ...}
        'state': jnp.ndarray,                 # [B, state_dim]
        'tokenized_prompt': jnp.ndarray,      # [B, prompt_len]
        'tokenized_prompt_mask': jnp.ndarray, # [B, prompt_len]
    },
    'next_observations': _model.Observation,  # åŒä¸Šæ ¼å¼ï¼Œç”¨äºbootstrap
    
    # Q-chunkingåŠ¨ä½œåºåˆ—
    'actions': jnp.ndarray,          # [B, H, action_dim] - Hæ­¥åŠ¨ä½œåºåˆ—
    'rewards': jnp.ndarray,          # [B, H] - å¥–åŠ±åºåˆ—
    'masks': jnp.ndarray,           # [B, H] - Bootstrap mask
    'valid': jnp.ndarray,           # [B, H] - åŠ¨ä½œæœ‰æ•ˆæ€§
    'terminals': jnp.ndarray,       # [B, H] - æ­¥éª¤ç»ˆæ­¢æ ‡å¿—
    'next_terminal': jnp.ndarray,   # [B] - ä¸‹ä¸€çŠ¶æ€terminalæ ‡å¿—
    'sequence_mask': jnp.ndarray,   # [B] - åºåˆ—æœ‰æ•ˆæ€§
}
```

#### Agentè®­ç»ƒçŠ¶æ€æ ¼å¼
```python
# ACRLPDPi0Agentå†…éƒ¨çŠ¶æ€ç»“æ„
agent_state = {
    # æ¨¡å‹å‚æ•°
    'pi0_params': PyTree,           # Ï€â‚€æ¨¡å‹å‚æ•°
    'critic_params': PyTree,        # Criticç½‘ç»œå‚æ•°
    'target_critic_params': PyTree, # Target Criticå‚æ•°
    
    # EMAå‚æ•°
    'pi0_ema_params': PyTree,       # Ï€â‚€ EMAå‚æ•°
    'critic_ema_params': PyTree,    # Critic EMAå‚æ•°
    
    # ä¼˜åŒ–å™¨çŠ¶æ€
    'pi0_opt_state': optax.OptState,    # Ï€â‚€ä¼˜åŒ–å™¨çŠ¶æ€
    'critic_opt_state': optax.OptState, # Criticä¼˜åŒ–å™¨çŠ¶æ€
    'temp_opt_state': optax.OptState,   # æ¸©åº¦ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
    
    # è®­ç»ƒè®¡æ•°å™¨
    'step': int,                    # è®­ç»ƒæ­¥æ•°
    'epoch': int,                   # å½“å‰epoch
}
```

### agents_v2 â†’ training_v2 æ¥å£å¥‘çº¦

#### åˆå§‹åŒ–æ¥å£
```python
# training_v2ä¸­çš„Agentåˆ›å»ºæ¨¡å¼
agent = create_acrlpd_pi0_agent_from_rl_config(
    rl_config=rl_config,           # RLTrainConfigç»Ÿä¸€é…ç½®
    rng=rng,                       # JAXéšæœºçŠ¶æ€
    pi0_opt_state=None,            # å¯é€‰ï¼šé¢„å…ˆåˆ›å»ºçš„ä¼˜åŒ–å™¨çŠ¶æ€
    critic_opt_state=None,         # å¯é€‰ï¼šé¢„å…ˆåˆ›å»ºçš„ä¼˜åŒ–å™¨çŠ¶æ€
    temp_opt_state=None            # å¯é€‰ï¼šæ¸©åº¦ä¼˜åŒ–å™¨çŠ¶æ€
) -> ACRLPDPi0Agent
```

#### è®­ç»ƒå¾ªç¯æ¥å£
```python
# å•æ­¥è®­ç»ƒçš„æ ‡å‡†è°ƒç”¨æ¨¡å¼
def standard_train_step(
    agent: ACRLPDPi0Agent,
    batch: Dict[str, jnp.ndarray],
    rng: jax.random.PRNGKey
) -> Tuple[ACRLPDPi0Agent, Dict[str, Any]]:
    """æ ‡å‡†è®­ç»ƒæ­¥éª¤æ¥å£"""
    
    # 1. è®¡ç®—æŸå¤±ï¼ˆå·²å†…åŒ–åˆ°Agentï¼‰
    total_loss, loss_info = agent.compute_loss(batch, rng)
    
    # 2. æ‰§è¡Œè®­ç»ƒæ­¥éª¤
    updated_agent, train_info = agent.train_step(batch, rng)
    
    # 3. åˆå¹¶ä¿¡æ¯
    train_info.update(loss_info)
    
    return updated_agent, train_info

# FSDPè®­ç»ƒæ­¥éª¤çš„è°ƒç”¨æ¨¡å¼
@jax.jit
def fsdp_train_step(
    agent: ACRLPDPi0Agent,
    batch: Dict[str, jnp.ndarray],
    rng: jax.random.PRNGKey
) -> Tuple[ACRLPDPi0Agent, Dict[str, Any]]:
    """FSDPä¼˜åŒ–çš„è®­ç»ƒæ­¥éª¤"""
    return agent.train_step(batch, rng)
```

#### Checkpointæ¥å£
```python
# OpenPIå…¼å®¹checkpointä¿å­˜
def save_checkpoint_interface(
    agent: ACRLPDPi0Agent,
    step: int,
    checkpoint_dir: str
) -> Dict[str, str]:
    """Checkpointä¿å­˜æ¥å£"""
    
    # 1. ä¿å­˜å®Œæ•´ACRLPDçŠ¶æ€
    full_path = f"{checkpoint_dir}/acrlpd_full_step_{step}.pkl"
    save_acrlpd_checkpoint(agent, step, checkpoint_dir, save_full=True)
    
    # 2. ä¿å­˜Ï€â‚€ OpenPIæ ¼å¼
    openpi_path = f"{checkpoint_dir}/pi0_openpi_step_{step}"
    openpi_state = agent.create_openpi_train_state()  # åªåŒ…å«Ï€â‚€
    save_openpi_pi0_checkpoint(agent, openpi_path)
    
    return {
        'full_checkpoint': full_path,
        'openpi_checkpoint': openpi_path
    }
```

### training_v2 â†’ data_v2 æ¥å£å¥‘çº¦

#### æ•°æ®åŠ è½½æ¥å£
```python
# ç»Ÿä¸€æ•°æ®åŠ è½½å™¨åˆ›å»º
dataloader = create_acrlpd_data_loader(
    rl_config=rl_config,                    # åŒ…å«æ•°æ®é…ç½®
    batch_size=rl_config.batch_size,        # æ‰¹æ¬¡å¤§å°
    episodes_per_memory_pool=64,            # å†…å­˜æ± å¤§å°
    device_sharding=data_sharding           # FSDPæ•°æ®åˆ†ç‰‡
) -> ACRLPDDataLoader

# æ‰¹æ¬¡é‡‡æ ·æ¥å£
batch = dataloader.sample_batch()  # -> Dict[str, jnp.ndarray]

# Epochç®¡ç†æ¥å£
dataloader.refresh_memory_pool(epoch_seed)  # æ–°epochåˆ·æ–°æ•°æ®
```

### scripts â†’ å…¨ç³»ç»Ÿ æ¥å£å¥‘çº¦

#### ç»Ÿä¸€è®­ç»ƒå…¥å£
```python
def unified_training_interface(
    config_name: str,
    exp_name: str,
    **training_args
) -> ACRLPDPi0Agent:
    """ç»Ÿä¸€è®­ç»ƒæ¥å£"""
    
    # 1. åŠ è½½é…ç½®
    rl_config = get_config(config_name)
    rl_config = customize_config(rl_config, **training_args)
    
    # 2. è®¾ç½®ç¯å¢ƒ
    mesh, data_sharding, replicated_sharding = setup_jax_environment(rl_config)
    
    # 3. åˆ›å»ºç»„ä»¶
    agent, dataloader, trainer = create_training_components(
        rl_config, mesh, data_sharding, rng
    )
    
    # 4. æ‰§è¡Œè®­ç»ƒ
    trained_agent = trainer.train()
    
    return trained_agent
```

### FSDPé›†æˆæ¥å£å¥‘çº¦

#### FSDPåˆå§‹åŒ–æ¥å£
```python
def fsdp_integration_interface(
    rl_config: RLTrainConfig,
    mesh: jax.sharding.Mesh,
    rng: jax.random.PRNGKey
) -> Tuple[ACRLPDPi0Agent, Callable]:
    """FSDPé›†æˆæ¥å£"""
    
    # 1. åˆ›å»ºå…¨å±€ä¼˜åŒ–å™¨ï¼ˆè§£å†³pytreeä¸€è‡´æ€§ï¼‰
    global_optimizers = create_global_optimizers(rl_config)
    
    # 2. åˆå§‹åŒ–FSDPè®­ç»ƒçŠ¶æ€
    agent, state_sharding = init_fsdp_train_state(
        rl_config, mesh, rng, global_optimizers
    )
    
    # 3. åˆ›å»ºFSDPè®­ç»ƒå‡½æ•°
    fsdp_train_step_fn = create_fsdp_train_step_fn(
        agent, mesh, data_sharding
    )
    
    return agent, fsdp_train_step_fn
```

### èŒè´£è¾¹ç•Œæ˜ç¡®å®šä¹‰

#### agents_v2èŒè´£
- **ç®—æ³•é€»è¾‘**ï¼šACRLPDç®—æ³•å®ç°ï¼ŒQ-learning + AWR
- **æŸå¤±è®¡ç®—**ï¼šCriticæŸå¤±ã€ActoræŸå¤±ã€BCæŸå¤±çš„å†…åŒ–è®¡ç®—
- **æ¨¡å‹ç®¡ç†**ï¼šÏ€â‚€æ¨¡å‹ã€Criticç½‘ç»œã€EMAå‚æ•°ç®¡ç†
- **çŠ¶æ€æ›´æ–°**ï¼šå‚æ•°æ›´æ–°ã€Targetç½‘ç»œæ›´æ–°ã€ä¼˜åŒ–å™¨çŠ¶æ€ç®¡ç†
- **OpenPIå…¼å®¹**ï¼šTrainStateæ ¼å¼è½¬æ¢ï¼ŒÏ€â‚€ checkpointç”Ÿæˆ

#### training_v2èŒè´£  
- **è®­ç»ƒå¾ªç¯**ï¼šEpochå¾ªç¯ã€æ‰¹æ¬¡è¿­ä»£ã€æ€§èƒ½ç›‘æ§
- **FSDPåˆ†ç‰‡**ï¼šåˆ†å¸ƒå¼è®­ç»ƒsetupã€å†…å­˜ä¼˜åŒ–
- **Checkpointç®¡ç†**ï¼šå®Œæ•´çŠ¶æ€ä¿å­˜/åŠ è½½ã€å®éªŒç®¡ç†
- **ç³»ç»Ÿé›†æˆ**ï¼šAgent-DataLoaderåè°ƒã€JITç¼–è¯‘ç®¡ç†

#### data_v2èŒè´£
- **æ•°æ®æ ¼å¼è½¬æ¢**ï¼šLeRobot â†’ OpenPIæ ¼å¼è½¬æ¢
- **Q-chunkingç”Ÿæˆ**ï¼šåŠ¨ä½œåºåˆ—æ„å»ºã€Bootstrapå¤„ç†
- **å†…å­˜æ± ç®¡ç†**ï¼šEpisodeç¼“å­˜ã€éšæœºé‡‡æ ·ç­–ç•¥
- **æ€§èƒ½ä¼˜åŒ–**ï¼šå¹¶è¡ŒåŠ è½½ã€Transform pipelineä¼˜åŒ–

#### scriptsèŒè´£
- **å‘½ä»¤è¡Œæ¥å£**ï¼šå‚æ•°è§£æã€é…ç½®å®šåˆ¶
- **å®éªŒç®¡ç†**ï¼šcheckpointè·¯å¾„ã€WandBé›†æˆ
- **ç¯å¢ƒè®¾ç½®**ï¼šJAXé…ç½®ã€FSDP meshåˆ›å»º
- **ç»„ä»¶åè°ƒ**ï¼šAgent-Trainer-DataLoaderçš„ç»Ÿä¸€ç®¡ç†

### æ•°æ®æµå‘å›¾
```
ç”¨æˆ·å‘½ä»¤ â†’ scripts/train_acrlpd_v2.py
    â†“
config.py (RLTrainConfigåŠ è½½)
    â†“
data_v2/acrlpd_data_loader.py (LeRobotâ†’OpenPIè½¬æ¢)
    â†“
agents_v2/acrlpd_pi0_agent.py (ç®—æ³•é€»è¾‘)
    â†“
training_v2/trainer.py (è®­ç»ƒå¾ªç¯ + FSDP)
    â†“
training_v2/checkpointing.py (OpenPIå…¼å®¹ä¿å­˜)
```

## âœ… éªŒè¯æ ‡å‡†

### åŠŸèƒ½éªŒè¯
- [ ] agents_v2å’Œtraining_v2ååŒå·¥ä½œæ­£å¸¸
- [ ] FSDPå’Œæ ‡å‡†è®­ç»ƒæ¨¡å¼éƒ½èƒ½è¿è¡Œ
- [ ] Ï€â‚€ checkpointå¯ä¿å­˜ä¸ºOpenPIæ ¼å¼
- [ ] è®­ç»ƒæ”¶æ•›æ€§ä¸åŸç³»ç»Ÿä¸€è‡´

### æ¶æ„éªŒè¯
- [ ] æ¨¡å—èŒè´£è¾¹ç•Œæ¸…æ™°
- [ ] æ¥å£ç®€å•ç»Ÿä¸€
- [ ] æ— é‡å¤åŠŸèƒ½
- [ ] ä¾èµ–å…³ç³»æ˜ç¡®

## ğŸ‰ é¢„æœŸæ”¶ç›Š

1. **æ¶æ„ç®€åŒ–**ï¼šæ¸…æ™°çš„æ¨¡å—èŒè´£ï¼Œç®€å•çš„æ¥å£è®¾è®¡
2. **ç»´æŠ¤æ€§æå‡**ï¼šæ¶ˆé™¤é‡å¤ä»£ç ï¼Œç»Ÿä¸€é…ç½®ç®¡ç†
3. **æ‰©å±•æ€§æå‡**ï¼šæ ‡å‡†åŒ–æ¥å£ä¾¿äºæ·»åŠ æ–°åŠŸèƒ½
4. **æ€§èƒ½ä¼˜åŒ–**ï¼šæ¶ˆé™¤ä¸å¿…è¦çš„å‚æ•°ä¼ é€’å’Œè®¡ç®—
5. **å…¼å®¹æ€§ä¿è¯**ï¼šä¸OpenPIç”Ÿæ€ç³»ç»Ÿå®Œå…¨å…¼å®¹

---

**ä¸‹ä¸€æ­¥ï¼šè¯¦ç»†å®æ–½è®¡åˆ’è¯·å‚è€ƒ `refactor_detail.md`**