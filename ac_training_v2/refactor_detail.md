# AC Training V2 ç³»ç»Ÿæ€§é‡æ„æ–¹æ¡ˆ (å®Œæ•´ç‰ˆ)

**åŸºäºå…¨é¢ä»£ç åˆ†æçš„è¯¦ç»†å®æ–½è®¡åˆ’**

**ç›®æ ‡**: è§£å†³ac_training_v2ä¸­çš„ä¸¥é‡æ¥å£æ–­è£‚é—®é¢˜ï¼Œå®ç°agents_v2ä¸training_v2/scriptsçš„å®Œå…¨ååŒï¼Œç¡®ä¿ä¸OpenPIå’ŒFSDPçš„å®Œå…¨å…¼å®¹æ€§ã€‚

**é‡æ„åŸåˆ™**:
1. **ä¸è€ƒè™‘å‘åå…¼å®¹** - ac_training_v2å®Œå…¨ç‹¬ç«‹
2. **åˆ é™¤æ¨ç†ç›¸å…³ä»£ç ** - ä¸“æ³¨è®­ç»ƒåŠŸèƒ½  
3. **OpenPI Ï€â‚€ checkpointå…¼å®¹** - ä¾¿äºå¤åˆ¶åˆ°ALOHAæ¨ç†
4. **æ¶ˆé™¤åŠŸèƒ½é‡å¤** - æ¸…æ™°çš„æ¨¡å—è¾¹ç•Œ

---

åŸºäºæ·±å…¥ä»£ç åˆ†æï¼Œac_training_v2ç³»ç»Ÿå­˜åœ¨ä¸¥é‡çš„æ¥å£æ–­è£‚å±æœºï¼š

### âœ… å·²é‡æ„å®Œæˆï¼šagents_v2
- **ACRLPDPi0Agentç±»**: åˆ é™¤ACRLPDPi0Configï¼Œå‚æ•°åŒ–æ„é€ å‡½æ•°
- **æŸå¤±è®¡ç®—å†…åŒ–**: compute_critic_loss, compute_actor_loss, compute_bc_loss
- **ç»Ÿä¸€å·¥å‚å‡½æ•°**: create_acrlpd_pi0_agent_from_rl_config
- **OpenPIå…¼å®¹**: create_openpi_train_state, to_train_stateæ–¹æ³•

### âŒ ä¸¥é‡æ¥å£ä¸åŒ¹é…é—®é¢˜
1. **training_v2/training_loop.py**:
   ```python
   # é”™è¯¯ï¼šä»å¯¼å…¥æ—§æ¥å£
   from agents.acrlpd_pi0_agent import ACRLPDPi0Agent, ACRLPDPi0Config
   ```
   
2. **scripts/train_acrlpd_pi0.py**:
   ```python 
   # é”™è¯¯ï¼šå¯¼å…¥å·²åˆ é™¤çš„é…ç½®
   from agents.acrlpd_pi0_agent import ACRLPDPi0Agent, create_acrlpd_pi0_agent
   ```
   
3. **åŠŸèƒ½é‡å¤**: training_v2ä¸­ä»æœ‰æŸå¤±è®¡ç®—é€»è¾‘ï¼Œä¸agents_v2å†…åŒ–æ–¹æ³•å†²çª

### ğŸ¯ è§£å†³ç­–ç•¥ï¼š5é˜¶æ®µç³»ç»Ÿæ€§é‡æ„

**ç›®æ ‡**: ä¿®å¤ä¸¥é‡çš„æ¥å£æ–­è£‚ï¼Œä½¿agents_v2ä¸training_v2/scriptsååŒå·¥ä½œ

### 1.1 ä¿®å¤training_v2å¯¼å…¥å’Œæ¥å£é€‚é…

#### å…³é”®ä¿®å¤ï¼štraining_v2/training_loop.py
```python
# ä¿®å¤å‰ï¼ˆé”™è¯¯ï¼‰ï¼š
from agents.acrlpd_pi0_agent import ACRLPDPi0Agent, ACRLPDPi0Config

# ä¿®å¤åï¼ˆæ­£ç¡®ï¼‰ï¼š
from agents_v2.acrlpd_pi0_agent import ACRLPDPi0Agent, create_acrlpd_pi0_agent_from_rl_config
```

#### åˆ é™¤é‡å¤çš„æŸå¤±è®¡ç®—é€»è¾‘
training_v2ä¸­åˆ é™¤ä»¥ä¸‹é‡å¤åŠŸèƒ½ï¼š
- âœ‚ï¸ **åˆ é™¤**: `compute_critic_losses()` - å·²å†…åŒ–åˆ°Agent  
- âœ‚ï¸ **åˆ é™¤**: `compute_actor_losses()` - å·²å†…åŒ–åˆ°Agent
- âœ‚ï¸ **åˆ é™¤**: `compute_bc_losses()` - å·²å†…åŒ–åˆ°Agent
- âœ… **ä¿ç•™**: è®­ç»ƒå¾ªç¯ã€FSDPç®¡ç†ã€checkpointä¿å­˜

### 1.2 ä¿®å¤scripts/train_acrlpd_pi0.pyæ¥å£

#### å…³é”®ä¿®å¤ï¼šå¯¼å…¥å’Œå·¥å‚å‡½æ•°è°ƒç”¨
```python
# ä¿®å¤å‰ï¼ˆé”™è¯¯ï¼‰ï¼š
from agents.acrlpd_pi0_agent import ACRLPDPi0Agent, create_acrlpd_pi0_agent

# ä¿®å¤åï¼ˆæ­£ç¡®ï¼‰ï¼š  
from agents_v2.acrlpd_pi0_agent import create_acrlpd_pi0_agent_from_rl_config
```

#### ç»Ÿä¸€Agentåˆ›å»ºæ¥å£
```python
# æ–°çš„ç»Ÿä¸€è°ƒç”¨æ–¹å¼
agent = create_acrlpd_pi0_agent_from_rl_config(
    rl_config=rl_config,
    rng=agent_rng
)
```

### 1.3 è®­ç»ƒå™¨ç±»é‡æ„ï¼šACRLPDTrainer

#### æ–°çš„ç®€åŒ–è®­ç»ƒå™¨æ¥å£
```python
class ACRLPDTrainer:
    def __init__(
        self,
        agent: ACRLPDPi0Agent,              # agents_v2çš„ç»Ÿä¸€Agent
        dataloader: ACRLPDDataLoader,       # æ•°æ®åŠ è½½å™¨
        rl_config: RLTrainConfig,           # ç»Ÿä¸€é…ç½®
        training_config: ACRLPDTrainingConfig,
        # FSDPæ”¯æŒ
        mesh: Optional[jax.sharding.Mesh] = None,
        data_sharding: Optional[jax.sharding.Sharding] = None,
    ):
    
    def train(self) -> ACRLPDPi0Agent:
        """ä¸»è®­ç»ƒå¾ªç¯ - ç›´æ¥è°ƒç”¨Agentçš„å†…åŒ–æ–¹æ³•"""
        for step in range(self.rl_config.num_train_steps):
            batch = self.dataloader.sample_batch()
            
            # å…³é”®ï¼šç›´æ¥è°ƒç”¨agents_v2çš„å†…åŒ–æ–¹æ³•
            updated_agent, train_info = self.agent.train_step(batch, rng)
            
            # è®­ç»ƒå™¨ä¸“æ³¨äºåŸºç¡€è®¾æ–½
            self._log_metrics(step, train_info)
            self._save_checkpoint_if_needed(step, updated_agent)
            self.agent = updated_agent
```

## é˜¶æ®µ 2: FSDPé›†æˆç»Ÿä¸€

**ç›®æ ‡**: ç»Ÿä¸€FSDPæ”¯æŒï¼Œæ¶ˆé™¤pytreeä¸€è‡´æ€§é—®é¢˜å’Œå†…å­˜ä¼˜åŒ–

### 2.1 åˆ›å»ºtraining_v2/fsdp_support.py

#### ç»Ÿä¸€FSDPåˆå§‹åŒ–æ¥å£
```python
def init_acrlpd_fsdp_training(
    rl_config: RLTrainConfig,
    mesh: jax.sharding.Mesh,
    rng: jax.random.PRNGKey,
    data_sharding: jax.sharding.Sharding,
    step: int = 0,
    global_pi0_tx: Optional[optax.GradientTransformation] = None,
    global_critic_tx: Optional[optax.GradientTransformation] = None
) -> Tuple[Any, jax.sharding.Sharding, Callable]:
    """
    ç»Ÿä¸€FSDPè®­ç»ƒåˆå§‹åŒ– - é¿å…æ¨¡å‹é‡å¤åˆ›å»º
    
    Returns:
        - train_state: FSDPåˆ†ç‰‡çš„è®­ç»ƒçŠ¶æ€
        - state_sharding: çŠ¶æ€åˆ†ç‰‡é…ç½®
        - lazy_jit_creator: å»¶è¿ŸJITå‡½æ•°åˆ›å»ºå™¨
    """
```

### 2.2 è§£å†³PyTreeä¸€è‡´æ€§é—®é¢˜

#### å…¨å±€ä¼˜åŒ–å™¨åˆ›å»º
```python
# åœ¨FSDPä¸Šä¸‹æ–‡å¤–åˆ›å»ºå…¨å±€ä¼˜åŒ–å™¨ï¼ˆç¡®ä¿ä¸€è‡´æ€§ï¼‰
global_pi0_tx = _optimizer.create_optimizer(rl_config.actor_optimizer, rl_config.actor_lr_schedule)
global_critic_tx = _optimizer.create_optimizer(rl_config.critic_optimizer, rl_config.critic_lr_schedule)

# ä¼ é€’ç»™FSDPåˆå§‹åŒ–é¿å…pytreeå…ƒæ•°æ®ä¸åŒ¹é…
train_state, state_sharding, lazy_jit_creator = init_acrlpd_fsdp_training(
    rl_config, mesh, rng, data_sharding, 
    global_pi0_tx=global_pi0_tx,
    global_critic_tx=global_critic_tx
)
```

### 2.3 GPUå†…å­˜ä¼˜åŒ–å’Œç›‘æ§

#### é›†æˆmemory_monitor.py
```python
from utils.memory_monitor import memory_monitor, log_memory_usage

# è®­ç»ƒè¿‡ç¨‹ä¸­çš„å†…å­˜ç›‘æ§
log_memory_usage(step, train_state, phase="training")
memory_monitor.checkpoint_memory(f"training_step_{step}", train_state)
```

## é˜¶æ®µ 3: Checkpointæœºåˆ¶ç»Ÿä¸€

**ç›®æ ‡**: å¤ç”¨agents_v2çš„OpenPIå…¼å®¹checkpointæœºåˆ¶

### 3.1 åˆ›å»ºtraining_v2/checkpointing.py

#### OpenPIå…¼å®¹ä¿å­˜æ¥å£
```python
def save_unified_checkpoints(
    agent: ACRLPDPi0Agent,
    step: int,
    checkpoint_dir: str
) -> Dict[str, str]:
    """ç»Ÿä¸€checkpointä¿å­˜ - æ”¯æŒOpenPIå’ŒACRLPDæ ¼å¼"""
    
    # 1. ä¿å­˜å®Œæ•´ACRLPDè®­ç»ƒçŠ¶æ€
    full_checkpoint_path = f"{checkpoint_dir}/acrlpd_full_step_{step}"
    save_acrlpd_full_state(agent, full_checkpoint_path)
    
    # 2. ä¿å­˜Ï€â‚€ OpenPIå…¼å®¹æ ¼å¼ï¼ˆå…³é”®ï¼šä¾¿äºæ¨ç†ï¼‰
    openpi_checkpoint_path = f"{checkpoint_dir}/pi0_openpi_step_{step}"
    openpi_train_state = agent.create_openpi_train_state()
    save_openpi_train_state(openpi_train_state, openpi_checkpoint_path)
    
    return {
        'full_checkpoint': full_checkpoint_path,
        'openpi_checkpoint': openpi_checkpoint_path
    }
```

### 3.2 OpenPI Ï€â‚€æ ¼å¼éªŒè¯
```python
def verify_openpi_checkpoint_compatibility(checkpoint_path: str) -> bool:
    """éªŒè¯Ï€â‚€ checkpointä¸OpenPIåº“çš„å…¼å®¹æ€§"""
    
    # åŠ è½½checkpointå¹¶éªŒè¯ç»“æ„
    loaded_state = load_openpi_train_state(checkpoint_path)
    
    # éªŒè¯å¿…è¦å­—æ®µå­˜åœ¨
    required_fields = ['params', 'step', 'config']
    return all(field in loaded_state for field in required_fields)
```

## é˜¶æ®µ 4: æ•°æ®æµç»Ÿä¸€

**ç›®æ ‡**: data_v2ä¸æ–°æ¶æ„çš„å®Œå…¨é›†æˆ

### 4.1 data_v2æ¥å£æ ‡å‡†åŒ–

#### ç»Ÿä¸€æ•°æ®åŠ è½½å™¨æ¥å£
```python
# data_v2/acrlpd_data_loader.pyæ¥å£æ ‡å‡†åŒ–
def create_acrlpd_data_loader(
    rl_config: RLTrainConfig,           # ç»Ÿä¸€é…ç½®å…¥å£
    batch_size: int = 128,
    episodes_per_memory_pool: int = 64,
    device_sharding: Optional[jax.sharding.Sharding] = None,
    **kwargs
) -> ACRLPDDataLoader:
    """åˆ›å»ºä¸agents_v2å…¼å®¹çš„æ•°æ®åŠ è½½å™¨"""
```

### 4.2 Q-chunkingæ ¼å¼éªŒè¯
```python
# ç¡®ä¿æ•°æ®æ ¼å¼ä¸agents_v2æœŸæœ›ä¸€è‡´
def validate_qchunking_batch_format(batch: Dict[str, jnp.ndarray]) -> bool:
    """éªŒè¯Q-chunkingæ‰¹æ¬¡æ ¼å¼ä¸agents_v2çš„å…¼å®¹æ€§"""
    
    required_keys = [
        'observations', 'next_observations', 
        'actions', 'rewards', 'masks', 'valid', 
        'terminals', 'next_terminal', 'sequence_mask'
    ]
    return all(key in batch for key in required_keys)
```

## é˜¶æ®µ 5: è„šæœ¬å’Œé…ç½®æ¸…ç†

**ç›®æ ‡**: å®Œå–„scriptså’Œé…ç½®ç³»ç»Ÿï¼Œåˆ é™¤æ¨ç†ç›¸å…³ä»£ç 

### 5.1 scripts/train_acrlpd_v2.pyé‡æ„

#### ç»Ÿä¸€è®­ç»ƒå…¥å£
```python
def main():
    # 1. åŠ è½½ç»Ÿä¸€é…ç½®
    rl_config = load_rl_config(args)
    
    # 2. è®¾ç½®JAX+FSDPç¯å¢ƒ
    mesh, data_sharding, replicated_sharding = setup_jax_environment_with_fsdp(args, rl_config)
    
    # 3. åˆ›å»ºå…¨å±€ä¼˜åŒ–å™¨ï¼ˆè§£å†³pytreeä¸€è‡´æ€§ï¼‰
    global_pi0_tx, global_critic_tx = create_global_optimizers(rl_config)
    
    # 4. ç»Ÿä¸€FSDPåˆå§‹åŒ–
    train_state, state_sharding, fsdp_train_step_fn = init_acrlpd_fsdp_training(
        rl_config, mesh, rng, data_sharding, 
        global_pi0_tx=global_pi0_tx, global_critic_tx=global_critic_tx
    )
    
    # 5. åˆ›å»ºè½»é‡çº§agentå’Œtrainer
    agent = create_acrlpd_pi0_agent_from_rl_config(rl_config, rng)
    dataloader = create_acrlpd_data_loader(rl_config, rl_config.batch_size, data_sharding=data_sharding)
    trainer = ACRLPDTrainer(agent, dataloader, rl_config)
    
    # 6. è®¾ç½®FSDPçŠ¶æ€
    trainer.setup_fsdp_state(train_state, state_sharding, fsdp_train_step_fn)
    
    # 7. æ‰§è¡Œè®­ç»ƒ
    trained_agent = trainer.train()
```

### 5.2 åˆ é™¤æ¨ç†ç›¸å…³ä»£ç 

#### æ¸…ç†æ¨ç†åŠŸèƒ½
- âœ‚ï¸ **åˆ é™¤**: æ‰€æœ‰`inference`ç›¸å…³å‡½æ•°å’Œç±»
- âœ‚ï¸ **åˆ é™¤**: ç¯å¢ƒè¯„ä¼°ä»£ç ï¼ˆevaluationå‡½æ•°ï¼‰  
- âœ‚ï¸ **åˆ é™¤**: ç­–ç•¥æœåŠ¡å™¨ç›¸å…³ä»£ç 
- âœ… **ä¿ç•™**: Ï€â‚€ checkpointä¿å­˜ï¼ˆOpenPIæ ¼å¼ï¼Œä¾¿äºæ¨ç†æ—¶ä½¿ç”¨ï¼‰

## ğŸ“‹ è¯¦ç»†å®æ–½æ£€æŸ¥æ¸…å•

### é˜¶æ®µ1æ£€æŸ¥æ¸…å•ï¼šæ¥å£ç»Ÿä¸€
- [ ] ä¿®å¤training_v2/training_loop.pyçš„å¯¼å…¥é”™è¯¯
- [ ] åˆ é™¤training_v2ä¸­é‡å¤çš„æŸå¤±è®¡ç®—å‡½æ•°
- [ ] ä¿®å¤scripts/train_acrlpd_pi0.pyçš„å¯¼å…¥å’Œå·¥å‚å‡½æ•°è°ƒç”¨
- [ ] é‡æ„ACRLPDTrainerç±»ä»¥é€‚é…agents_v2æ¥å£
- [ ] éªŒè¯Agentåˆ›å»ºå’Œè®­ç»ƒæ­¥éª¤çš„æ­£å¸¸å·¥ä½œ

### é˜¶æ®µ2æ£€æŸ¥æ¸…å•ï¼šFSDPç»Ÿä¸€
- [ ] åˆ›å»ºtraining_v2/fsdp_support.pyç»Ÿä¸€FSDPåˆå§‹åŒ–
- [ ] è§£å†³pytreeä¸€è‡´æ€§é—®é¢˜ï¼ˆå…¨å±€ä¼˜åŒ–å™¨ï¼‰
- [ ] é›†æˆGPUå†…å­˜ç›‘æ§å’Œåˆ†æ
- [ ] éªŒè¯8å¡FSDPåˆ†ç‰‡æ­£ç¡®å·¥ä½œ
- [ ] æµ‹è¯•å†…å­˜ä½¿ç”¨ä¼˜åŒ–æ•ˆæœ

### é˜¶æ®µ3æ£€æŸ¥æ¸…å•ï¼šCheckpointç»Ÿä¸€
- [ ] åˆ›å»ºtraining_v2/checkpointing.pyç»Ÿä¸€ä¿å­˜æ¥å£
- [ ] å®ç°OpenPI Ï€â‚€æ ¼å¼å…¼å®¹ä¿å­˜
- [ ] éªŒè¯checkpointä¸OpenPIåº“å…¼å®¹æ€§
- [ ] æµ‹è¯•checkpointåŠ è½½å’Œæ¢å¤è®­ç»ƒ
- [ ] ç¡®è®¤Ï€â‚€ checkpointå¯ç›´æ¥å¤åˆ¶åˆ°ALOHA

### é˜¶æ®µ4æ£€æŸ¥æ¸…å•ï¼šæ•°æ®æµç»Ÿä¸€
- [ ] æ ‡å‡†åŒ–data_v2/acrlpd_data_loader.pyæ¥å£
- [ ] éªŒè¯Q-chunkingæ ¼å¼ä¸agents_v2å…¼å®¹
- [ ] ä¼˜åŒ–æ•°æ®åŠ è½½æ€§èƒ½å’ŒFSDPåˆ†ç‰‡
- [ ] æµ‹è¯•æ•°æ®æµå®Œæ•´æ€§
- [ ] éªŒè¯LeRobotâ†’OpenPIè½¬æ¢æ­£ç¡®æ€§

### é˜¶æ®µ5æ£€æŸ¥æ¸…å•ï¼šæœ€ç»ˆæ¸…ç†
- [ ] é‡æ„scripts/train_acrlpd_v2.pyä¸ºç»Ÿä¸€å…¥å£
- [ ] åˆ é™¤æ‰€æœ‰æ¨ç†ç›¸å…³ä»£ç 
- [ ] æ¸…ç†æ®‹ç•™çš„é‡å¤åŠŸèƒ½
- [ ] ä¼˜åŒ–importä¾èµ–å…³ç³»
- [ ] å®Œå–„æ–‡æ¡£å’Œä½¿ç”¨è¯´æ˜

## ğŸ¯ æˆåŠŸéªŒè¯æ ‡å‡†

### åŠŸèƒ½éªŒè¯
1. **agents_v2 â†” training_v2ååŒ**: æ— æ¥å£é”™è¯¯ï¼Œæ­£å¸¸è®­ç»ƒ
2. **FSDPåˆ†ç‰‡**: 8å¡è®­ç»ƒå†…å­˜å‡åŒ€ï¼Œæ— å¤åˆ¶é—®é¢˜
3. **Ï€â‚€ checkpoint**: å¯ç›´æ¥å¤åˆ¶åˆ°ALOHAè¿›è¡Œæ¨ç†
4. **è®­ç»ƒæ”¶æ•›**: ä¸åŸç³»ç»Ÿä¸€è‡´çš„è®­ç»ƒæ•ˆæœ
5. **å†…å­˜ä¼˜åŒ–**: GPUå†…å­˜ä½¿ç”¨åˆç†ï¼Œæ— OOMé”™è¯¯

### æ¶æ„éªŒè¯
1. **æ— é‡å¤åŠŸèƒ½**: æŸå¤±è®¡ç®—ã€é…ç½®ç®¡ç†ç­‰æ— é‡å¤
2. **æ¸…æ™°èŒè´£**: agentsç®—æ³•ã€trainingåŸºç¡€è®¾æ–½è¾¹ç•Œæ˜ç¡®
3. **æ¥å£ç®€å•**: ç»Ÿä¸€çš„å‚æ•°ä¼ é€’å’Œè°ƒç”¨æ–¹å¼
4. **ä¾èµ–æ¸…æ™°**: importå…³ç³»ç®€å•æ˜äº†

---

**å®æ–½ä¼˜å…ˆçº§**: Stage 1 â†’ Stage 2 â†’ Stage 3 â†’ Stage 4 â†’ Stage 5

**é¢„è®¡å®Œæˆæ—¶é—´**: Stage 1 (é«˜ä¼˜å…ˆçº§ï¼Œç«‹å³å¼€å§‹) â†’ å…¶ä»–é˜¶æ®µæŒ‰éœ€æ¨è¿›

#### è§£å†³æ–¹æ¡ˆï¼š

**æ–‡ä»¶: `training_v2/acrlpd_train_state.py`**
```python
# é‡æ„ï¼šç»Ÿä¸€çš„çŠ¶æ€è½¬æ¢æ¥å£

@dataclasses.dataclass(frozen=True)
class ACRLPDTrainState:
    """ç»Ÿä¸€çš„ACRLPDè®­ç»ƒçŠ¶æ€ - å…¼å®¹agents_v2æ–°æ¶æ„"""
    
    # åŸºç¡€çŠ¶æ€
    step: int
    
    # Ï€â‚€ ç»„ä»¶ï¼ˆOpenPIå…¼å®¹ï¼‰
    pi0_params: nnx.State
    pi0_opt_state: Any
    pi0_ema_params: Optional[nnx.State] = None
    
    # Critic ç»„ä»¶ï¼ˆFSDPåˆ†ç‰‡å…¼å®¹ï¼‰
    critic_params: nnx.State  
    critic_opt_state: Any
    critic_target_params: Optional[nnx.State] = None  # Targetç½‘ç»œå‚æ•°
    
    # Temperature ç»„ä»¶ï¼ˆå¯é€‰ï¼‰
    temperature_params: Optional[nnx.State] = None
    temperature_opt_state: Optional[Any] = None
    
    # é…ç½®ä¿¡æ¯ï¼ˆJITå…¼å®¹ï¼‰
    config: ACRLPDJITConfig
    
    # Ï€â‚€ checkpointä¿å­˜å…¼å®¹æ€§
    openpi_checkpoint_params: Optional[nnx.State] = None  # Ï€â‚€å‚æ•°ç”¨äºOpenPIæ ¼å¼ä¿å­˜

def create_acrlpd_train_state_from_agent(
    agent: "ACRLPDPi0Agent",  # æ¥è‡ªagents_v2
    config_dict: Dict[str, Any]
) -> ACRLPDTrainState:
    """
    ä»é‡æ„åçš„Agentåˆ›å»ºFSDPå…¼å®¹çš„è®­ç»ƒçŠ¶æ€
    
    è¿™ä¸ªå‡½æ•°æ˜¯agents_v2å’Œtraining_v2ä¹‹é—´çš„æ¡¥æ¢
    """
    # æå–Agentçš„æ‰€æœ‰ç»„ä»¶çŠ¶æ€
    pi0_params = nnx.state(agent.pi0_model, nnx.Param)
    critic_params = nnx.state(agent.critic_networks, nnx.Param)
    
    # åˆ›å»ºJITå…¼å®¹é…ç½®
    jit_config = ACRLPDJITConfig(
        critic_weight=agent.critic_weight,
        actor_weight=agent.actor_weight,
        bc_loss_weight=agent.bc_weight,
        horizon_length=agent.horizon_length,
        discount=agent.discount,
        q_aggregation=agent.q_aggregation,
        real_action_dim=agent.real_action_dim,
        target_update_tau=agent.target_update_tau
    )
    
    # åˆ›å»ºOpenPI checkpointå…¼å®¹çš„å‚æ•°ï¼ˆä»…Ï€â‚€ï¼‰
    openpi_checkpoint_params = pi0_params
    
    return ACRLPDTrainState(
        step=agent.step,
        pi0_params=pi0_params,
        pi0_opt_state=agent.pi0_optimizer_state.value,
        pi0_ema_params=getattr(agent, '_pi0_ema_params', None),
        critic_params=critic_params,
        critic_opt_state=agent.critic_optimizer_state.value,
        critic_target_params=nnx.state(agent.critic_networks.target_networks, nnx.Param),
        temperature_params=nnx.state(agent.temperature_module, nnx.Param) if agent.temperature_module else None,
        temperature_opt_state=agent.temperature_optimizer_state.value if agent.temperature_module else None,
        config=jit_config,
        openpi_checkpoint_params=openpi_checkpoint_params
    )

def update_agent_from_acrlpd_train_state(
    agent: "ACRLPDPi0Agent",
    train_state: ACRLPDTrainState
) -> "ACRLPDPi0Agent":
    """
    å°†FSDPè®­ç»ƒåçš„çŠ¶æ€æ›´æ–°å›Agent
    """
    # æ›´æ–°æ­¥æ•°
    agent._step = int(train_state.step)
    
    # æ›´æ–°Ï€â‚€ç»„ä»¶
    nnx.update(agent.pi0_model, train_state.pi0_params)
    agent.pi0_optimizer_state.value = train_state.pi0_opt_state
    if train_state.pi0_ema_params is not None:
        agent._pi0_ema_params = train_state.pi0_ema_params
    
    # æ›´æ–°Criticç»„ä»¶
    nnx.update(agent.critic_networks, train_state.critic_params)
    agent.critic_optimizer_state.value = train_state.critic_opt_state
    if train_state.critic_target_params is not None:
        nnx.update(agent.critic_networks.target_networks, train_state.critic_target_params)
    
    # æ›´æ–°Temperatureç»„ä»¶
    if agent.temperature_module and train_state.temperature_params:
        nnx.update(agent.temperature_module, train_state.temperature_params)
        if train_state.temperature_opt_state:
            agent.temperature_optimizer_state.value = train_state.temperature_opt_state
    
    return agent
```

### 1.2 Training Loop é‡æ„

**æ–‡ä»¶: `training_v2/training_loop.py`**
```python
# é‡æ„ï¼šé€‚é…agents_v2çš„æ–°æ¶æ„

class ACRLPDTrainingLoop:
    """ç»Ÿä¸€çš„ACRLPDè®­ç»ƒå¾ªç¯ - ååŒagents_v2å’Œtraining_v2"""
    
    def __init__(
        self,
        agent: "ACRLPDPi0Agent",  # æ¥è‡ªagents_v2ï¼Œå·²å†…åŒ–æŸå¤±è®¡ç®—
        data_loader: Any,
        enable_fsdp: bool = False,
        enable_wandb: bool = True
    ):
        self.agent = agent
        self.data_loader = data_loader
        self.enable_fsdp = enable_fsdp
        self.enable_wandb = enable_wandb
        
        # æ ¹æ®æ˜¯å¦å¯ç”¨FSDPé€‰æ‹©è®­ç»ƒæ¨¡å¼
        if enable_fsdp:
            self._setup_fsdp_training()
        else:
            self._setup_standard_training()
    
    def _setup_fsdp_training(self):
        """è®¾ç½®FSDPåˆ†å¸ƒå¼è®­ç»ƒ"""
        # åˆ›å»ºFSDPå…¼å®¹çš„è®­ç»ƒçŠ¶æ€
        config_dict = {
            'critic_weight': self.agent.critic_weight,
            'actor_weight': self.agent.actor_weight,
            'bc_loss_weight': self.agent.bc_weight,
            'horizon_length': self.agent.horizon_length,
            'discount': self.agent.discount,
            'real_action_dim': self.agent.real_action_dim
        }
        
        self.train_state = create_acrlpd_train_state_from_agent(self.agent, config_dict)
        
        # è®¾ç½®FSDPåˆ†ç‰‡
        from . import acrlpd_sharding
        self.train_state = acrlpd_sharding.setup_fsdp_sharding(self.train_state)
    
    def _setup_standard_training(self):
        """è®¾ç½®æ ‡å‡†å•æœºè®­ç»ƒ"""
        self.train_state = None  # ç›´æ¥ä½¿ç”¨Agentçš„å†…éƒ¨çŠ¶æ€
    
    def train_step(self, batch: Dict[str, jnp.ndarray], rng: jnp.ndarray) -> Tuple[float, Dict[str, Any]]:
        """
        ç»Ÿä¸€çš„è®­ç»ƒæ­¥éª¤ - è‡ªåŠ¨é€‰æ‹©FSDPæˆ–æ ‡å‡†æ¨¡å¼
        """
        if self.enable_fsdp:
            return self._fsdp_train_step(batch, rng)
        else:
            return self._standard_train_step(batch, rng)
    
    def _standard_train_step(self, batch: Dict[str, jnp.ndarray], rng: jnp.ndarray) -> Tuple[float, Dict[str, Any]]:
        """æ ‡å‡†è®­ç»ƒæ­¥éª¤ - ç›´æ¥ä½¿ç”¨Agentçš„å†…åŒ–æŸå¤±è®¡ç®—"""
        
        # ä½¿ç”¨Agentå†…åŒ–çš„æŸå¤±è®¡ç®—å’Œæ¢¯åº¦æ›´æ–°
        updated_agent, loss_info = self.agent.train_step(batch, rng)
        self.agent = updated_agent
        
        return loss_info['total_loss'], loss_info
    
    def _fsdp_train_step(self, batch: Dict[str, jnp.ndarray], rng: jnp.ndarray) -> Tuple[float, Dict[str, Any]]:
        """FSDPè®­ç»ƒæ­¥éª¤ - ä½¿ç”¨åˆ†ç‰‡çŠ¶æ€"""
        
        # åœ¨FSDPä¸Šä¸‹æ–‡ä¸­ä½¿ç”¨Agentçš„æŸå¤±è®¡ç®—
        def fsdp_loss_fn(train_state):
            # ä¸´æ—¶ä»è®­ç»ƒçŠ¶æ€é‡å»ºAgentä»¥è®¡ç®—æŸå¤±
            temp_agent = update_agent_from_acrlpd_train_state(self.agent, train_state)
            total_loss, loss_info = temp_agent.compute_loss(batch, rng)
            return total_loss, loss_info
        
        # åº”ç”¨FSDPæ¢¯åº¦æ›´æ–°
        (total_loss, loss_info), updated_train_state = self._apply_fsdp_gradients(
            fsdp_loss_fn, self.train_state, batch, rng
        )
        
        # æ›´æ–°è®­ç»ƒçŠ¶æ€
        self.train_state = updated_train_state
        
        # åŒæ­¥AgentçŠ¶æ€ï¼ˆå¯é€‰ï¼Œç”¨äºç›‘æ§ï¼‰
        if hasattr(self, '_sync_agent_state') and self._sync_agent_state:
            self.agent = update_agent_from_acrlpd_train_state(self.agent, self.train_state)
        
        return total_loss, loss_info
    


### 1.3 FSDP Sharding é€‚é…

**æ–‡ä»¶: `training_v2/acrlpd_sharding.py`**
```python
# é‡æ„ï¼šé€‚é…æ–°çš„ACRLPDTrainStateç»“æ„

def setup_fsdp_sharding(train_state: ACRLPDTrainState) -> ACRLPDTrainState:
    """
    ä¸ºæ–°çš„è®­ç»ƒçŠ¶æ€è®¾ç½®FSDPåˆ†ç‰‡
    
    é€‚é…agents_v2çš„å¤šç»„ä»¶æ¶æ„
    """
    
    # Ï€â‚€ç»„ä»¶åˆ†ç‰‡ï¼ˆå¤§æ¨¡å‹ï¼Œéœ€è¦ç»†ç²’åº¦åˆ†ç‰‡ï¼‰
    pi0_sharding_spec = sharding.create_fsdp_sharding_spec(
        train_state.pi0_params,
        shard_strategy='model_parallel',  # æ¨¡å‹å¹¶è¡Œåˆ†ç‰‡
        min_shard_size=1024 * 1024  # 1MBæœ€å°åˆ†ç‰‡
    )
    
    # Criticç»„ä»¶åˆ†ç‰‡ï¼ˆç›¸å¯¹è¾ƒå°ï¼Œå¯ä»¥å¤åˆ¶æˆ–ç²—ç²’åº¦åˆ†ç‰‡ï¼‰
    critic_sharding_spec = sharding.create_fsdp_sharding_spec(
        train_state.critic_params,
        shard_strategy='data_parallel',  # æ•°æ®å¹¶è¡Œå¤åˆ¶
        min_shard_size=512 * 1024  # 512KBæœ€å°åˆ†ç‰‡
    )
    
    # åº”ç”¨åˆ†ç‰‡
    sharded_train_state = jax.tree.map(
        lambda x, spec: sharding.apply_sharding(x, spec),
        train_state,
        {
            'pi0_params': pi0_sharding_spec,
            'pi0_opt_state': pi0_sharding_spec,
            'critic_params': critic_sharding_spec,
            'critic_opt_state': critic_sharding_spec,
            'temperature_params': None,  # å°å‚æ•°ä¸åˆ†ç‰‡
            'temperature_opt_state': None
        }
    )
    
    return sharded_train_state

def create_fsdp_mesh() -> jax.sharding.Mesh:
    """
    åˆ›å»ºé€‚åˆACRLPDçš„FSDP mesh
    """
    devices = jax.devices()
    if len(devices) == 1:
        # å•æœºæ¨¡å¼
        mesh = jax.sharding.Mesh(devices, ('data',))
    elif len(devices) <= 8:
        # å•èŠ‚ç‚¹å¤šGPU
        mesh = jax.sharding.Mesh(devices, ('fsdp',))
    else:
        # å¤šèŠ‚ç‚¹
        mesh = jax.sharding.Mesh(
            devices.reshape((-1, 8)), 
            ('data', 'model')
        )
    
    return mesh
```

---

## é˜¶æ®µ 2: å¤ç”¨ç°æœ‰Checkpointæœºåˆ¶ âœ… **å·²ç¡®è®¤æ»¡è¶³è¦æ±‚**

### ğŸ¯ å®æ–½çŠ¶æ€ï¼šCOMPLETED

**åˆ†æç»“æœ**: å½“å‰agentsä¸­çš„checkpointä¿å­˜æœºåˆ¶å·²ç»æ»¡è¶³OpenPIå…¼å®¹æ€§è¦æ±‚ï¼Œæ— éœ€é‡æ–°è®¾è®¡ã€‚

**ç°æœ‰æœºåˆ¶åˆ†æ**:
- âœ… **`create_train_state()`** - åˆ›å»ºOpenPIå…¼å®¹TrainStateï¼Œä»…åŒ…å«Ï€â‚€å‚æ•°å’Œæ¨¡å‹å®šä¹‰
- âœ… **`save_component_checkpoints()`** - åˆ†åˆ«ä¿å­˜Ï€â‚€ã€criticç­‰å„ç»„ä»¶çš„å®Œæ•´checkpoint
- âœ… **æ ¼å¼å…¼å®¹**: TrainStateæ ¼å¼ä¸OpenPIåº“æ ‡å‡†å®Œå…¨ä¸€è‡´
- âœ… **EMAæ”¯æŒ**: è‡ªåŠ¨ä½¿ç”¨EMAå‚æ•°ä½œä¸ºæ¨ç†æƒé‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
- âœ… **éƒ¨ç½²å‹å¥½**: Ï€â‚€ checkpointå¯ç›´æ¥æ‹·è´åˆ°ALOHAæœºå™¨äºº

### 2.1 Ï€â‚€ Checkpoint OpenPIæ ¼å¼ä¿å­˜

**æ–‡ä»¶: `training_v2/pi0_checkpoint_saver.py`** (æ–°å»º)
```python
"""
Ï€â‚€ Checkpoint OpenPIæ ¼å¼ä¿å­˜å™¨

ä¸“é—¨ç”¨äºä¿å­˜Ï€â‚€æ¨¡å‹ä¸ºOpenPIæ ‡å‡†æ ¼å¼ï¼Œæ–¹ä¾¿æ‹·è´åˆ°ALOHAæœºå™¨äºº
"""

import openpi.training.utils as training_utils
from typing import Optional

def save_pi0_openpi_checkpoint(
    acrlpd_state: ACRLPDTrainState,
    checkpoint_path: str,
    step: int
):
    """
    ä¿å­˜Ï€â‚€æ¨¡å‹ä¸ºOpenPIæ ‡å‡†æ ¼å¼
    
    Args:
        acrlpd_state: ACRLPDè®­ç»ƒçŠ¶æ€
        checkpoint_path: ä¿å­˜è·¯å¾„
        step: è®­ç»ƒæ­¥æ•°
    """
    
    # æå–Ï€â‚€ç»„ä»¶ï¼ˆä»…Ï€â‚€å‚æ•°ï¼‰
    pi0_params = acrlpd_state.pi0_params
    pi0_ema_params = acrlpd_state.pi0_ema_params
    
    # ä½¿ç”¨EMAå‚æ•°ä½œä¸ºä¸»è¦å‚æ•°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    main_params = pi0_ema_params if pi0_ema_params is not None else pi0_params
    
    # åˆ›å»ºæœ€å°ä¼˜åŒ–å™¨ï¼ˆOpenPIæ ¼å¼è¦æ±‚ï¼‰
    dummy_tx = optax.sgd(learning_rate=1e-4)
    dummy_opt_state = jax.tree.map(
        lambda x: jnp.zeros((), dtype=x.dtype),
        jax.eval_shape(lambda: dummy_tx.init(main_params))
    )
    
    # åˆ›å»ºOpenPI TrainState
    openpi_train_state = training_utils.TrainState(
        step=step,
        params=main_params,  # Ï€â‚€ä¸»è¦å‚æ•°
        model_def=acrlpd_state.pi0_model_def,  # Ï€â‚€æ¨¡å‹å®šä¹‰
        opt_state=dummy_opt_state,  # å ä½ç¬¦ä¼˜åŒ–å™¨çŠ¶æ€
        tx=dummy_tx,  # å ä½ç¬¦ä¼˜åŒ–å™¨
        ema_decay=0.999 if pi0_ema_params is not None else None,
        ema_params=pi0_ema_params  # EMAå‚æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
    )
    
    # ä¿å­˜ä¸ºOpenPIæ ‡å‡†æ ¼å¼
    checkpoint_dir = Path(checkpoint_path)
    checkpoint_dir.mkdir(parents=True, exist_ok=True)
    
    # ä½¿ç”¨OpenPIæ ‡å‡†ä¿å­˜å‡½æ•°
    from openpi.training.checkpoints import save_train_state
    save_train_state(openpi_train_state, str(checkpoint_dir))
    
    # ä¿å­˜ç®€å•å…ƒæ•°æ®
    metadata = {
        'step': step,
        'model_type': 'pi0',
        'openpi_compatible': True,
        'has_ema': pi0_ema_params is not None
    }
    
    with open(checkpoint_dir / "metadata.json", "w") as f:
        json.dump(metadata, f, indent=2)
    
    logger.info(f"âœ… Ï€â‚€ OpenPIæ ¼å¼checkpointå·²ä¿å­˜: {checkpoint_dir}")
```

### 2.2 ç»Ÿä¸€çš„é…ç½®ç®¡ç†

**æ–‡ä»¶: `config.py`** (æ›´æ–°)
```python
# æ‰©å±•ï¼šæ·»åŠ OpenPIå…¼å®¹æ€§é…ç½®

@dataclasses.dataclass(frozen=True)
class OpenPICompatibilityConfig:
    """OpenPIå…¼å®¹æ€§é…ç½®"""
    
    # æ¨ç†ç›¸å…³
    enable_ema_for_inference: bool = True
    ema_decay_rate: float = 0.999
    
    # Checkpointæ ¼å¼
    save_openpi_checkpoints: bool = True
    validate_openpi_compatibility: bool = True
    

@dataclasses.dataclass(frozen=True) 
class ACRLPDv2Config:
    """AC Training V2 ç»Ÿä¸€é…ç½®"""
    
    # åŸºç¡€é…ç½®
    name: str
    batch_size: int
    
    # ç»„ä»¶é…ç½®
    model: Any  # Ï€â‚€æ¨¡å‹é…ç½®
    critic: CriticConfig
    acrlpd: ACRLPDConfig
    qchunking: QChunkingConfig
    
    # è®­ç»ƒé…ç½®
    actor_lr_schedule: Any
    critic_lr_schedule: Any
    actor_optimizer: Any
    critic_optimizer: Any
    
    # å…¼å®¹æ€§é…ç½®
    openpi_compatibility: OpenPICompatibilityConfig
    
    # FSDPé…ç½®
    enable_fsdp: bool = False
    fsdp_config: Optional[FSDPConfig] = None

# é¢„å®šä¹‰é…ç½®ï¼ˆæ›´æ–°ï¼‰
ACRLPD_V2_ALOHA_FOLD = ACRLPDv2Config(
    name="acrlpd_v2_aloha_fold",
    batch_size=256,
    model=PI0_CONFIG_FOLD,
    critic=CriticConfig(
        num_critics=10,
        hidden_dims=[512, 512, 512],
        use_layer_norm=True,
        dropout_rate=0.1
    ),
    acrlpd=ACRLPDConfig(
        num_action_samples=4,  # Best-of-N
        discount=0.99,
        bc_loss_weight=0.05,
        use_adaptive_temperature=True
    ),
    qchunking=QChunkingConfig(
        action_dim=14,  # ALOHAçœŸå®åŠ¨ä½œç»´åº¦
        horizon_length=20
    ),
    actor_lr_schedule=create_cosine_schedule(3e-5, 50000),
    critic_lr_schedule=create_cosine_schedule(3e-4, 50000),
    actor_optimizer=_optimizer.AdamW(weight_decay=1e-4),
    critic_optimizer=_optimizer.AdamW(weight_decay=1e-4),
    openpi_compatibility=OpenPICompatibilityConfig(
        enable_ema_for_inference=True,
        save_openpi_checkpoints=True,
        validate_openpi_compatibility=True
    ),
    enable_fsdp=False
)
```

---

## é˜¶æ®µ 3: FSDP åˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ– (Medium Priority) 

### ğŸ¯ å®æ–½çŠ¶æ€ï¼šPENDING

**ç›®æ ‡**: ä¼˜åŒ–FSDPåˆ†å¸ƒå¼è®­ç»ƒæ€§èƒ½ï¼Œç¡®ä¿å¤šGPUå’Œå¤šèŠ‚ç‚¹è®­ç»ƒçš„ç¨³å®šæ€§ã€‚

### 3.1 å†…å­˜ä¼˜åŒ–çš„FSDPç­–ç•¥

**æ–‡ä»¶: `training_v2/fsdp_optimization.py`** (æ–°å»º)
```python
"""
FSDPåˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ–

é’ˆå¯¹ACRLPDå¤šç»„ä»¶æ¶æ„çš„FSDPä¼˜åŒ–ç­–ç•¥
"""

class ACRLPDFSDPOptimizer:
    """ACRLPDä¸“ç”¨çš„FSDPä¼˜åŒ–å™¨"""
    
    def __init__(self, config: ACRLPDv2Config):
        self.config = config
        self.mesh = self._create_optimal_mesh()
        
    def _create_optimal_mesh(self) -> jax.sharding.Mesh:
        """
        æ ¹æ®ç¡¬ä»¶é…ç½®åˆ›å»ºæœ€ä¼˜çš„mesh
        """
        devices = jax.devices()
        total_devices = len(devices)
        
        if total_devices == 1:
            # å•GPUï¼šä¸åˆ†ç‰‡
            return jax.sharding.Mesh([devices[0]], ('replica',))
        elif total_devices <= 8:
            # å•èŠ‚ç‚¹ï¼šÏ€â‚€æ¨¡å‹å¹¶è¡Œ + Criticæ•°æ®å¹¶è¡Œ
            return jax.sharding.Mesh(devices, ('model',))
        else:
            # å¤šèŠ‚ç‚¹ï¼š2D mesh
            nodes = total_devices // 8
            gpus_per_node = 8
            mesh_shape = (nodes, gpus_per_node)
            return jax.sharding.Mesh(
                devices.reshape(mesh_shape), 
                ('data', 'model')
            )
    
    def create_optimized_sharding_strategy(
        self, 
        train_state: ACRLPDTrainState
    ) -> Dict[str, jax.sharding.NamedSharding]:
        """
        åˆ›å»ºä¼˜åŒ–çš„åˆ†ç‰‡ç­–ç•¥
        """
        strategies = {}
        
        # Ï€â‚€ç»„ä»¶ï¼šå¤§æ¨¡å‹ï¼Œéœ€è¦æ¨¡å‹å¹¶è¡Œ
        pi0_strategy = jax.sharding.NamedSharding(
            self.mesh, 
            jax.sharding.PartitionSpec('model',)
        )
        strategies['pi0_params'] = pi0_strategy
        strategies['pi0_opt_state'] = pi0_strategy
        strategies['pi0_ema_params'] = pi0_strategy
        
        # Criticç»„ä»¶ï¼šå°æ¨¡å‹ï¼Œæ•°æ®å¹¶è¡Œå¤åˆ¶
        critic_strategy = jax.sharding.NamedSharding(
            self.mesh,
            jax.sharding.PartitionSpec(None,)  # å¤åˆ¶åˆ°æ‰€æœ‰è®¾å¤‡
        )
        strategies['critic_params'] = critic_strategy
        strategies['critic_opt_state'] = critic_strategy
        
        # Temperatureï¼šéå¸¸å°ï¼Œå¤åˆ¶
        temp_strategy = jax.sharding.NamedSharding(
            self.mesh,
            jax.sharding.PartitionSpec(None,)
        )
        strategies['temperature_params'] = temp_strategy
        strategies['temperature_opt_state'] = temp_strategy
        
        return strategies
    
    def optimize_gradient_accumulation(
        self,
        global_batch_size: int,
        num_devices: int
    ) -> Tuple[int, int]:
        """
        ä¼˜åŒ–æ¢¯åº¦ç´¯ç§¯ç­–ç•¥
        
        Returns:
            (per_device_batch_size, gradient_accumulation_steps)
        """
        # è®¡ç®—æ¯è®¾å¤‡æ‰¹æ¬¡å¤§å°
        per_device_batch_size = global_batch_size // num_devices
        
        # å†…å­˜é™åˆ¶æ£€æŸ¥
        max_per_device_batch = self._estimate_max_batch_size()
        
        if per_device_batch_size <= max_per_device_batch:
            # æ— éœ€æ¢¯åº¦ç´¯ç§¯
            return per_device_batch_size, 1
        else:
            # éœ€è¦æ¢¯åº¦ç´¯ç§¯
            grad_acc_steps = (per_device_batch_size + max_per_device_batch - 1) // max_per_device_batch
            adjusted_batch_size = per_device_batch_size // grad_acc_steps
            return adjusted_batch_size, grad_acc_steps
    
    def _estimate_max_batch_size(self) -> int:
        """
        ä¼°ç®—å•è®¾å¤‡æœ€å¤§æ‰¹æ¬¡å¤§å°
        """
        # åŸºäºGPUå†…å­˜å’Œæ¨¡å‹å¤§å°çš„å¯å‘å¼ä¼°ç®—
        gpu_memory_gb = self._get_gpu_memory_gb()
        
        if gpu_memory_gb >= 40:  # A100
            return 64
        elif gpu_memory_gb >= 24:  # 4090
            return 32
        elif gpu_memory_gb >= 16:  # V100
            return 16
        else:
            return 8

    def setup_fsdp_training(
        self, 
        train_state: ACRLPDTrainState
    ) -> Tuple[ACRLPDTrainState, Dict[str, Any]]:
        """
        è®¾ç½®ä¼˜åŒ–çš„FSDPè®­ç»ƒ
        """
        # åˆ›å»ºåˆ†ç‰‡ç­–ç•¥
        sharding_strategies = self.create_optimized_sharding_strategy(train_state)
        
        # åº”ç”¨åˆ†ç‰‡
        sharded_train_state = self._apply_sharding(train_state, sharding_strategies)
        
        # åˆ›å»ºè®­ç»ƒé…ç½®
        training_config = {
            'mesh': self.mesh,
            'sharding_strategies': sharding_strategies,
            'gradient_accumulation': self.optimize_gradient_accumulation(
                self.config.batch_size, 
                len(jax.devices())
            )
        }
        
        return sharded_train_state, training_config
```

### 3.2 Multi-Host è®­ç»ƒæ”¯æŒ

**æ–‡ä»¶: `training_v2/multi_host_training.py`** (æ–°å»º)
```python
"""
å¤šèŠ‚ç‚¹åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
"""

class MultiHostACRLPDTrainer:
    """å¤šèŠ‚ç‚¹ACRLPDè®­ç»ƒå™¨"""
    
    def __init__(self, config: ACRLPDv2Config):
        self.config = config
        self.setup_multi_host()
    
    def setup_multi_host(self):
        """è®¾ç½®å¤šèŠ‚ç‚¹ç¯å¢ƒ"""
        # åˆå§‹åŒ–JAXåˆ†å¸ƒå¼
        jax.distributed.initialize()
        
        # è·å–ä¸»æœºä¿¡æ¯
        self.process_index = jax.process_index()
        self.process_count = jax.process_count()
        self.local_device_count = jax.local_device_count()
        
        logger.info(f"Multi-host setup: process {self.process_index}/{self.process_count}, "
                   f"local devices: {self.local_device_count}")
    
    def create_global_mesh(self) -> jax.sharding.Mesh:
        """åˆ›å»ºå…¨å±€mesh"""
        global_devices = jax.devices()
        
        # é‡å¡‘ä¸º2D mesh: [num_hosts, devices_per_host]
        devices_per_host = len(global_devices) // self.process_count
        mesh_shape = (self.process_count, devices_per_host)
        
        global_mesh = jax.sharding.Mesh(
            global_devices.reshape(mesh_shape),
            ('hosts', 'devices')
        )
        
        return global_mesh
    
    def sync_train_state_across_hosts(
        self, 
        train_state: ACRLPDTrainState
    ) -> ACRLPDTrainState:
        """è·¨ä¸»æœºåŒæ­¥è®­ç»ƒçŠ¶æ€"""
        
        # ä½¿ç”¨JAXçš„é›†åˆé€šä¿¡åŒæ­¥å‚æ•°
        def sync_component(component):
            return jax.experimental.multihost_utils.sync_global_devices(component)
        
        synced_train_state = jax.tree.map(sync_component, train_state)
        
        return synced_train_state
    
    def save_global_checkpoint(
        self, 
        train_state: ACRLPDTrainState, 
        checkpoint_path: str
    ):
        """ä¿å­˜å…¨å±€checkpointï¼ˆä»…åœ¨ä¸»è¿›ç¨‹ï¼‰"""
        
        if self.process_index == 0:
            # æ”¶é›†æ‰€æœ‰ä¸»æœºçš„çŠ¶æ€
            global_train_state = self.sync_train_state_across_hosts(train_state)
            
            # ä¿å­˜checkpoint
            checkpoint_manager = OpenPICheckpointManager(checkpoint_path)
            checkpoint_manager.save_openpi_checkpoint(
                global_train_state, 
                int(global_train_state.step)
            )
            
            logger.info(f"Global checkpoint saved: {checkpoint_path}")
        
        # ç­‰å¾…ä¸»è¿›ç¨‹å®Œæˆä¿å­˜
        jax.experimental.multihost_utils.sync_global_devices("checkpoint_sync")
```

---

## é˜¶æ®µ 4: æ•°æ®å¤„ç†å’Œè„šæœ¬ç³»ç»Ÿé‡æ„ (Medium Priority)

### ğŸ¯ å®æ–½çŠ¶æ€ï¼šPENDING

**ç›®æ ‡**: é‡æ„data_v2å’Œscriptsï¼Œç¡®ä¿ä¸æ–°çš„agents_v2å’Œtraining_v2ç³»ç»ŸååŒå·¥ä½œã€‚

### 4.1 æ•°æ®åŠ è½½å™¨é€‚é…

**æ–‡ä»¶: `data_v2/acrlpd_data_loader_v2.py`** (æ–°å»º)
```python
"""
ACRLPD V2 æ•°æ®åŠ è½½å™¨

é€‚é…æ–°çš„agents_v2æ¶æ„å’Œtraining_v2éœ€æ±‚
"""

class ACRLPDv2DataLoader:
    """
    ACRLPD V2 æ•°æ®åŠ è½½å™¨
    
    ç‰¹ç‚¹ï¼š
    - æ”¯æŒåŠ¨æ€æ‰¹æ¬¡å¤§å°ï¼ˆFSDPé€‚é…ï¼‰
    - å†…å­˜ä¼˜åŒ–çš„æ•°æ®ç®¡é“
    - OpenPIå…¼å®¹çš„æ•°æ®æ ¼å¼
    """
    
    def __init__(self, config: ACRLPDv2Config):
        self.config = config
        self.setup_data_pipeline()
    
    def setup_data_pipeline(self):
        """è®¾ç½®ä¼˜åŒ–çš„æ•°æ®ç®¡é“"""
        
        # è®¡ç®—æœ€ä¼˜çš„æ•°æ®åŠ è½½å‚æ•°
        num_devices = len(jax.devices())
        global_batch_size = self.config.batch_size
        
        # FSDPä¸‹çš„æ‰¹æ¬¡é…ç½®
        if self.config.enable_fsdp:
            per_device_batch_size = global_batch_size // num_devices
            prefetch_factor = 2  # å‡å°‘é¢„å–é™ä½å†…å­˜ä½¿ç”¨
        else:
            per_device_batch_size = global_batch_size
            prefetch_factor = 4
        
        self.per_device_batch_size = per_device_batch_size
        self.prefetch_factor = prefetch_factor
        
        # è®¾ç½®æ•°æ®è½¬æ¢ç®¡é“
        self.transforms = self._create_data_transforms()
    
    def _create_data_transforms(self) -> List[Callable]:
        """åˆ›å»ºæ•°æ®è½¬æ¢ç®¡é“"""
        transforms = []
        
        # 1. åŸºç¡€æ ¼å¼è½¬æ¢
        transforms.append(self._convert_to_acrlpd_format)
        
        # 2. åŠ¨ä½œç»´åº¦å¤„ç†ï¼ˆOpenPI 32ç»´ -> çœŸå®åŠ¨ä½œç»´åº¦ï¼‰
        transforms.append(partial(
            self._process_action_dimensions,
            target_dim=self.config.qchunking.action_dim
        ))
        
        # 3. Q-chunkingå‡†å¤‡
        transforms.append(partial(
            self._prepare_qchunking_data,
            horizon_length=self.config.qchunking.horizon_length
        ))
        
        # 4. è§‚å¯Ÿç¼–ç é¢„å¤„ç†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.config.enable_observation_preprocessing:
            transforms.append(self._preprocess_observations)
        
        return transforms
    
    def _convert_to_acrlpd_format(self, batch: Dict[str, Any]) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºACRLPDæ ¼å¼"""
        
        # ç¡®ä¿æœ‰å¿…è¦çš„é”®
        required_keys = ['observations', 'actions', 'rewards', 'next_observations', 'masks']
        for key in required_keys:
            if key not in batch:
                raise ValueError(f"Missing required key: {key}")
        
        # ç±»å‹è½¬æ¢
        converted_batch = {}
        for key, value in batch.items():
            if isinstance(value, np.ndarray):
                converted_batch[key] = jnp.array(value)
            else:
                converted_batch[key] = value
        
        return converted_batch
    
    def _process_action_dimensions(
        self, 
        batch: Dict[str, Any], 
        target_dim: int
    ) -> Dict[str, Any]:
        """å¤„ç†åŠ¨ä½œç»´åº¦å…¼å®¹æ€§"""
        
        actions = batch['actions']
        
        # æ£€æŸ¥å½“å‰åŠ¨ä½œç»´åº¦
        if actions.shape[-1] > target_dim:
            # ä»32ç»´æˆªæ–­åˆ°ç›®æ ‡ç»´åº¦
            batch['actions'] = actions[..., :target_dim]
            logger.debug(f"Action dimensions truncated: {actions.shape[-1]} -> {target_dim}")
        elif actions.shape[-1] < target_dim:
            # å¡«å……åˆ°ç›®æ ‡ç»´åº¦
            padding_shape = actions.shape[:-1] + (target_dim - actions.shape[-1],)
            padding = jnp.zeros(padding_shape)
            batch['actions'] = jnp.concatenate([actions, padding], axis=-1)
            logger.debug(f"Action dimensions padded: {actions.shape[-1]} -> {target_dim}")
        
        return batch
    
    def create_data_iterator(
        self, 
        dataset_path: str, 
        split: str = "train",
        shuffle: bool = True
    ) -> Iterator[Dict[str, jnp.ndarray]]:
        """
        åˆ›å»ºæ•°æ®è¿­ä»£å™¨
        
        æ”¯æŒFSDPå’Œæ ‡å‡†è®­ç»ƒæ¨¡å¼
        """
        
        # åŠ è½½LeRobotæ•°æ®é›†
        from lerobot.common.datasets.lerobot_dataset import LeRobotDataset
        
        dataset = LeRobotDataset(
            repo_id=dataset_path,
            split=split,
            batch_size=self.per_device_batch_size,
            shuffle=shuffle
        )
        
        # åˆ›å»ºæ•°æ®è¿­ä»£å™¨
        data_iterator = iter(dataset)
        
        # åº”ç”¨å˜æ¢ç®¡é“
        def transform_batch(batch):
            for transform in self.transforms:
                batch = transform(batch)
            return batch
        
        # åŒ…è£…è¿­ä»£å™¨
        transformed_iterator = map(transform_batch, data_iterator)
        
        return transformed_iterator
```

### 4.2 ç»Ÿä¸€è®­ç»ƒè„šæœ¬

**æ–‡ä»¶: `scripts/train_acrlpd_v2.py`** (æ–°å»º)
```python
"""
ACRLPD V2 ç»Ÿä¸€è®­ç»ƒè„šæœ¬

æ”¯æŒå•æœºå’ŒFSDPåˆ†å¸ƒå¼è®­ç»ƒ
"""

import argparse
from pathlib import Path

def main():
    parser = argparse.ArgumentParser(description="ACRLPD V2 Training")
    
    # åŸºç¡€é…ç½®
    parser.add_argument("--config", required=True, help="é…ç½®åç§°")
    parser.add_argument("--exp_name", required=True, help="å®éªŒåç§°")
    parser.add_argument("--data_path", required=True, help="æ•°æ®è·¯å¾„")
    
    # è®­ç»ƒæ¨¡å¼
    parser.add_argument("--enable_fsdp", action="store_true", help="å¯ç”¨FSDPåˆ†å¸ƒå¼è®­ç»ƒ")
    parser.add_argument("--num_steps", type=int, default=50000, help="è®­ç»ƒæ­¥æ•°")
    parser.add_argument("--save_interval", type=int, default=5000, help="ä¿å­˜é—´éš”")
    
    # ç³»ç»Ÿé…ç½®
    parser.add_argument("--no_wandb", action="store_true", help="ç¦ç”¨W&B")
    parser.add_argument("--debug", action="store_true", help="è°ƒè¯•æ¨¡å¼")
    parser.add_argument("--overwrite", action="store_true", help="è¦†ç›–ç°æœ‰å®éªŒ")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logging(debug=args.debug)
    
    # åŠ è½½é…ç½®
    config = load_config(args.config)
    if args.enable_fsdp:
        config = dataclasses.replace(config, enable_fsdp=True)
    
    logger.info(f"ğŸš€ Starting ACRLPD V2 training: {args.exp_name}")
    logger.info(f"ğŸ“Š Config: {config.name}")
    logger.info(f"ğŸ”§ FSDP: {'enabled' if args.enable_fsdp else 'disabled'}")
    
    # è®¾ç½®å®éªŒç›®å½•
    exp_dir = Path("experiments") / args.exp_name
    if exp_dir.exists() and not args.overwrite:
        raise ValueError(f"Experiment {args.exp_name} exists. Use --overwrite to overwrite.")
    
    exp_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    rng = jax.random.PRNGKey(42)
    
    # åˆ›å»ºAgentï¼ˆæ¥è‡ªagents_v2ï¼‰
    from agents_v2 import create_acrlpd_pi0_agent_from_rl_config
    agent = create_acrlpd_pi0_agent_from_rl_config(config, rng)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    from data_v2 import ACRLPDv2DataLoader
    data_loader = ACRLPDv2DataLoader(config)
    train_iterator = data_loader.create_data_iterator(args.data_path)
    
    # åˆ›å»ºè®­ç»ƒå¾ªç¯ï¼ˆæ¥è‡ªtraining_v2ï¼‰
    from training_v2 import ACRLPDTrainingLoop
    training_loop = ACRLPDTrainingLoop(
        agent=agent,
        data_loader=data_loader,
        enable_fsdp=args.enable_fsdp,
        enable_wandb=not args.no_wandb
    )
    
    # è®¾ç½®W&Bï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if not args.no_wandb:
        setup_wandb(args.exp_name, config)
    
    # ä¸»è®­ç»ƒå¾ªç¯
    logger.info("ğŸ¯ Starting training loop")
    
    for step in range(args.num_steps):
        # è·å–æ‰¹æ¬¡
        batch = next(train_iterator)
        step_rng = jax.random.fold_in(rng, step)
        
        # è®­ç»ƒæ­¥éª¤
        total_loss, loss_info = training_loop.train_step(batch, step_rng)
        
        # è®°å½•æ—¥å¿—
        if step % 100 == 0:
            log_training_metrics(step, total_loss, loss_info)
            
            if not args.no_wandb:
                wandb.log({
                    "step": step,
                    "total_loss": total_loss,
                    **loss_info
                })
        
        # ä¿å­˜checkpoint
        if step % args.save_interval == 0 and step > 0:
            checkpoint_path = exp_dir / "checkpoints" / f"step_{step}"
            
            # ä¿å­˜ACRLPD checkpoint
            training_loop.save_acrlpd_checkpoint(checkpoint_path)
            
            # ä¿å­˜OpenPIå…¼å®¹checkpoint
            training_loop.create_openpi_checkpoint(checkpoint_path / "openpi")
            
            logger.info(f"ğŸ’¾ Saved checkpoint: step {step}")
    
    logger.info("âœ… Training completed successfully")

def load_config(config_name: str):
    """åŠ è½½é¢„å®šä¹‰é…ç½®"""
    from config import (
        ACRLPD_V2_ALOHA_FOLD, 
        ACRLPD_V2_DROID, 
        ACRLPD_V2_LIBERO
    )
    
    config_map = {
        "aloha_fold": ACRLPD_V2_ALOHA_FOLD,
        "droid": ACRLPD_V2_DROID,
        "libero": ACRLPD_V2_LIBERO
    }
    
    if config_name not in config_map:
        raise ValueError(f"Unknown config: {config_name}. Available: {list(config_map.keys())}")
    
    return config_map[config_name]

if __name__ == "__main__":
    main()
```

---

## é˜¶æ®µ 5: æµ‹è¯•å’ŒéªŒè¯ç³»ç»Ÿ (Medium Priority)

### ğŸ¯ å®æ–½çŠ¶æ€ï¼šPENDING

**ç›®æ ‡**: å»ºç«‹å®Œæ•´çš„æµ‹è¯•å’ŒéªŒè¯ç³»ç»Ÿï¼Œç¡®ä¿é‡æ„åçš„ç³»ç»ŸåŠŸèƒ½æ­£ç¡®æ€§å’Œæ€§èƒ½ã€‚

### 5.1 å•å…ƒæµ‹è¯•æ¡†æ¶

**æ–‡ä»¶: `tests/test_agents_v2.py`** (æ–°å»º)
```python
"""
Agents V2 å•å…ƒæµ‹è¯•
"""

import pytest
import jax
import jax.numpy as jnp
from agents_v2 import ACRLPDPi0Agent, create_acrlpd_pi0_agent_from_rl_config
from config import ACRLPD_V2_ALOHA_FOLD

class TestACRLPDPi0Agent:
    
    @pytest.fixture
    def agent(self):
        """åˆ›å»ºæµ‹è¯•Agent"""
        rng = jax.random.PRNGKey(42)
        config = ACRLPD_V2_ALOHA_FOLD
        return create_acrlpd_pi0_agent_from_rl_config(config, rng)
    
    @pytest.fixture
    def test_batch(self):
        """åˆ›å»ºæµ‹è¯•æ‰¹æ¬¡"""
        batch_size = 4
        horizon_length = 20
        action_dim = 14
        
        return {
            'observations': {
                'state': jnp.ones((batch_size, action_dim)),
                'image': {
                    'base_camera': jnp.ones((batch_size, 224, 224, 3)),
                    'wrist_camera': jnp.ones((batch_size, 224, 224, 3))
                }
            },
            'actions': jnp.ones((batch_size, horizon_length, action_dim)),
            'rewards': jnp.ones((batch_size,)),
            'next_observations': {
                'state': jnp.ones((batch_size, action_dim)),
                'image': {
                    'base_camera': jnp.ones((batch_size, 224, 224, 3)),
                    'wrist_camera': jnp.ones((batch_size, 224, 224, 3))
                }
            },
            'masks': jnp.ones((batch_size,))
        }
    
    def test_agent_creation(self, agent):
        """æµ‹è¯•Agentåˆ›å»º"""
        assert isinstance(agent, ACRLPDPi0Agent)
        assert agent.num_action_samples == 4
        assert agent.horizon_length == 20
        assert agent.real_action_dim == 14
    
    def test_loss_computation(self, agent, test_batch):
        """æµ‹è¯•æŸå¤±è®¡ç®—"""
        rng = jax.random.PRNGKey(42)
        
        total_loss, loss_info = agent.compute_loss(test_batch, rng)
        
        # éªŒè¯æŸå¤±å€¼
        assert jnp.isfinite(total_loss)
        assert total_loss > 0
        
        # éªŒè¯æŸå¤±ç»„ä»¶
        assert 'critic_loss' in loss_info
        assert 'actor_loss' in loss_info
        assert 'bc_loss' in loss_info
        
        # éªŒè¯æ•°å€¼ç¨³å®šæ€§
        assert jnp.isfinite(loss_info['critic_loss'])
        assert jnp.isfinite(loss_info['actor_loss'])
        assert jnp.isfinite(loss_info['bc_loss'])
    
    def test_train_step(self, agent, test_batch):
        """æµ‹è¯•è®­ç»ƒæ­¥éª¤"""
        rng = jax.random.PRNGKey(42)
        initial_step = agent.step
        
        updated_agent, loss_info = agent.train_step(test_batch, rng)
        
        # éªŒè¯æ­¥æ•°æ›´æ–°
        assert updated_agent.step == initial_step + 1
        
        # éªŒè¯è¿”å›çš„æŸå¤±ä¿¡æ¯
        assert 'total_loss' in loss_info
        assert jnp.isfinite(loss_info['total_loss'])
    
    def test_openpi_compatibility(self, agent):
        """æµ‹è¯•OpenPIå…¼å®¹æ€§"""
        
        # åˆ›å»ºOpenPIè®­ç»ƒçŠ¶æ€
        openpi_state = agent.create_openpi_train_state()
        
        # éªŒè¯çŠ¶æ€ç»“æ„
        assert hasattr(openpi_state, 'params')
        assert hasattr(openpi_state, 'step')
        assert hasattr(openpi_state, 'model_def')
        
        # éªŒè¯æ¨ç†å…¼å®¹æ€§
        test_obs = create_dummy_observation()
        try:
            model = openpi_state.model_def.apply(openpi_state.params)
            actions = model.sample_actions(jax.random.PRNGKey(42), test_obs)
            assert actions.shape[-1] == 32  # OpenPIæ ‡å‡†åŠ¨ä½œç»´åº¦
        except Exception as e:
            pytest.fail(f"OpenPIå…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")

def create_dummy_observation():
    """åˆ›å»ºæµ‹è¯•è§‚å¯Ÿ"""
    return {
        'state': jnp.ones((1, 14)),
        'image': {
            'base_camera': jnp.ones((1, 224, 224, 3)),
            'wrist_camera': jnp.ones((1, 224, 224, 3))
        }
    }
```

### 5.2 é›†æˆæµ‹è¯•

**æ–‡ä»¶: `tests/test_training_integration.py`** (æ–°å»º)
```python
"""
è®­ç»ƒç³»ç»Ÿé›†æˆæµ‹è¯•
"""

import pytest
import tempfile
from pathlib import Path

class TestTrainingIntegration:
    
    def test_standard_training_pipeline(self):
        """æµ‹è¯•æ ‡å‡†è®­ç»ƒç®¡é“"""
        
        with tempfile.TemporaryDirectory() as tmp_dir:
            # è®¾ç½®é…ç½®
            config = ACRLPD_V2_ALOHA_FOLD
            
            # åˆ›å»ºç»„ä»¶
            rng = jax.random.PRNGKey(42)
            agent = create_acrlpd_pi0_agent_from_rl_config(config, rng)
            
            data_loader = ACRLPDv2DataLoader(config)
            training_loop = ACRLPDTrainingLoop(
                agent=agent,
                data_loader=data_loader,
                enable_fsdp=False
            )
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            test_batch = create_test_batch()
            
            # è¿è¡Œå‡ ä¸ªè®­ç»ƒæ­¥éª¤
            for step in range(5):
                step_rng = jax.random.fold_in(rng, step)
                total_loss, loss_info = training_loop.train_step(test_batch, step_rng)
                
                assert jnp.isfinite(total_loss)
                assert total_loss > 0
            
            # æµ‹è¯•checkpointä¿å­˜
            checkpoint_path = Path(tmp_dir) / "checkpoint"
            training_loop.create_openpi_checkpoint(str(checkpoint_path))
            
            assert checkpoint_path.exists()
    
    @pytest.mark.skipif(len(jax.devices()) < 2, reason="éœ€è¦å¤šGPUè¿›è¡ŒFSDPæµ‹è¯•")
    def test_fsdp_training_pipeline(self):
        """æµ‹è¯•FSDPè®­ç»ƒç®¡é“"""
        
        # è®¾ç½®FSDPé…ç½®
        config = dataclasses.replace(ACRLPD_V2_ALOHA_FOLD, enable_fsdp=True)
        
        # åˆ›å»ºç»„ä»¶
        rng = jax.random.PRNGKey(42)
        agent = create_acrlpd_pi0_agent_from_rl_config(config, rng)
        
        data_loader = ACRLPDv2DataLoader(config)
        training_loop = ACRLPDTrainingLoop(
            agent=agent,
            data_loader=data_loader,
            enable_fsdp=True
        )
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_batch = create_test_batch()
        
        # è¿è¡ŒFSDPè®­ç»ƒæ­¥éª¤
        step_rng = jax.random.PRNGKey(42)
        total_loss, loss_info = training_loop.train_step(test_batch, step_rng)
        
        assert jnp.isfinite(total_loss)
        assert total_loss > 0
```

### 5.3 æ€§èƒ½åŸºå‡†æµ‹è¯•

**æ–‡ä»¶: `tests/benchmark_performance.py`** (æ–°å»º)
```python
"""
æ€§èƒ½åŸºå‡†æµ‹è¯•
"""

import time
import jax

class PerformanceBenchmark:
    
    def benchmark_training_step(self, config, num_steps=100):
        """è®­ç»ƒæ­¥éª¤æ€§èƒ½åŸºå‡†"""
        
        # åˆ›å»ºç»„ä»¶
        rng = jax.random.PRNGKey(42)
        agent = create_acrlpd_pi0_agent_from_rl_config(config, rng)
        test_batch = create_test_batch()
        
        # é¢„çƒ­
        for _ in range(10):
            step_rng = jax.random.fold_in(rng, 0)
            agent.compute_loss(test_batch, step_rng)
        
        # åŒæ­¥è®¾å¤‡
        jax.block_until_ready(jnp.ones(1))
        
        # åŸºå‡†æµ‹è¯•
        start_time = time.time()
        
        for step in range(num_steps):
            step_rng = jax.random.fold_in(rng, step)
            total_loss, _ = agent.compute_loss(test_batch, step_rng)
            jax.block_until_ready(total_loss)
        
        end_time = time.time()
        
        avg_time_per_step = (end_time - start_time) / num_steps
        steps_per_second = 1.0 / avg_time_per_step
        
        return {
            'avg_time_per_step': avg_time_per_step,
            'steps_per_second': steps_per_second,
            'total_time': end_time - start_time
        }
    
    def benchmark_memory_usage(self, config):
        """å†…å­˜ä½¿ç”¨åŸºå‡†"""
        
        # è·å–åˆå§‹å†…å­˜
        initial_memory = get_gpu_memory_usage()
        
        # åˆ›å»ºAgentå’Œæ•°æ®
        rng = jax.random.PRNGKey(42)
        agent = create_acrlpd_pi0_agent_from_rl_config(config, rng)
        test_batch = create_test_batch()
        
        # è¿è¡Œè®­ç»ƒæ­¥éª¤
        step_rng = jax.random.PRNGKey(42)
        total_loss, _ = agent.compute_loss(test_batch, step_rng)
        jax.block_until_ready(total_loss)
        
        # è·å–å³°å€¼å†…å­˜
        peak_memory = get_gpu_memory_usage()
        
        return {
            'initial_memory_mb': initial_memory,
            'peak_memory_mb': peak_memory,
            'memory_increase_mb': peak_memory - initial_memory
        }

def get_gpu_memory_usage() -> float:
    """è·å–GPUå†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
    try:
        import nvidia_ml_py3 as nvml
        nvml.nvmlInit()
        handle = nvml.nvmlDeviceGetHandleByIndex(0)
        info = nvml.nvmlDeviceGetMemoryInfo(handle)
        return info.used / 1024**2  # è½¬æ¢ä¸ºMB
    except:
        return 0.0  # å¦‚æœæ— æ³•è·å–ï¼Œè¿”å›0

if __name__ == "__main__":
    benchmark = PerformanceBenchmark()
    
    # åŸºå‡†æµ‹è¯•é…ç½®
    configs = [
        ("ALOHA_FOLD", ACRLPD_V2_ALOHA_FOLD),
    ]
    
    for name, config in configs:
        print(f"\n=== {name} æ€§èƒ½åŸºå‡† ===")
        
        # è®­ç»ƒé€Ÿåº¦åŸºå‡†
        perf_results = benchmark.benchmark_training_step(config)
        print(f"è®­ç»ƒæ­¥éª¤/ç§’: {perf_results['steps_per_second']:.2f}")
        print(f"å¹³å‡æ¯æ­¥æ—¶é—´: {perf_results['avg_time_per_step']:.3f}s")
        
        # å†…å­˜ä½¿ç”¨åŸºå‡†
        memory_results = benchmark.benchmark_memory_usage(config)
        print(f"å†…å­˜ä½¿ç”¨: {memory_results['peak_memory_mb']:.1f}MB")
        print(f"å†…å­˜å¢é•¿: {memory_results['memory_increase_mb']:.1f}MB")
```

---

## ğŸ”„ å®æ–½æ—¶é—´è¡¨å’ŒéªŒè¯è®¡åˆ’

### å®æ–½ä¼˜å…ˆçº§ï¼š

**ç¬¬1å‘¨ - æ¥å£é‡æ„ (High Priority)**
- [ ] ç»Ÿä¸€agents_v2å’Œtraining_v2çš„çŠ¶æ€æ¥å£
- [ ] é‡æ„è®­ç»ƒå¾ªç¯é€‚é…æ–°Agentæ¶æ„  
- [ ] éªŒè¯æ ‡å‡†è®­ç»ƒç®¡é“æ­£å¸¸å·¥ä½œ

**ç¬¬2å‘¨ - OpenPIå…¼å®¹æ€§ (High Priority)**
- [ ] å®ç°ç³»ç»Ÿçº§OpenPIå…¼å®¹æ€§æ¨¡å—
- [ ] ç»Ÿä¸€checkpointæ ¼å¼å’ŒçŠ¶æ€è½¬æ¢
- [ ] éªŒè¯ä¸OpenPIæ¨ç†æµç¨‹çš„å®Œå…¨å…¼å®¹

**ç¬¬3å‘¨ - FSDPä¼˜åŒ– (Medium Priority)**
- [ ] ä¼˜åŒ–FSDPåˆ†ç‰‡ç­–ç•¥
- [ ] å®ç°å¤šèŠ‚ç‚¹è®­ç»ƒæ”¯æŒ
- [ ] éªŒè¯åˆ†å¸ƒå¼è®­ç»ƒæ€§èƒ½å’Œç¨³å®šæ€§

**ç¬¬4å‘¨ - ç³»ç»Ÿé›†æˆ (Medium Priority)**
- [ ] é‡æ„æ•°æ®åŠ è½½å’Œè„šæœ¬ç³»ç»Ÿ
- [ ] å»ºç«‹å®Œæ•´çš„æµ‹è¯•å’ŒéªŒè¯æ¡†æ¶
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•å’Œä¼˜åŒ–

### éªŒè¯æ£€æŸ¥åˆ—è¡¨ï¼š

**åŠŸèƒ½éªŒè¯ï¼š**
- [ ] agents_v2å’Œtraining_v2ååŒå·¥ä½œæ­£å¸¸
- [ ] FSDPå’Œæ ‡å‡†è®­ç»ƒæ¨¡å¼éƒ½èƒ½æ­£å¸¸è¿è¡Œ
- [ ] OpenPIå…¼å®¹æ€§å®Œå…¨éªŒè¯
- [ ] è®­ç»ƒæ”¶æ•›æ€§ä¸åŸç³»ç»Ÿä¸€è‡´

**æ€§èƒ½éªŒè¯ï¼š**
- [ ] è®­ç»ƒé€Ÿåº¦ä¸ä½äºåŸç³»ç»Ÿ
- [ ] å†…å­˜ä½¿ç”¨ä¼˜åŒ–
- [ ] åˆ†å¸ƒå¼è®­ç»ƒçº¿æ€§æ‰©å±•æ€§

**å…¼å®¹æ€§éªŒè¯ï¼š**
- [ ] ç°æœ‰æ•°æ®é›†ç›´æ¥å¯ç”¨
- [ ] OpenPIæ¨ç†æ— éœ€ä¿®æ”¹  
- [ ] checkpointæ ¼å¼å‘åå…¼å®¹

---

## ğŸ“ˆ é¢„æœŸæ”¶ç›Š

### æ¶æ„æ”¶ç›Šï¼š
1. **ç»Ÿä¸€æ¥å£** - agentså’Œtrainingç³»ç»Ÿæ ‡å‡†åŒ–ååŒ
2. **ç®€åŒ–è®¾è®¡** - æ¶ˆé™¤è¿‡åº¦å·¥ç¨‹åŒ–ï¼Œæ¸…æ™°çš„èŒè´£åˆ†å·¥
3. **å®Œå…¨å…¼å®¹** - OpenPIå’ŒFSDPåœ¨ç³»ç»Ÿå±‚é¢çš„æ— ç¼é›†æˆ
4. **æ‰©å±•æ€§** - æ˜“äºæ·»åŠ æ–°åŠŸèƒ½å’Œä¼˜åŒ–

### æ€§èƒ½æ”¶ç›Šï¼š
1. **å†…å­˜ä¼˜åŒ–** - FSDPåˆ†ç‰‡ç­–ç•¥å’Œæ¢¯åº¦ç´¯ç§¯ä¼˜åŒ–
2. **è®­ç»ƒåŠ é€Ÿ** - å¤šèŠ‚ç‚¹æ‰©å±•å’Œå†…å­˜ç®¡ç†ä¼˜åŒ–
3. **ç³»ç»Ÿç¨³å®šæ€§** - ç»Ÿä¸€çŠ¶æ€ç®¡ç†å’Œé”™è¯¯å¤„ç†

### å¼€å‘æ”¶ç›Šï¼š
1. **å¯ç»´æŠ¤æ€§** - æ¸…æ™°çš„æ¨¡å—è¾¹ç•Œå’Œæ¥å£
2. **å¯æµ‹è¯•æ€§** - å®Œæ•´çš„å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•
3. **è°ƒè¯•å‹å¥½** - ç»Ÿä¸€æ—¥å¿—å’Œæ€§èƒ½ç›‘æ§

é‡æ„åçš„ac_training_v2å°†æˆä¸ºä¸€ä¸ª**ç°ä»£åŒ–ã€é«˜æ€§èƒ½ã€å®Œå…¨å…¼å®¹**çš„ACRLPDè®­ç»ƒç³»ç»Ÿï¼Œä¸ºæœªæ¥çš„æ‰©å±•å’Œä¼˜åŒ–å¥ å®šåšå®åŸºç¡€ã€‚