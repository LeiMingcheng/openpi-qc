"""
ç»Ÿä¸€çš„ACRLPDè®­ç»ƒé…ç½®ç³»ç»Ÿ

å®Œå…¨åŸºäºOpenPI TrainConfigæ¶æ„çš„Q-chunkingå¼ºåŒ–å­¦ä¹ é…ç½®ç³»ç»Ÿã€‚
æ¶ˆé™¤äº†æ‰€æœ‰å‘½åå†²çªå’Œæ¶æ„å…¼å®¹æ€§é—®é¢˜ã€‚

ä¸»è¦ç‰¹æ€§ï¼š
- ç»§æ‰¿OpenPI TrainConfigï¼Œå®Œå…¨å…¼å®¹ç°æœ‰åŸºç¡€è®¾æ–½
- é›†æˆQ-chunking RLä¸“ç”¨å‚æ•°ï¼ˆACRLPDã€Q-chunkingï¼‰
- ç®€æ´ç»Ÿä¸€çš„é…ç½®æ¥å£
- æ”¯æŒÏ€â‚€æ¨¡å‹ä¸Q-chunking RLçš„å®Œæ•´é›†æˆ
"""

import dataclasses
from typing import Any, Dict, Optional, Tuple, Union

import tyro
import jax
import jax.numpy as jnp

# OpenPIæ ¸å¿ƒå¯¼å…¥
import openpi.models.model as _model
import openpi.models.pi0 as pi0
import openpi.models.pi0_fast as pi0_fast
import openpi.training.config as openpi_config
import openpi.training.optimizer as _optimizer
import openpi.training.weight_loaders as weight_loaders
import openpi.transforms as _transforms
# normalizeå¯¼å…¥å·²ç§»é™¤ï¼Œä½¿ç”¨OpenPIåŸç”Ÿassetsæœºåˆ¶è‡ªåŠ¨åŠ è½½norm_stats
# aloha_policyä¸éœ€è¦æ‰‹åŠ¨importï¼ŒLeRobotAlohaDataConfigä¼šè‡ªåŠ¨å¤„ç†


# ===============================================================================
# Q-chunking RLç®—æ³•å‚æ•°é…ç½®
# ===============================================================================

@dataclasses.dataclass(frozen=True)
class ACRLPDHyperparams:
    """ACRLPDç®—æ³•è¶…å‚æ•°é…ç½®"""
    
    # Q-learningæ ¸å¿ƒå‚æ•°
    discount: float = 0.99
    target_update_rate: float = 0.005  # Targetç½‘ç»œè½¯æ›´æ–°ç‡
    
    # Criticç½‘ç»œå‚æ•°
    critic_lr: float = 3e-4
    critic_hidden_dims: Tuple[int, ...] = (256, 256, 256)
    num_critics: int = 2  # Critic ensembleå¤§å°
    
    # Actor (Ï€â‚€) å‚æ•°
    actor_lr: float = 1e-4
    actor_update_freq: int = 2  # Actoræ›´æ–°é¢‘ç‡
    
    # Best-of-Né‡‡æ ·å‚æ•°
    num_action_samples: int = 32  # å€™é€‰åŠ¨ä½œæ•°é‡
    action_sampling_temperature: float = 1.0
    
    # Behavior Cloningæ­£åˆ™åŒ–
    bc_loss_weight: float = 0.1
    
    # Q-chunkingå‚æ•°
    chunk_length: int = 5  # åŠ¨ä½œåºåˆ—é•¿åº¦
    bootstrap_length: int = 5  # Bootstrapç›®æ ‡é•¿åº¦
    
    # è®­ç»ƒåŠ¨æ€å‚æ•°
    batch_size: int = 256
    utd_ratio: int = 1  # Update-to-dataæ¯”ç‡
    
    # ACRLPDPi0Configæ˜ å°„çš„é¢å¤–å‚æ•°
    q_aggregation: str = "min"  # Q-value aggregation method
    target_update_tau: float = 0.005  # Target network soft update rate
    
    # EMAå‚æ•°
    use_ema: bool = True
    pi0_ema_decay: float = 0.999
    critic_ema_decay: float = 0.99
    use_ema_for_inference: bool = True
    
    # é‡‡æ ·å‚æ•°
    diffusion_steps: int = 10
    use_best_of_n: bool = True
    
    # æ¸©åº¦æ§åˆ¶å‚æ•°
    use_adaptive_temperature: bool = True
    target_entropy_multiplier: float = 0.5
    
    # è®­ç»ƒé˜¶æ®µå‚æ•° (æ³¨æ„ï¼šç°åœ¨ä½¿ç”¨RLTrainConfig.num_train_stepsä½œä¸ºæ€»æ­¥æ•°)
    eval_frequency: int = 10000
    save_frequency: int = 50000
    
    # ğŸ”§ æ¢¯åº¦ç§¯ç´¯é…ç½®ï¼ˆå®Œæ•´åŠŸèƒ½ï¼Œç”¨æˆ·è¦æ±‚ä¸ç®€åŒ–ï¼‰
    gradient_accumulation_steps: int = 4  # æ¢¯åº¦ç§¯ç´¯æ­¥æ•°ï¼Œæœ‰æ•ˆbatch_size = batch_size Ã— gradient_accumulation_steps
    max_grad_norm: float = 1.0           # æ¢¯åº¦è£å‰ªé˜ˆå€¼
    enable_gradient_accumulation: bool = True  # å¯ç”¨æ¢¯åº¦ç§¯ç´¯
    
    # ğŸ”§ æ•°æ®é‡‡æ ·æ¯”ä¾‹æ§åˆ¶ï¼ˆæ–°å¢ï¼šæ­£è´Ÿæ ·æœ¬episodeæ¯”ä¾‹æ§åˆ¶ï¼‰
    positive_episode_ratio: float = 0.4  # å†…å­˜æ± ä¸­æ­£æ ·æœ¬episodeçš„æ¯”ä¾‹ï¼ˆé»˜è®¤60%ï¼‰
    enable_reward_balanced_episodes: bool = True  # å¯ç”¨episodeçº§åˆ«çš„æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹æ§åˆ¶
    
    # ğŸ”§ æ‰¹é‡é‡‡æ ·æ¯”ä¾‹æ§åˆ¶ï¼ˆæ–°å¢ï¼‰
    positive_batch_ratio: float = 0.5  # batchä¸­æ­£æ ·æœ¬çš„æ¯”ä¾‹ï¼ˆé»˜è®¤50%ï¼‰
    enable_batch_ratio_control: bool = True  # å¯ç”¨batchçº§åˆ«æ¯”ä¾‹æ§åˆ¶
    
    # ğŸ”§ Epochå’Œå­¦ä¹ ç‡è°ƒèŠ‚é…ç½®
    enable_epoch_based_lr_schedule: bool = False  # ä½¿ç”¨OpenPIæ ‡å‡†å­¦ä¹ ç‡æ§åˆ¶ï¼Œä¸éœ€è¦å¤æ‚åŒå±‚è°ƒèŠ‚
    lr_decay_strategy: str = "cosine"             # å­¦ä¹ ç‡è¡°å‡ç­–ç•¥: "cosine", "step", "exp"
    lr_decay_factor: float = 0.95                 # å­¦ä¹ ç‡è¡°å‡å› å­ï¼ˆepoché—´ï¼‰
    lr_min_factor: float = 0.1                    # æœ€å°å­¦ä¹ ç‡å› å­ï¼ˆç›¸å¯¹åˆå§‹å­¦ä¹ ç‡ï¼‰
    warmup_epochs: int = 2                        # å­¦ä¹ ç‡é¢„çƒ­epochæ•°
    total_epochs: int = 100                       # æ€»epochæ•°ï¼ˆç”¨äºcosineè¡°å‡è®¡ç®—ï¼‰
    
    # Epochå†…å­¦ä¹ ç‡è°ƒèŠ‚ï¼ˆé…åˆåŸºäºstepçš„è°ƒèŠ‚ï¼‰
    intra_epoch_lr_decay: bool = True             # å¯ç”¨epochå†…å­¦ä¹ ç‡è¡°å‡
    intra_epoch_strategy: str = "cosine"          # Epochå†…è¡°å‡ç­–ç•¥: "cosine", "linear", "exp"
    steps_per_epoch: int = 10000                  # æ¯ä¸ªepochçš„é¢„æœŸæ­¥æ•°
    intra_epoch_min_factor: float = 0.9           # Epochå†…æœ€å°å­¦ä¹ ç‡å› å­
    lr_absolute_min: float = 1e-7                 # å­¦ä¹ ç‡ç»å¯¹ä¸‹é™


@dataclasses.dataclass(frozen=True)
class QChunkingConfig:
    """Q-chunkingç‰¹å®šé…ç½®"""
    
    # åŠ¨ä½œåºåˆ—å‚æ•°
    horizon_length: int = 5
    action_dim: int = 14  # æœºå™¨äººåŠ¨ä½œç»´åº¦
    
    # åºåˆ—ç”Ÿæˆå‚æ•°
    discount: float = 0.99
    reward_scale: float = 1.0
    use_sparse_rewards: bool = True
    terminal_reward: float = 1.0
    
    # Bootstrapå’Œæ©ç å‚æ•°
    bootstrap_type: str = "standard"  # "standard", "n_step", "lambda"
    mask_invalid_actions: bool = True
    
    # Episodeè¾¹ç•Œå¤„ç†
    pad_episodes: bool = True
    episode_timeout_penalty: float = 0.0


# ===============================================================================
# ç»Ÿä¸€è®­ç»ƒé…ç½®ç³»ç»Ÿ
# ===============================================================================

@dataclasses.dataclass(frozen=True)
class RLTrainConfig(openpi_config.TrainConfig):
    """
    ç»Ÿä¸€çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒé…ç½®
    
    ç»§æ‰¿OpenPI TrainConfigï¼Œæ·»åŠ Q-chunking RLä¸“ç”¨å‚æ•°ã€‚
    å®Œå…¨å…¼å®¹OpenPIåŸºç¡€è®¾æ–½ï¼Œæ¶ˆé™¤æ‰€æœ‰é…ç½®å†²çªã€‚
    """
    
    # === Q-chunking RLä¸“ç”¨å‚æ•° ===
    acrlpd: ACRLPDHyperparams = dataclasses.field(default_factory=ACRLPDHyperparams)
    qchunking: QChunkingConfig = dataclasses.field(default_factory=QChunkingConfig)
    
    # === æ•°æ®åŠ è½½å‚æ•° ===
    episodes_per_memory_pool: int = 32  # ğŸš€ ä¼˜åŒ–ï¼šä»2å¢åŠ åˆ°8ï¼Œå‡å°‘é¢‘ç¹é‡è½½å¼€é”€
    
    # === å¼‚æ­¥åŠ è½½å‚æ•° ===
    async_loading_enabled: bool = True          # å¯ç”¨å¼‚æ­¥åŠ è½½
    async_trigger_ratio: float = 0.6            # åœ¨epochå¤šå°‘è¿›åº¦æ—¶å¼€å§‹é¢„åŠ è½½ï¼ˆ75%ï¼‰
    
    # === æ€§èƒ½åˆ†æå‚æ•° ===
    enable_perf_analysis: bool = False  # æ˜¯å¦å¯ç”¨è¯¦ç»†æ€§èƒ½åˆ†æ
    
    # === RLä¸“ç”¨ä¼˜åŒ–å™¨é…ç½® ===
    # è¦†ç›–åŸºç±»çš„å•ä¸€optimizerï¼Œæ”¯æŒActor-CriticåŒä¼˜åŒ–å™¨
    critic_optimizer: _optimizer.OptimizerConfig = dataclasses.field(
        default_factory=lambda: _optimizer.AdamW(weight_decay=1e-5)
    )
    actor_optimizer: _optimizer.OptimizerConfig = dataclasses.field(
        default_factory=lambda: _optimizer.AdamW(weight_decay=1e-6)
    )
    
    # === RLä¸“ç”¨å­¦ä¹ ç‡è°ƒåº¦ï¼ˆä¸Epoch-basedå…¼å®¹ï¼‰ ===
    critic_lr_schedule: _optimizer.LRScheduleConfig = dataclasses.field(
        default_factory=lambda: _optimizer.CosineDecaySchedule(
            warmup_steps=20000, peak_lr=3e-4, decay_steps=200_000  # åŒ¹é…epochè®¾ç½®ï¼š2 epochs warmup, 200k total steps
        )
    )
    actor_lr_schedule: _optimizer.LRScheduleConfig = dataclasses.field(
        default_factory=lambda: _optimizer.CosineDecaySchedule(
            warmup_steps=20000, peak_lr=1e-4, decay_steps=200_000   # åŒ¹é…epochè®¾ç½®ï¼š2 epochs warmup, 200k total steps
        )
    )
    
    # === Ï€â‚€æ¨¡å‹ä¸“ç”¨æƒé‡åŠ è½½å™¨ ===
    pi0_weight_loader: weight_loaders.WeightLoader = dataclasses.field(
        default_factory=lambda: weight_loaders.CheckpointWeightLoader(
            "gs://openpi-assets/checkpoints/pi0_base/params"
        )
    )
    
    def get_effective_actor_lr_schedule(self) -> _optimizer.LRScheduleConfig:
        """è·å–æœ‰æ•ˆçš„Actorå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ŒåŸºäºacrlpdä¸­çš„é™æ€å­¦ä¹ ç‡"""
        return _optimizer.CosineDecaySchedule(
            warmup_steps=2000,
            peak_lr=self.acrlpd.actor_lr,  # ä½¿ç”¨acrlpdä¸­çš„å­¦ä¹ ç‡ä½œä¸ºpeak_lr
            decay_steps=200_000
        )

    def get_effective_critic_lr_schedule(self) -> _optimizer.LRScheduleConfig:
        """è·å–æœ‰æ•ˆçš„Criticå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ŒåŸºäºacrlpdä¸­çš„é™æ€å­¦ä¹ ç‡"""
        return _optimizer.CosineDecaySchedule(
            warmup_steps=2000,
            peak_lr=self.acrlpd.critic_lr,  # ä½¿ç”¨acrlpdä¸­çš„å­¦ä¹ ç‡ä½œä¸ºpeak_lr
            decay_steps=200_000
        )

    def validate_rl_config(self):
        """Q-chunking RLä¸“ç”¨é…ç½®éªŒè¯"""
        # è°ƒç”¨åŸºç±»éªŒè¯
        super().__post_init__()
        
        # éªŒè¯åŠ¨ä½œç»´åº¦é…ç½®åˆç†æ€§
        # æ³¨æ„ï¼šqchunking.action_dimæ˜¯çœŸå®æœºå™¨äººåŠ¨ä½œç»´åº¦
        # model.action_dimæ˜¯æ¨¡å‹å†…éƒ¨å¤„ç†ç»´åº¦ï¼Œä¸¤è€…å¯ä»¥ä¸åŒ
        # OpenPIçš„transformsä¼šè‡ªåŠ¨å¤„ç†ç»´åº¦è½¬æ¢
        if hasattr(self.model, 'action_dim'):
            # ç¡®ä¿ç»´åº¦éƒ½æ˜¯æ­£æ•°
            assert self.qchunking.action_dim > 0, f"QChunking action_dim must be positive: {self.qchunking.action_dim}"
            assert self.model.action_dim > 0, f"Model action_dim must be positive: {self.model.action_dim}"
        
        # éªŒè¯Q-chunkingå‚æ•°
        assert self.qchunking.horizon_length > 0, "Horizon length must be positive"
        assert 0 < self.qchunking.discount <= 1.0, "Discount must be in (0, 1]"
        
        # éªŒè¯ACRLPDå‚æ•°
        assert self.batch_size > 0, "Batch size must be positive"  # ä½¿ç”¨ä¸»é…ç½®çš„batch_size
        assert self.acrlpd.num_action_samples > 0, "Action samples must be positive"
        
        # FSDPå…¼å®¹æ€§æ£€æŸ¥ï¼šbatch_sizeå¿…é¡»èƒ½è¢«è®¾å¤‡æ•°é‡æ•´é™¤
        if hasattr(self, 'fsdp_devices') and self.fsdp_devices > 1:
            if self.batch_size % self.fsdp_devices != 0:
                suggested_batch_size = ((self.batch_size // self.fsdp_devices) + 1) * self.fsdp_devices
                raise ValueError(
                    f"FSDPè¦æ±‚batch_size ({self.batch_size}) èƒ½è¢«è®¾å¤‡æ•°é‡ ({self.fsdp_devices}) æ•´é™¤ã€‚"
                    f"å»ºè®®ä½¿ç”¨ batch_size={suggested_batch_size}"
                )


# ===============================================================================
# é¢„å®šä¹‰é…ç½®
# ===============================================================================

# ALOHAæŠ˜å ä»»åŠ¡é…ç½®
RL_ALOHA_FOLD = RLTrainConfig(
    name="rl_aloha_fold",
    project_name="acrlpd_pi0",
    
    # Ï€â‚€æ¨¡å‹é…ç½® - ä½¿ç”¨OpenPIé»˜è®¤32ç»´ä»¥ä¿æŒé¢„è®­ç»ƒæƒé‡å…¼å®¹
    model=pi0.Pi0Config(
        action_horizon=50  # AlohaInputsä¼šè‡ªåŠ¨å¤„ç†14â†’32ç»´è½¬æ¢
    ),
    weight_loader=weight_loaders.CheckpointWeightLoader(
        "gs://openpi-assets/checkpoints/pi0_base/params"
    ),
    
    # æ•°æ®é…ç½®
    data=openpi_config.LeRobotAlohaDataConfig(
        repo_id="aloha_fold",
        default_prompt="fold the clothes on the table",
        adapt_to_pi=False,
        assets=openpi_config.AssetsConfig(asset_id="aloha_fold")
    ),
    
    # åŸºç¡€è®­ç»ƒå‚æ•°
    batch_size=256,
    num_train_steps=50_000,
    log_interval=100,
    save_interval=5000,
    
    # Q-chunking RLå‚æ•°
    acrlpd=ACRLPDHyperparams(
        chunk_length=5,
        num_action_samples=32,
        bc_loss_weight=0.1,
        batch_size=256
    ),
    qchunking=QChunkingConfig(
        horizon_length=5,
        action_dim=14
    )
)

# Liberoä»¿çœŸé…ç½®
RL_LIBERO = RLTrainConfig(
    name="rl_libero",
    project_name="acrlpd_pi0_libero",
    
    # Ï€â‚€æ¨¡å‹é…ç½®
    model=pi0.Pi0Config(
        action_dim=7, 
        action_horizon=5
    ),
    weight_loader=weight_loaders.CheckpointWeightLoader(
        "gs://openpi-assets/checkpoints/pi0_base/params"
    ),
    
    # æ•°æ®é…ç½®
    data=openpi_config.LeRobotLiberoDataConfig(
        repo_id="physical-intelligence/libero",
        base_config=openpi_config.DataConfig(prompt_from_task=True)
    ),
    
    # ä»¿çœŸè®­ç»ƒå‚æ•°
    batch_size=256,
    num_train_steps=100_000,
    log_interval=100,
    save_interval=5000,
    
    # Q-chunking RLå‚æ•°ï¼ˆé’ˆå¯¹ä»¿çœŸä¼˜åŒ–ï¼‰
    acrlpd=ACRLPDHyperparams(
        chunk_length=5,
        num_action_samples=64,  # ä»¿çœŸä¸­ä½¿ç”¨æ›´å¤šé‡‡æ ·
        bc_loss_weight=0.1,
        critic_lr=1e-3,  # æ›´æ¿€è¿›çš„å­¦ä¹ ç‡
        batch_size=256
    ),
    qchunking=QChunkingConfig(
        horizon_length=5,
        action_dim=7
    )
)

# Fold Boxé…ç½® - ç¬¬ä¸€é˜¶æ®µä¼˜åŒ–ï¼šæ¢¯åº¦ç´¯ç§¯ + æ¿€è¿›FSDP + æ¢å¤è®­ç»ƒè´¨é‡å‚æ•°
RL_FOLD_BOX = RLTrainConfig(
    name="rl_fold_box",
    project_name="acrlpd_pi0_fold_box",
    
    # Ï€â‚€æ¨¡å‹é…ç½® - ä½¿ç”¨OpenPIé»˜è®¤32ç»´ä»¥ä¿æŒé¢„è®­ç»ƒæƒé‡å…¼å®¹
    model=pi0.Pi0Config(
        action_horizon=20,  # AlohaInputsä¼šè‡ªåŠ¨å¤„ç†14â†’32ç»´padding
        dtype="bfloat16"    # é‡è¦ï¼šä½¿ç”¨bfloat16å‡å°‘å†…å­˜ä½¿ç”¨
    ),
    # ä¿®å¤æƒé‡åŠ è½½è·¯å¾„ - ä½¿ç”¨æ­£ç¡®çš„å®Œæ•´æ£€æŸ¥ç‚¹
    weight_loader=weight_loaders.CheckpointWeightLoader(
        #"/dev/shm/lmc/openpi/checkpoints/pi0_base/openpi-assets/checkpoints/pi0_base/pi0_base/params"
        "/era-ai/lm/weight/pi0/pi0_dual_box_full/yzy_fold_box/90000/params/"
    ),
    
    # æ•°æ®é…ç½® - ä½¿ç”¨OpenPIæ ‡å‡†ALOHAé…ç½®
    data=openpi_config.LeRobotAlohaDataConfig(
        repo_id="fold_box_unified",  # ä½¿ç”¨æ–°è½¬æ¢çš„fold_boxæ•°æ®é›†
        #repo_id="aloha_test_dataset",
        default_prompt="fold the box",  # ä¸ç”¨æˆ·é…ç½®ä¸€è‡´
        adapt_to_pi=False,  # ä¸ç”¨æˆ·é…ç½®ä¸€è‡´ï¼šä½¿ç”¨æ ‡å‡†ALOHAæ•°æ®ç©ºé—´
        assets=openpi_config.AssetsConfig(
            assets_dir="/era-ai/lm/weight/pi0/pi0_dual_box_full/yzy_fold_box/90000/assets/yzysmile",
            asset_id="aloha_fold_box"
        ),
        
        # è‡ªå®šä¹‰repack transformsç”¨äºç›¸æœºæ˜ å°„
        # å…³é”®ä¿®å¤ï¼šå°†æˆ‘ä»¬çš„ç›¸æœºåç§°æ˜ å°„åˆ°AlohaInputsæœŸæœ›çš„æ ‡å‡†åç§°
        repack_transforms=_transforms.Group(
            inputs=[
                _transforms.RepackTransform(
                    {
                        "images": {
                            "cam_high": "observation.images.cam_high",
                            "cam_left_wrist": "observation.images.cam_left_wrist", 
                            "cam_right_wrist": "observation.images.cam_right_wrist",
                        },
                        "state": "observation.state",
                        "actions": "action",
                        "reward": "reward",  # æ·»åŠ rewardå­—æ®µæ˜ å°„ï¼Œç¡®ä¿RLæ•°æ®å®Œæ•´æ€§
                    }
                )
            ]
        ),
        # ä¸ä¼ é€’data_transforms - è®©LeRobotAlohaDataConfigè‡ªåŠ¨åˆ›å»º
        # å®ƒä¼šè‡ªåŠ¨åˆ›å»ºï¼šAlohaInputs(action_dim=14, adapt_to_pi=False) + AlohaOutputs
        base_config=openpi_config.DataConfig(
            prompt_from_task=False  # ä¸ç”¨æˆ·é…ç½®ä¸€è‡´ï¼šä¸æ ¹æ®LeRobotæ•°æ®é›†åç§°ç»™è¯­è¨€æŒ‡ä»¤
            # norm_statsç”±AssetsConfigçš„assets_dirè‡ªåŠ¨åŠ è½½ï¼Œæ— éœ€æ‰‹åŠ¨é…ç½®
        )
    ),
    
    # è®­ç»ƒå‚æ•° - ç¬¬ä¸€é˜¶æ®µæ¢¯åº¦ç´¯ç§¯ç‰ˆæœ¬
    batch_size=32,   # ç‰©ç†batch_sizeä¿æŒå°ä»¥èŠ‚çœç¬æ—¶å†…å­˜
    num_train_steps=20000,  # ä¸ç”¨æˆ·é…ç½®ä¸€è‡´
    log_interval=2,
    save_interval=1000,
    
    # å¤šGPUé…ç½® - æœ€å¤§åŒ–FSDPåˆ†ç‰‡å‡å°‘å†…å­˜
    fsdp_devices=8,  # æœ€å¤§åˆ†ç‰‡ï¼š(1,8) = 1ä¸ªæ•°æ®å¹¶è¡Œç»„ Ã— 8è®¾å¤‡FSDPåˆ†ç‰‡
    
    # Q-chunking RLå‚æ•°ï¼ˆç¬¬ä¸€é˜¶æ®µï¼šæ¢å¤å…³é”®è®­ç»ƒè´¨é‡å‚æ•°ï¼‰
    acrlpd=ACRLPDHyperparams(
        # åŸºæœ¬RLå‚æ•°
        discount=0.99,
        target_update_rate=0.005,
        q_aggregation="min",
        target_update_tau=0.005,
        
        # Criticç½‘ç»œå‚æ•°ï¼ˆç¬¬ä¸€é˜¶æ®µæ¢å¤ï¼‰
        critic_lr=3e-5,  # ä¿å®ˆçš„å­¦ä¹ ç‡ï¼Œé€‚åˆçœŸå®æœºå™¨äººæ•°æ®
        critic_hidden_dims=(512, 512, 512),  # ç¬¬ä¸€é˜¶æ®µç›®æ ‡ï¼šéƒ¨åˆ†æ¢å¤åˆ°(192,192)
        num_critics=6,  # ç¬¬ä¸€é˜¶æ®µç›®æ ‡ï¼šæ¢å¤ensembleåˆ°2
        
        # Actor (Ï€â‚€) å‚æ•°
        actor_lr=1e-5,
        actor_update_freq=4,
        
        # Best-of-Né‡‡æ ·å‚æ•°ï¼ˆç¬¬ä¸€é˜¶æ®µéƒ¨åˆ†æ¢å¤ï¼‰
        num_action_samples=4,  # ç¬¬ä¸€é˜¶æ®µç›®æ ‡ï¼šä»1æ¢å¤åˆ°8
        action_sampling_temperature=1.0,
        
        # Behavior Cloningæ­£åˆ™åŒ–
        bc_loss_weight=200,  # é€‚ä¸­çš„BCæƒé‡ï¼Œé€‚åˆmanipulationä»»åŠ¡
        
        # Q-chunkingå‚æ•°ï¼ˆä¸action_horizonåŒ¹é…ï¼‰
        chunk_length=20,  # ä¸action_horizonåŒ¹é…ï¼šæ¢å¤åˆ°20
        bootstrap_length=20,
        
        # è®­ç»ƒåŠ¨æ€å‚æ•°ï¼ˆæ¢¯åº¦ç´¯ç§¯ä¼˜åŒ–ï¼‰
        batch_size=32,   # ä¸ä¸»é…ç½®RLTrainConfig.batch_sizeä¿æŒä¸€è‡´
        utd_ratio=1,
        
        # EMAå‚æ•°
        use_ema=True,
        pi0_ema_decay=0.999,
        critic_ema_decay=0.99,
        use_ema_for_inference=True,
        
        # é‡‡æ ·å‚æ•°
        diffusion_steps=10,
        use_best_of_n=True,
        
        # æ¸©åº¦æ§åˆ¶å‚æ•°ï¼ˆç¬¬ä¸€é˜¶æ®µä»ç¦ç”¨ä»¥èŠ‚çœå†…å­˜ï¼‰
        use_adaptive_temperature=False,  # ç¬¬ä¸€é˜¶æ®µä»ç¦ç”¨ç†µä¼°è®¡
        target_entropy_multiplier=0.5,
        
        # è®­ç»ƒé˜¶æ®µå‚æ•° (ä½¿ç”¨num_train_steps=200000ä½œä¸ºæ€»æ­¥æ•°)
        eval_frequency=10000,
        save_frequency=1000,
        
        # ğŸ”§ æ¢¯åº¦ç§¯ç´¯é…ç½®ï¼ˆç¬¬ä¸€é˜¶æ®µä¼˜åŒ–ï¼š4æ­¥ç§¯ç´¯ï¼Œæœ‰æ•ˆbatch_size=8Ã—4=32ï¼‰
        gradient_accumulation_steps=4,  # æ¢¯åº¦ç§¯ç´¯æ­¥æ•°ï¼Œæå‡è®­ç»ƒæ•ˆç‡
        max_grad_norm=0.1,             # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        enable_gradient_accumulation=False,  # å¯ç”¨æ¢¯åº¦ç§¯ç´¯
        
        # ğŸ”§ Epochå’Œå­¦ä¹ ç‡è°ƒèŠ‚é…ç½®ï¼ˆåº”ç”¨ä¿®æ”¹ï¼‰
        enable_epoch_based_lr_schedule=False,  # ä½¿ç”¨OpenPIæ ‡å‡†å­¦ä¹ ç‡æ§åˆ¶
        lr_decay_strategy="cosine",            # å­¦ä¹ ç‡è¡°å‡ç­–ç•¥: cosine
        lr_decay_factor=0.95,                  # å­¦ä¹ ç‡è¡°å‡å› å­ï¼ˆepoché—´ï¼‰
        lr_min_factor=0.1,                     # æœ€å°å­¦ä¹ ç‡å› å­ï¼ˆç›¸å¯¹åˆå§‹å­¦ä¹ ç‡ï¼‰
        warmup_epochs=2,                       # å­¦ä¹ ç‡é¢„çƒ­epochæ•°
        total_epochs=100,                       # æ€»epochæ•°
        
        # Epochå†…å­¦ä¹ ç‡è°ƒèŠ‚ï¼ˆåº”ç”¨cosineè°ƒèŠ‚ï¼‰
        intra_epoch_lr_decay=True,             # å¯ç”¨epochå†…å­¦ä¹ ç‡è¡°å‡
        intra_epoch_strategy="cosine",         # Epochå†…è¡°å‡ç­–ç•¥: cosine
        steps_per_epoch=200,                 # æ¯ä¸ªepochçš„æ­¥æ•°
        intra_epoch_min_factor=0.9,            # Epochå†…æœ€å°å­¦ä¹ ç‡å› å­
        lr_absolute_min=3e-8                   # å­¦ä¹ ç‡ç»å¯¹ä¸‹é™
    ),
    qchunking=QChunkingConfig(
        horizon_length=20,  # ä¸action_horizonåŒ¹é…ï¼šæ¢å¤åˆ°20
        action_dim=14  # ALOHAåŒè‡‚æœºå™¨äºº
    ),
    
    # === å¼‚æ­¥åŠ è½½å‚æ•° ===
    async_loading_enabled=True,          # å¯ç”¨å¼‚æ­¥åŠ è½½
    async_trigger_ratio=0.7             # åœ¨epochå¤šå°‘è¿›åº¦æ—¶å¼€å§‹é¢„åŠ è½½ï¼ˆ75%ï¼‰
)

# DROIDæ•°æ®é›†é…ç½®
RL_DROID = RLTrainConfig(
    name="rl_droid",
    project_name="acrlpd_pi0_droid",
    
    # Ï€â‚€-FASTæ¨¡å‹é…ç½®ï¼ˆæ›´é€‚åˆå¤§è§„æ¨¡æ•°æ®ï¼‰
    model=pi0_fast.Pi0FASTConfig(
        action_dim=8, 
        action_horizon=10,
        max_token_len=180
    ),
    weight_loader=weight_loaders.CheckpointWeightLoader(
        "gs://openpi-assets/checkpoints/pi0_fast_base/params"
    ),
    
    # DROIDæ•°æ®é…ç½®
    data=openpi_config.SimpleDataConfig(
        assets=openpi_config.AssetsConfig(asset_id="droid"),
        data_transforms=lambda model: _transforms.Group(
            inputs=[],  # å°†ç”±DROID-specific transformså¡«å……
            outputs=[]
        ),
        base_config=openpi_config.DataConfig(prompt_from_task=True)
    ),
    
    # é’ˆå¯¹fold boxä»»åŠ¡çš„è®­ç»ƒå‚æ•°
    batch_size=32,  # å°æ‰¹æ¬¡ä»¥é€‚åº”å†…å­˜å’Œå¿«é€Ÿæµ‹è¯•
    num_train_steps=200_000,
    log_interval=200,
    save_interval=10_000,
    
    # Q-chunking RLå‚æ•°ï¼ˆé’ˆå¯¹ALOHA fold boxä»»åŠ¡ä¼˜åŒ–ï¼‰
    acrlpd=ACRLPDHyperparams(
        chunk_length=20,  # ä¸action_horizonä¿æŒä¸€è‡´
        num_action_samples=16,  # å‡å°‘é‡‡æ ·æ•°é‡ä»¥æé«˜é€Ÿåº¦
        bc_loss_weight=0.01,  # æ›´ä½çš„BCæƒé‡
        batch_size=32  # ä¸å¤–éƒ¨batch_sizeä¿æŒä¸€è‡´
    ),
    qchunking=QChunkingConfig(
        horizon_length=10,  # ä¸model.action_horizonä¿æŒä¸€è‡´ï¼š20â†’10
        action_dim=14       # ALOHAåŒè‡‚æœºå™¨äººçš„çœŸå®åŠ¨ä½œç»´åº¦
    )
)


# ===============================================================================
# é…ç½®æ³¨å†Œå’Œç®¡ç†
# ===============================================================================

_CONFIGS = {
    "rl_aloha_fold": RL_ALOHA_FOLD,
    "rl_fold_box": RL_FOLD_BOX,
    "rl_libero": RL_LIBERO,
    "rl_droid": RL_DROID,
}


def get_config(config_name: str) -> RLTrainConfig:
    """è·å–ç»Ÿä¸€çš„RLè®­ç»ƒé…ç½®"""
    if config_name not in _CONFIGS:
        available = ", ".join(_CONFIGS.keys())
        raise ValueError(f"Config '{config_name}' not found. Available: {available}")
    
    config = _CONFIGS[config_name]
    config.validate_rl_config()
    return config


def list_configs() -> Dict[str, str]:
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„é…ç½®"""
    return {name: "RLTrainConfig (ç»Ÿä¸€é…ç½®)" for name in _CONFIGS.keys()}


def cli() -> RLTrainConfig:
    """å‘½ä»¤è¡Œé…ç½®é€‰æ‹©æ¥å£"""
    return tyro.extras.overridable_config_cli(
        {k: (k, v) for k, v in _CONFIGS.items()}
    )


# ===============================================================================
# æµ‹è¯•å’ŒéªŒè¯
# ===============================================================================

if __name__ == "__main__":
    print("ğŸ”§ æµ‹è¯•ç»Ÿä¸€ACRLPDé…ç½®ç³»ç»Ÿ...")
    
    print(f"\nğŸ“‹ å¯ç”¨é…ç½®: {len(_CONFIGS)} ä¸ª")
    config_info = list_configs()
    for name, description in config_info.items():
        print(f"  {name}: {description}")
    
    print(f"\nâœ… é…ç½®éªŒè¯:")
    for config_name in _CONFIGS.keys():
        try:
            config = get_config(config_name)
            print(f"  {config_name}: âœ“")
            print(f"    æ¨¡å‹: {type(config.model).__name__}")
            print(f"    åŠ¨ä½œç»´åº¦: {config.model.action_dim}")
            print(f"    Q-chunkingåºåˆ—é•¿åº¦: {config.qchunking.horizon_length}")
            print(f"    æ‰¹æ¬¡å¤§å°: {config.batch_size}")
            print(f"    OpenPIå…¼å®¹æ€§: âœ“ (ç»§æ‰¿TrainConfig)")
        except Exception as e:
            print(f"  {config_name}: âŒ {e}")
    
    print(f"\nğŸ¯ é…ç½®ç³»ç»Ÿç‰¹æ€§:")
    print(f"  - ç»Ÿä¸€æ¶æ„: åŸºäºOpenPI TrainConfig")  
    print(f"  - é›¶å†²çª: æ— å‘½åå†²çªå’Œæ¶æ„é—®é¢˜")
    print(f"  - å®Œå…¨å…¼å®¹: å¯ç›´æ¥ç”¨äºOpenPIåŸºç¡€è®¾æ–½")
    print(f"  - Q-chunkingæ”¯æŒ: å®Œæ•´çš„RLå‚æ•°é…ç½®")
    
    print(f"\nâœ… ç»Ÿä¸€é…ç½®ç³»ç»Ÿæµ‹è¯•å®Œæˆ!")