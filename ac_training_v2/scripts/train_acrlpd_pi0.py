#!/usr/bin/env python3
"""
Training script for ACRLPD + Ï€â‚€ integration.

This script provides a complete command-line interface for training ACRLPD
agents with Ï€â‚€ models on robotic manipulation tasks.

Usage:
    # Basic training with H5 data
    python train_acrlpd_pi0.py --data_dir /path/to/data --config droid

    # Training with LeRobot dataset
    python train_acrlpd_pi0.py --lerobot_repo_id lerobot/aloha_sim_insertion_human --config aloha

    # Resume from checkpoint
    python train_acrlpd_pi0.py --data_dir /path/to/data --config aloha --resume /path/to/checkpoint

    # Custom hyperparameters
    python train_acrlpd_pi0.py --lerobot_repo_id lerobot/pusht --config libero \
        --horizon_length 10 --bc_alpha 0.01 --batch_size 128

    # Disable WandB logging
    python train_acrlpd_pi0.py --data_dir /path/to/data --config droid --no_wandb
"""

import argparse
import logging
import os
import sys
from pathlib import Path
from typing import Optional

import jax
import jax.numpy as jnp
import numpy as np

# CRITICAL: Set HuggingFace cache to data disk, not system disk
# This prevents "No space left on device" errors and enables local dataset access
os.environ['HF_HOME'] = '/era-ai/lm/dataset/huggingface'
os.environ['HF_CACHE_HOME'] = '/era-ai/lm/dataset/huggingface_cache'
os.environ['HF_DATASETS_CACHE'] = '/era-ai/lm/dataset/huggingface_cache/datasets'
os.environ['HF_LEROBOT_HOME'] = '/era-ai/lm/dataset/huggingface_cache/lerobot'

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

# âœ… Stage 1.3 ä¿®å¤ï¼šæ›´æ–°ä¸ºagents_v2å’Œtraining_v2å¯¼å…¥
from config import get_config, RLTrainConfig
from agents_v2.acrlpd_pi0_agent import ACRLPDPi0Agent, create_acrlpd_pi0_agent_from_rl_config
from training_v2.training_loop import ACRLPDTrainer, ACRLPDTrainingConfig, create_simple_eval_fn
from data_v2.acrlpd_data_loader import create_acrlpd_data_loader

import openpi.models.pi0 as _pi0
import openpi.training.config as openpi_config
import openpi.training.optimizer as _optimizer
import openpi.training.sharding as sharding

# Reduce verbose output before imports
os.environ.setdefault("TF_CPP_MIN_LOG_LEVEL", "1")      # Suppress TF info logs
os.environ.setdefault("JAX_LOG_COMPILES", "0")          # Disable JAX compilation logs

# Setup logging - å‡å°‘è¯¦ç»†è¾“å‡º
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# Suppress verbose output from JAX/Flax/transformers libraries
logging.getLogger("jax").setLevel(logging.WARNING)
logging.getLogger("flax").setLevel(logging.WARNING)  
logging.getLogger("optax").setLevel(logging.WARNING)
logging.getLogger("transformers").setLevel(logging.WARNING)
logging.getLogger("torch").setLevel(logging.WARNING)
logging.getLogger("absl").setLevel(logging.WARNING)
logger = logging.getLogger(__name__)


def create_argument_parser() -> argparse.ArgumentParser:
    """Create command-line argument parser for unified config system."""
    parser = argparse.ArgumentParser(
        description="Train ACRLPD + Ï€â‚€ agents using unified configuration system",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # Configuration selection (main argument)
    parser.add_argument(
        "--config", type=str, required=True,
        choices=["rl_aloha_fold", "rl_fold_box", "rl_libero", "rl_droid"],
        help="Predefined configuration for different robot platforms"
    )
    
    # Experiment setup
    parser.add_argument(
        "--exp_name", type=str, required=True,
        help="Experiment name for checkpoints and logging"
    )
    
    # Training control
    parser.add_argument(
        "--resume", action="store_true",
        help="Resume from last checkpoint"
    )
    parser.add_argument(
        "--overwrite", action="store_true", 
        help="Overwrite existing experiment"
    )
    
    # Environment (for online training)
    parser.add_argument(
        "--env_name", type=str, default=None,
        help="Environment name for online training evaluation"
    )
    
    # System configuration
    parser.add_argument(
        "--seed", type=int, default=42,
        help="Random seed"
    )
    
    # Debug options
    parser.add_argument(
        "--debug", action="store_true",
        help="Enable debug mode"
    )
    parser.add_argument(
        "--no_wandb", action="store_true",
        help="Disable WandB logging"
    )
    parser.add_argument(
        "--dry_run", action="store_true",
        help="Setup without training"
    )
    
    # FSDPä¿®å¤æ–¹æ¡ˆé€‰æ‹©
    parser.add_argument(
        "--fsdp_fix", type=str, default="direct", 
        choices=["direct", "manual"],
        help="é€‰æ‹©FSDPä¿®å¤æ–¹æ¡ˆ: direct(é»˜è®¤) æˆ– manual"
    )
    
    return parser


def load_rl_config(args: argparse.Namespace) -> RLTrainConfig:
    """åŠ è½½å¹¶å®šåˆ¶RLTrainConfig"""
    # è·å–é¢„å®šä¹‰é…ç½®
    rl_config = get_config(args.config)
    
    # ä½¿ç”¨dataclasses.replaceæ›´æ–°å®éªŒåå’Œé€‰é¡¹
    import dataclasses
    rl_config = dataclasses.replace(
        rl_config,
        exp_name=args.exp_name,
        resume=args.resume,
        overwrite=args.overwrite,
        seed=args.seed,
        wandb_enabled=not args.no_wandb
    )
    
    return rl_config


def log_gpu_memory(step_name: str = "", alert_threshold: float = 95.0):
    """Enhanced GPU memory logging with step tracking and alerts."""
    try:
        import subprocess
        result = subprocess.run(['nvidia-smi', '--query-gpu=index,memory.used,memory.total', '--format=csv,noheader,nounits'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            step_prefix = f"ğŸ” GPU Memory - {step_name}: " if step_name else "ğŸ” GPU Memory Status:"
            logger.info(step_prefix)
            total_used = 0
            total_capacity = 0
            usage_data = []
            max_usage_gpu = 0
            max_usage_mb = 0
            
            for line in result.stdout.strip().split('\n'):
                parts = line.split(', ')
                if len(parts) == 3:
                    gpu_id, used, total = parts
                    used_mb = int(used)
                    total_mb = int(total)
                    usage_pct = (used_mb / total_mb) * 100
                    total_used += used_mb
                    total_capacity += total_mb
                    usage_data.append(used_mb)
                    
                    # è®°å½•æœ€é«˜ä½¿ç”¨GPU
                    if used_mb > max_usage_mb:
                        max_usage_mb = used_mb
                        max_usage_gpu = int(gpu_id)
                    
                    logger.info(f"  GPU {gpu_id}: {used}MB/{total}MB ({usage_pct:.1f}%)")
            
            # è®¡ç®—ç»Ÿè®¡æ•°æ®
            overall_pct = (total_used / total_capacity) * 100 if total_capacity > 0 else 0
            avg_per_gpu = total_used / len(usage_data) if len(usage_data) > 0 else 0
            std_dev = (sum((x - avg_per_gpu) ** 2 for x in usage_data) / len(usage_data)) ** 0.5 if len(usage_data) > 0 else 0
            
            logger.info(f"  Total: {total_used}MB/{total_capacity}MB ({overall_pct:.1f}%) | Avg/GPU: {avg_per_gpu:.0f}MB")
            logger.info(f"  Max GPU: GPU{max_usage_gpu} ({max_usage_mb}MB) | StdDev: {std_dev:.0f}MB")
            
            # FSDPæ•ˆæœåˆ†æ
            analyze_fsdp_effectiveness(usage_data, avg_per_gpu, std_dev)
            
            # å†…å­˜å¼‚å¸¸æŠ¥è­¦
            if overall_pct > alert_threshold:
                logger.error(f"ğŸš¨ GPUå†…å­˜è­¦å‘Š: {overall_pct:.1f}% > {alert_threshold}% é˜ˆå€¼!")
                if max_usage_mb > 76000:  # 76GB alarm (95% of 80GB)
                    logger.error(f"ğŸš¨ ä¸¥é‡è­¦å‘Š: GPU{max_usage_gpu}ä½¿ç”¨{max_usage_mb}MBï¼Œæ¥è¿‘80GBä¸Šé™!")
            
            # è¿”å›æ•°æ®ç”¨äºè¿›ä¸€æ­¥åˆ†æ
            return {
                'usage_data': usage_data,
                'avg_per_gpu': avg_per_gpu,
                'max_usage': max_usage_mb,
                'max_usage_gpu': max_usage_gpu,
                'std_dev': std_dev,
                'overall_pct': overall_pct
            }
                    
    except Exception as e:
        logger.warning(f"Failed to get GPU memory info: {e}")
        return None


def analyze_fsdp_effectiveness(usage_data, avg_per_gpu, std_dev):
    """åˆ†æFSDPåˆ†ç‰‡æ•ˆæœ"""
    if len(usage_data) < 2 or avg_per_gpu == 0:
        return
        
    # è®¡ç®—å˜å¼‚ç³»æ•° (CV = std_dev / mean)
    cv = std_dev / avg_per_gpu
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„å†…å­˜ä¸å‡
    min_usage = min(usage_data)
    max_usage = max(usage_data)
    usage_range = max_usage - min_usage
    
    logger.info(f"  FSDPåˆ†æ: CV={cv:.3f}, Range={usage_range:.0f}MB ({min_usage:.0f}-{max_usage:.0f})")
    
    if cv < 0.1 and usage_range < 5000:  # å˜å¼‚ç³»æ•°<0.1ä¸”èŒƒå›´<5GB
        logger.info("  âœ… FSDPæ•ˆæœä¼˜ç§€ï¼šå†…å­˜ä½¿ç”¨å‡åŒ€ï¼Œåˆ†ç‰‡å·¥ä½œè‰¯å¥½")
    elif cv < 0.3 and usage_range < 15000:  # å˜å¼‚ç³»æ•°<0.3ä¸”èŒƒå›´<15GB
        logger.info("  âš ï¸  FSDPæ•ˆæœä¸€èˆ¬ï¼šå­˜åœ¨ä¸€å®šå†…å­˜ä¸å‡")
    elif max_usage > 50000:  # ä»»ä½•GPUè¶…è¿‡50GB
        logger.info("  âŒ FSDPåˆ†ç‰‡å¯èƒ½å¤±æ•ˆï¼šå­˜åœ¨é«˜å†…å­˜ä½¿ç”¨GPU")
        # æ£€æµ‹æ˜¯å¦ä¸ºå¤åˆ¶è€Œéåˆ†ç‰‡
        high_usage_count = sum(1 for x in usage_data if x > 40000)
        if high_usage_count >= len(usage_data) * 0.8:  # 80%ä»¥ä¸ŠGPUé«˜å†…å­˜ä½¿ç”¨
            logger.error("  ğŸš¨ FSDPå®Œå…¨å¤±æ•ˆï¼šå‚æ•°è¢«å¤åˆ¶åˆ°å¤šä¸ªGPUè€Œéåˆ†ç‰‡!")
    else:
        logger.info("  âœ… FSDPåŸºæœ¬æ­£å¸¸ï¼šå†…å­˜ä½¿ç”¨åœ¨åˆç†èŒƒå›´å†…")


def verify_fsdp_sharding(train_state, step_name: str = ""):
    """éªŒè¯FSDPåˆ†ç‰‡æ˜¯å¦æ­£ç¡®å·¥ä½œ"""
    step_prefix = f"ğŸ” FSDPéªŒè¯ - {step_name}: " if step_name else "ğŸ” FSDPåˆ†ç‰‡éªŒè¯:"
    logger.info(step_prefix)
    
    try:
        # æ£€æŸ¥å…³é”®å‚æ•°çš„åˆ†ç‰‡æƒ…å†µ
        if hasattr(train_state, 'pi0_params'):
            param_count = 0
            sharded_count = 0
            replicated_count = 0
            
            def check_sharding(param_path, param):
                nonlocal param_count, sharded_count, replicated_count
                if hasattr(param, 'sharding') and hasattr(param, 'shape'):
                    param_count += 1
                    size_mb = param.nbytes / (1024 * 1024)
                    
                    # æ£€æŸ¥åˆ†ç‰‡çŠ¶æ€
                    if hasattr(param.sharding, 'spec'):
                        spec = param.sharding.spec
                        is_sharded = any(axis is not None for axis in spec)
                        
                        if is_sharded:
                            sharded_count += 1
                            if size_mb > 10:  # åªè®°å½•å¤§å‚æ•°
                                logger.info(f"  âœ… SHARDED: {param_path} ({param.shape}, {size_mb:.1f}MB)")
                        else:
                            replicated_count += 1
                            if size_mb > 1:  # è®°å½•ä¸­ç­‰å¤§å°å‚æ•°
                                logger.info(f"  ğŸ”„ REPLICATED: {param_path} ({param.shape}, {size_mb:.1f}MB)")
            
            # éå†å‚æ•°
            def traverse_params(params, prefix=""):
                if isinstance(params, dict):
                    for key, value in params.items():
                        new_prefix = f"{prefix}.{key}" if prefix else key
                        traverse_params(value, new_prefix)
                elif hasattr(params, 'shape'):
                    check_sharding(prefix, params)
                elif hasattr(params, 'value') and hasattr(params.value, 'shape'):
                    check_sharding(prefix, params.value)
            
            traverse_params(train_state.pi0_params, "pi0_params")
            
            # æ€»ç»“åˆ†ç‰‡æ•ˆæœ
            if param_count > 0:
                sharded_pct = (sharded_count / param_count) * 100
                logger.info(f"  ğŸ“Š åˆ†ç‰‡ç»Ÿè®¡: {sharded_count}/{param_count} å‚æ•°è¢«åˆ†ç‰‡ ({sharded_pct:.1f}%)")
                
                if sharded_pct > 70:
                    logger.info("  âœ… FSDPåˆ†ç‰‡æ•ˆæœè‰¯å¥½")
                elif sharded_pct > 30:
                    logger.info("  âš ï¸  FSDPåˆ†ç‰‡æ•ˆæœä¸­ç­‰")
                else:
                    logger.info("  âŒ FSDPåˆ†ç‰‡å¯èƒ½å¤±æ•ˆ")
            else:
                logger.info("  âš ï¸  æœªæ‰¾åˆ°å¯æ£€æŸ¥çš„å‚æ•°")
                
    except Exception as e:
        logger.warning(f"FSDPéªŒè¯å¤±è´¥: {e}")


def setup_jax_environment_with_fsdp(args: argparse.Namespace, rl_config: RLTrainConfig):
    """Setup JAX environment with OpenPI FSDP support."""
    logger.info(f"JAX version: {jax.__version__}")
    logger.info(f"Available devices: {jax.devices()}")
    
    # Log initial GPU memory
    log_gpu_memory("INITIAL")
    
    # OpenPIæ ‡å‡†ï¼šæ£€æŸ¥æ‰¹æ¬¡å¤§å°ä¸è®¾å¤‡æ•°é‡çš„å…¼å®¹æ€§
    if rl_config.batch_size % jax.device_count() != 0:
        raise ValueError(
            f"Batch size {rl_config.batch_size} must be divisible by device count {jax.device_count()}"
        )
    
    # åˆ›å»ºOpenPIæ ‡å‡†meshå’Œsharding
    mesh = sharding.make_mesh(rl_config.fsdp_devices)
    
    # OpenPIæ ‡å‡†ï¼šå§‹ç»ˆä½¿ç”¨DATA_AXISè¿›è¡Œåˆ†ç‰‡
    data_sharding = jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec(sharding.DATA_AXIS))
    replicated_sharding = jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec())
    
    logger.info(f"Created FSDP mesh: {mesh}")
    logger.info(f"Mesh shape: {mesh.shape}")
    logger.info(f"Data sharding: {data_sharding}")
    
    # ğŸ” éªŒè¯OpenPIæ ‡å‡†FSDPé…ç½®
    batch_per_device = rl_config.batch_size // jax.device_count()
    logger.info("âœ… OpenPIæ ‡å‡†FSDPé…ç½®éªŒè¯:")
    logger.info(f"   æ€»batch_size: {rl_config.batch_size}")
    logger.info(f"   è®¾å¤‡æ•°é‡: {jax.device_count()}")
    logger.info(f"   æ¯è®¾å¤‡batch: {batch_per_device}")
    logger.info(f"   æ•°æ®åˆ†ç‰‡ç­–ç•¥: {data_sharding}")
    
    # Log GPU memory after FSDP setup
    log_gpu_memory("FSDP_SETUP")
    
    # Set memory preallocation
    os.environ.setdefault("XLA_PYTHON_CLIENT_MEM_FRACTION", "0.9")
    
    if args.debug:
        jax.config.update("jax_debug_nans", True)
        jax.config.update("jax_debug_infs", True)
    
    return mesh, data_sharding, replicated_sharding


def create_acrlpd_data_loader_with_sharding(rl_config: RLTrainConfig, batch_size: int, data_sharding: jax.sharding.Sharding):
    """Create ACRLPD data loader with FSDP sharding support."""
    # åŸºäºç°æœ‰create_acrlpd_data_loaderï¼Œæ·»åŠ shardingå‚æ•°
    dataloader = create_acrlpd_data_loader(
        rl_config=rl_config,
        batch_size=batch_size,
        episodes_per_memory_pool=rl_config.episodes_per_memory_pool
    )
    
    # åŒ…è£…æ•°æ®åŠ è½½å™¨ä»¥æ”¯æŒåˆ†ç‰‡
    class ShardedACRLPDDataLoader:
        def __init__(self, base_loader, sharding):
            self.base_loader = base_loader
            self.sharding = sharding
        
        def __iter__(self):
            for batch in self.base_loader:
                # OpenPIæ ‡å‡†ï¼šä½¿ç”¨make_array_from_process_local_dataè¿›è¡Œåˆ†ç‰‡
                yield jax.tree.map(
                    lambda x: jax.make_array_from_process_local_data(self.sharding, np.asarray(x)), 
                    batch
                )
        
        def __len__(self):
            return len(self.base_loader)
            
        def sample_batch(self):
            """OpenPIæ ‡å‡†çš„FSDPåˆ†ç‰‡åº”ç”¨æ–¹æ³•"""
            batch = self.base_loader.sample_batch()
            return jax.tree.map(
                lambda x: jax.make_array_from_process_local_data(self.sharding, np.asarray(x)), 
                batch
            )
            
        def data_config(self):
            """Proxy data_config method."""
            if hasattr(self.base_loader, 'data_config'):
                return self.base_loader.data_config
            return None
            
        def __getattr__(self, name):
            """Proxy any other attributes to the base loader."""
            return getattr(self.base_loader, name)
    
    return ShardedACRLPDDataLoader(dataloader, data_sharding)


# create_agent_with_fsdp function removed to avoid nested mesh context
# FSDP initialization now handled directly in training loop using OpenPI patterns


def main():
    """Main training function using unified configuration system."""
    parser = create_argument_parser()
    args = parser.parse_args()
    
    # Setup logging
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
        logger.debug("Debug mode enabled")
    
    logger.info("ğŸš€ Starting ACRLPD + Ï€â‚€ training with OpenPI FSDP")
    logger.info(f"Arguments: {vars(args)}")
    
    # Load unified configuration first
    logger.info(f"Loading configuration: {args.config}")
    rl_config = load_rl_config(args)
    
    # Setup JAX environment with FSDP support
    mesh, data_sharding, replicated_sharding = setup_jax_environment_with_fsdp(args, rl_config)
    
    try:
        # Create training config for additional parameters
        training_config = ACRLPDTrainingConfig(
            env_name=args.env_name
        )
        
        logger.info(f"âœ“ Configuration loaded successfully")
        logger.info(f"  Model: {type(rl_config.model).__name__}")
        logger.info(f"  Dataset: {rl_config.data.repo_id if hasattr(rl_config.data, 'repo_id') else 'Unknown'}")
        logger.info(f"  Batch size: {rl_config.batch_size}")
        logger.info(f"  Training steps: {rl_config.num_train_steps}")
        logger.info(f"  FSDP devices: {rl_config.fsdp_devices}")
        
        if args.dry_run:
            logger.info("âœ“ Dry run completed successfully")
            return
        
        # Set random seed
        np.random.seed(rl_config.seed)
        rng = jax.random.PRNGKey(rl_config.seed)
        
        # Create data loader with FSDP sharding support
        logger.info("Creating data loader with FSDP sharding...")
        dataloader = create_acrlpd_data_loader_with_sharding(
            rl_config=rl_config,
            batch_size=rl_config.batch_size,
            data_sharding=data_sharding
        )
        logger.info(f"Data loader created with FSDP sharding support")
        
        # Create agent with proper FSDP support directly
        logger.info("Creating ACRLPD + Ï€â‚€ agent with FSDP...")
        logger.info(f"Debug: About to split RNG...")
        rng, agent_rng = jax.random.split(rng)
        logger.info(f"Debug: RNG split complete, entering mesh context...")
        
        # **å…³é”®ä¿®å¤ï¼šåˆ›å»ºå…¨å±€ä¼˜åŒ–å™¨é¿å…pytreeå…ƒæ•°æ®ä¸åŒ¹é…**
        logger.info("ğŸ”§ åˆ›å»ºå…¨å±€ä¼˜åŒ–å™¨å®ä¾‹ä»¥ç¡®ä¿pytreeä¸€è‡´æ€§...")
        
        # åœ¨FSDPä¸Šä¸‹æ–‡å¤–åˆ›å»ºå…¨å±€ä¼˜åŒ–å™¨ï¼ˆç¡®ä¿ä¸€è‡´æ€§ï¼‰
        import openpi.training.optimizer as _optimizer
        global_pi0_tx = _optimizer.create_optimizer(rl_config.actor_optimizer, rl_config.actor_lr_schedule)
        global_critic_tx = _optimizer.create_optimizer(rl_config.critic_optimizer, rl_config.critic_lr_schedule)
        global_temp_tx = None  # æš‚æ—¶ä¸ä½¿ç”¨æ¸©åº¦ä¼˜åŒ–å™¨
        
        logger.info("âœ… å…¨å±€ä¼˜åŒ–å™¨åˆ›å»ºæˆåŠŸï¼Œå°†ä¼ é€’ç»™FSDPåˆå§‹åŒ–")
        
        # **éªŒè¯ä¼˜åŒ–å™¨ä¸€è‡´æ€§ï¼ˆè°ƒè¯•pytreeé—®é¢˜ï¼‰**
        logger.info("ğŸ” éªŒè¯ä¼˜åŒ–å™¨pytreeä¸€è‡´æ€§...")
        from utils.pytree_checker import diagnose_pytree_structure, check_optimizer_consistency
        
        # è¯Šæ–­ä¼˜åŒ–å™¨ç»“æ„
        diagnose_pytree_structure(global_pi0_tx, "global_pi0_tx")
        diagnose_pytree_structure(global_critic_tx, "global_critic_tx")
        
        # åˆ›å»ºæµ‹è¯•ä¼˜åŒ–å™¨éªŒè¯ä¸€è‡´æ€§
        test_pi0_tx = _optimizer.create_optimizer(rl_config.actor_optimizer, rl_config.actor_lr_schedule)
        consistency_ok = check_optimizer_consistency(global_pi0_tx, test_pi0_tx, "global_pi0_tx", "test_pi0_tx")
        
        if not consistency_ok:
            logger.warning("âš ï¸ ä¼˜åŒ–å™¨ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥ï¼Œä½†ç»§ç»­è®­ç»ƒ...")
        else:
            logger.info("âœ… ä¼˜åŒ–å™¨ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")
        
        # **ä¿®å¤8å¡FSDPå†…å­˜çˆ†ç‚¸é—®é¢˜**
        # é—®é¢˜ï¼šå‚æ•°åœ¨æ‰€æœ‰GPUä¸Šå¤åˆ¶è€Œä¸æ˜¯åˆ†ç‰‡ï¼Œå¯¼è‡´æ¯ä¸ªGPUä½¿ç”¨61GB
        # è§£å†³æ–¹æ¡ˆï¼šé€‰æ‹©é€‚å½“çš„FSDPä¿®å¤æ–¹æ¡ˆï¼Œå¹¶ä¼ é€’å…¨å±€ä¼˜åŒ–å™¨
        
        logger.info(f"ğŸ”§ ä½¿ç”¨ {args.fsdp_fix.upper()} FSDPä¿®å¤æ–¹æ¡ˆ")
        
        # **ç»Ÿä¸€ä½¿ç”¨init_acrlpd_fsdp_trainingé¿å…æ¨¡å‹é‡å¤åˆ›å»º**
        logger.info("ğŸš€ ä½¿ç”¨ç»Ÿä¸€FSDPåˆå§‹åŒ–ï¼ˆé¿å…æ¨¡å‹é‡å¤åˆ›å»ºï¼‰")
        # âœ… Stage 1.3 ä¿®å¤ï¼šä½¿ç”¨training_v2å¯¼å…¥
        from training_v2.acrlpd_train_state import init_acrlpd_fsdp_training
        
        train_state, state_sharding, lazy_jit_creator = init_acrlpd_fsdp_training(
            rl_config=rl_config,
            mesh=mesh,
            rng=agent_rng,
            data_sharding=data_sharding,
            step=0,
            global_pi0_tx=global_pi0_tx,
            global_critic_tx=global_critic_tx
        )
        
        # å»¶è¿Ÿåˆ›å»ºJITå‡½æ•°
        logger.info("ğŸ”§ åˆ›å»ºJITè®­ç»ƒå‡½æ•°...")
        fsdp_train_step_fn = lazy_jit_creator()
        
        # éªŒè¯FSDPæ˜¯å¦çœŸæ­£å·¥ä½œ
        logger.info("éªŒè¯FSDPåˆ†ç‰‡æ•ˆæœ...")
        
        # æ£€æŸ¥å®é™…æ˜¾å­˜ä½¿ç”¨
        memory_stats = log_gpu_memory("FSDPéªŒè¯")
        if memory_stats:
            avg_memory = memory_stats['avg_per_gpu']
            max_memory = memory_stats['max_usage']
            logger.info(f"ğŸ’¾ FSDPå†…å­˜ä½¿ç”¨: {avg_memory:.0f}MB/GPU (æœ€å¤§{max_memory:.0f}MB)")
        
        # åˆ›å»ºè½»é‡çº§agentç”¨äºtrainerï¼ˆä¸åŒ…å«è®­ç»ƒçŠ¶æ€ï¼‰
        with jax.default_device(jax.devices('cpu')[0]):
            agent = create_acrlpd_pi0_agent_from_rl_config(rl_config, agent_rng)
        
        logger.info("FSDPéªŒè¯å®Œæˆ")
        logger.info("ç›´æ¥FSDPæ–¹å¼å·²ç»•è¿‡agent.to_train_state()çš„å¤æ‚è½¬æ¢")
        
        # Log GPU memory after FSDP creation and verify sharding
        logger.info("ğŸ” GPUå†…å­˜ä½¿ç”¨ - FSDPè®­ç»ƒçŠ¶æ€åˆ›å»ºå:")
        log_gpu_memory("FSDP_STATE_CREATED")
        
        # ğŸ”§ éªŒè¯FSDPåˆ†ç‰‡æ˜¯å¦æ­£ç¡®å·¥ä½œ
        verify_fsdp_sharding(train_state, "FSDP_STATE_CREATED")
        
        # Create evaluation function if needed
        eval_fn = None
        if args.env_name:
            eval_fn = create_simple_eval_fn(args.env_name)
            logger.info(f"ğŸ“ˆ Created evaluation function for {args.env_name}")
        
        # Create trainer with OpenPIæ ‡å‡†FSDP agentå’Œtrain_state  
        logger.info("Creating trainer with OpenPIæ ‡å‡†FSDP agent...")
        trainer = ACRLPDTrainer(
            agent=agent,  # ä¼ é€’OpenPIæ ‡å‡†æ–¹å¼åˆ›å»ºçš„agent
            dataloader=dataloader,
            rl_config=rl_config,
            training_config=training_config,
            eval_fn=eval_fn,
            # FSDPå‚æ•°
            mesh=mesh,
            data_sharding=data_sharding,
            replicated_sharding=replicated_sharding,
            # å…¨å±€ä¼˜åŒ–å™¨å‚æ•°ï¼ˆä¿®å¤pytreeä¸€è‡´æ€§ï¼‰
            global_pi0_tx=global_pi0_tx,
            global_critic_tx=global_critic_tx
        )
        
        # è®¾ç½®è®­ç»ƒå™¨çš„FSDPçŠ¶æ€ï¼ˆç»Ÿä¸€æ–¹å¼åˆ›å»ºï¼‰
        trainer.fsdp_train_state = train_state
        trainer.train_state_sharding = state_sharding
        trainer.fsdp_train_step = fsdp_train_step_fn
        trainer.use_fsdp = True
        
        # éªŒè¯FSDPç»„ä»¶è®¾ç½®æˆåŠŸ
        logger.info("âœ… FSDP components set successfully")
        
        # **æ·»åŠ pytreeä¸€è‡´æ€§éªŒè¯**
        logger.info("ğŸ” éªŒè¯è®­ç»ƒçŠ¶æ€pytreeä¸€è‡´æ€§...")
        from utils.pytree_checker import diagnose_pytree_structure, validate_fsdp_compatibility
        
        # è¯Šæ–­è®­ç»ƒçŠ¶æ€ç»“æ„
        diagnose_pytree_structure(train_state, "train_state")
        
        # éªŒè¯FSDPå…¼å®¹æ€§
        fsdp_compatible = validate_fsdp_compatibility(train_state, mesh, data_sharding)
        if not fsdp_compatible:
            logger.warning("âš ï¸ FSDPå…¼å®¹æ€§éªŒè¯å¤±è´¥ï¼Œä½†ç»§ç»­è®­ç»ƒ...")
        else:
            logger.info("âœ… FSDPå…¼å®¹æ€§éªŒè¯é€šè¿‡")
        
        # ç¡®ä¿PyTreeç»“æ„ä¸€è‡´æ€§ - åœ¨JITç¼–è¯‘å‰é¢„çƒ­train_state
        logger.info("ğŸ”§ é¢„çƒ­è®­ç»ƒçŠ¶æ€ç¡®ä¿PyTreeä¸€è‡´æ€§...")
        jax.block_until_ready(train_state)
        
        logger.info("âœ… è®­ç»ƒå™¨åˆ›å»ºæˆåŠŸï¼Œä½¿ç”¨OpenPIæ ‡å‡†8å¡FSDPé…ç½®")
        
        # Log GPU memory before training starts
        logger.info("ğŸ” GPUå†…å­˜ä½¿ç”¨ - è®­ç»ƒå¼€å§‹å‰:")
        log_gpu_memory("BEFORE_TRAINING")
        
        # Start training
        logger.info("Starting training...")
        trained_agent = trainer.train()
        
        logger.info("Training completed successfully!")
        
        # Final evaluation
        if eval_fn:
            logger.info("Running final evaluation...")
            rng, eval_rng = jax.random.split(rng) 
            final_reward, final_length = eval_fn(trained_agent, eval_rng, deterministic=True)
            logger.info(f"Final evaluation: reward={final_reward:.3f}, length={final_length}")
        
    except KeyboardInterrupt:
        logger.info(" Training interrupted by user")
        sys.exit(1)
    
    except Exception as e:
        logger.error(f" Training failed with error: {e}")
        if args.debug:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()